{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3af757e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alastor/miniconda3/envs/signalflow/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Any' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatetime\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m datetime\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/signal-flow/src/signalflow/__init__.py:25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01manalytic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01manalytic\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdetector\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfeature\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtarget\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtarget\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/signal-flow/src/signalflow/detector/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetector\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SignalDetector\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetector\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msma_cross\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExampleSmaCrossDetector\n\u001b[32m      4\u001b[39m __all__ = [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSignalDetector\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExampleSmaCrossDetector\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/signal-flow/src/signalflow/detector/base.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RawDataView, Signals, SfComponentType, SignalType, RawDataType\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeaturePipeline\n\u001b[32m     13\u001b[39m \u001b[38;5;129m@dataclass\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mSignalDetector\u001b[39;00m(ABC):\n\u001b[32m     15\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Base class for Polars-first signal detection.\u001b[39;00m\n\u001b[32m     16\u001b[39m \n\u001b[32m     17\u001b[39m \u001b[33;03m    Provides standardized pipeline for detecting trading signals from raw data:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     95\u001b[39m \u001b[33;03m        Signals: Container for signal output.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/signal-flow/src/signalflow/feature/__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Feature, ExampleRsiFeature, ExampleSmaFeature\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mglobal_feature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalFeature, ExampleGlobalMeanRsiFeature\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moffset_feature\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OffsetFeature\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/signal-flow/src/signalflow/feature/base.py:9\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpolars\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SfComponentType, sf_component\n\u001b[32m      8\u001b[39m \u001b[38;5;129;43m@dataclass\u001b[39;49m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mFeature\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Base class for all features.\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[33;43;03m    \u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[33;43;03m    Two methods to implement:\u001b[39;49;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m \u001b[33;43;03m        outputs: Output column templates, e.g. [\"rsi_{period}\"]\u001b[39;49;00m\n\u001b[32m     19\u001b[39m \u001b[33;43;03m    \"\"\"\u001b[39;49;00m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomponent_type\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mClassVar\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSfComponentType\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mSfComponentType\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFEATURE\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/signal-flow/src/signalflow/feature/base.py:27\u001b[39m, in \u001b[36mFeature\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     24\u001b[39m group_col: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mpair\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     25\u001b[39m ts_col: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, df: pl.DataFrame, context: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[43mAny\u001b[49m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> pl.DataFrame:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute feature for all pairs. Must override.\"\"\"\u001b[39;00m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df.group_by(\u001b[38;5;28mself\u001b[39m.group_col, maintain_order=\u001b[38;5;28;01mTrue\u001b[39;00m).map_groups(\u001b[38;5;28mself\u001b[39m.compute_pair)\n",
      "\u001b[31mNameError\u001b[39m: name 'Any' is not defined"
     ]
    }
   ],
   "source": [
    "import signalflow as sf\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8aabc8",
   "metadata": {},
   "source": [
    "## CORE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71749eb8",
   "metadata": {},
   "source": [
    "### Raw Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c99d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_store = sf.data.raw_store.DuckDbSpotStore(\n",
    "    db_path=Path(\"test.duckdb\")\n",
    ")\n",
    "\n",
    "loader = sf.data.source.BinanceSpotLoader(\n",
    "    store = spot_store\n",
    ")\n",
    "await loader.download(\n",
    "    pairs=[\"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\", \"BNBUSDT\", \"XRPUSDT\"],\n",
    "    start=datetime(2025, 10, 1),\n",
    "    end=datetime(2025, 12, 31),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed3ed5",
   "metadata": {},
   "source": [
    "### RawDataFactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725b2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = sf.data.RawDataFactory.from_duckdb_spot_store(\n",
    "    spot_store_path=Path(\"test.duckdb\"),\n",
    "    pairs=[\"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\"],\n",
    "    start=datetime(2025, 10, 1),\n",
    "    end=datetime(2025, 12, 31),\n",
    "    data_types=[\"spot\"],\n",
    ")\n",
    "raw_data_view = sf.core.RawDataView(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbdeca",
   "metadata": {},
   "source": [
    "## FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum\n",
    "from typing import Any, ClassVar\n",
    "import signalflow as sf\n",
    "import polars as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215cdb3",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'signalflow.feature' has no attribute 'ExampleRsiFeature'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 449\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# from dataclasses import dataclass, field\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# from enum import Enum\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# from typing import ClassVar\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    440\u001b[39m \n\u001b[32m    441\u001b[39m \u001b[38;5;66;03m#         return \"\\n\".join(lines)\u001b[39;00m\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msignalflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msf\u001b[39;00m\n\u001b[32m    447\u001b[39m pipeline = FeaturePipeline(\n\u001b[32m    448\u001b[39m     features=[\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m         \u001b[43msf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mExampleRsiFeature\u001b[49m(period=\u001b[32m140\u001b[39m),\n\u001b[32m    450\u001b[39m         sf.feature.ExampleRsiFeature(period=\u001b[32m210\u001b[39m),\n\u001b[32m    451\u001b[39m         OffsetFeature(\n\u001b[32m    452\u001b[39m             feature_cls=sf.feature.ExampleRsiFeature,\n\u001b[32m    453\u001b[39m             feature_params={\u001b[33m\"\u001b[39m\u001b[33mperiod\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m140\u001b[39m},\n\u001b[32m    454\u001b[39m             window=\u001b[32m60\u001b[39m,\n\u001b[32m    455\u001b[39m             prefix=\u001b[33m\"\u001b[39m\u001b[33mofs_\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    456\u001b[39m         ),\n\u001b[32m    457\u001b[39m         sf.feature.ExampleGlobalMeanRsiFeature(period=\u001b[32m140\u001b[39m),\n\u001b[32m    458\u001b[39m         sf.feature.ExampleGlobalMeanRsiFeature(period=\u001b[32m210\u001b[39m),\n\u001b[32m    459\u001b[39m     ],\n\u001b[32m    460\u001b[39m     raw_data_type=sf.RawDataType.SPOT,\n\u001b[32m    461\u001b[39m )\n\u001b[32m    463\u001b[39m df = pipeline.run(raw_data_view)\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'signalflow.feature' has no attribute 'ExampleRsiFeature'"
     ]
    }
   ],
   "source": [
    "# from dataclasses import dataclass, field\n",
    "# from enum import Enum\n",
    "# from typing import ClassVar\n",
    "# import signalflow as sf\n",
    "# import polars as pl\n",
    "# from signalflow.core import SfComponentType, sf_component\n",
    "\n",
    "\n",
    "# class RawDataType(str, Enum):\n",
    "#     \"\"\"Type of raw market data.\"\"\"\n",
    "    \n",
    "#     SPOT = \"spot\"\n",
    "#     FUTURES = \"futures\"\n",
    "#     PERPETUAL = \"perpetual\"\n",
    "    \n",
    "#     @property\n",
    "#     def columns(self) -> set[str]:\n",
    "#         \"\"\"Columns guaranteed to be present.\"\"\"\n",
    "#         base = {\"pair\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"}\n",
    "        \n",
    "#         if self == RawDataType.FUTURES:\n",
    "#             return base | {\"open_interest\"}\n",
    "#         elif self == RawDataType.PERPETUAL:\n",
    "#             return base | {\"funding_rate\", \"open_interest\"}\n",
    "        \n",
    "#         return base\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class Feature:\n",
    "#     \"\"\"Base class for all features.\n",
    "    \n",
    "#     Two methods to implement:\n",
    "#         - compute(df): all pairs, abstract for GlobalFeature/Pipeline\n",
    "#         - compute_pair(df): one pair, for regular features\n",
    "    \n",
    "#     Attributes:\n",
    "#         requires: Input column templates, e.g. [\"{price_col}\"]\n",
    "#         outputs: Output column templates, e.g. [\"rsi_{period}\"]\n",
    "#     \"\"\"\n",
    "#     component_type: ClassVar[SfComponentType] = SfComponentType.FEATURE\n",
    "#     requires: ClassVar[list[str]] = []\n",
    "#     outputs: ClassVar[list[str]] = []\n",
    "    \n",
    "#     group_col: str = \"pair\"\n",
    "#     ts_col: str = \"timestamp\"\n",
    "    \n",
    "#     def compute(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Compute feature for all pairs. Must override.\"\"\"\n",
    "#         return df.group_by(self.group_col, maintain_order=True).map_groups(self.compute_pair)\n",
    "    \n",
    "#     def compute_pair(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Compute feature for single pair. Override for per-pair features.\"\"\"\n",
    "#         raise NotImplementedError(f\"{self.__class__.__name__} must implement compute_pair()\")\n",
    "    \n",
    "#     def output_cols(self, prefix: str = \"\") -> list[str]:\n",
    "#         \"\"\"Actual output column names with parameter substitution.\"\"\"\n",
    "#         return [f\"{prefix}{tpl.format(**self.__dict__)}\" for tpl in self.outputs]\n",
    "    \n",
    "#     def required_cols(self) -> list[str]:\n",
    "#         \"\"\"Actual required column names with parameter substitution.\"\"\"\n",
    "#         return [\n",
    "#             tpl.format(**self.__dict__) if \"{\" in tpl else tpl \n",
    "#             for tpl in self.requires\n",
    "#         ]\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# @sf_component(name=\"test_rsi\")\n",
    "# class RsiFeature(Feature):\n",
    "#     \"\"\"Relative Strength Index.\n",
    "    \n",
    "#     Args:\n",
    "#         period: RSI period. Default: 14.\n",
    "#         price_col: Price column to use. Default: \"close\".\n",
    "    \n",
    "#     Example:\n",
    "#         >>> rsi = RsiFeature(period=21)\n",
    "#         >>> rsi.output_cols()  # [\"rsi_21\"]\n",
    "#     \"\"\"\n",
    "    \n",
    "#     period: int = 14\n",
    "#     price_col: str = \"close\"\n",
    "    \n",
    "#     requires = [\"{price_col}\"]\n",
    "#     outputs = [\"rsi_{period}\"]\n",
    "    \n",
    "#     def compute(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Compute RSI for all pairs.\"\"\"\n",
    "#         return df.group_by(self.group_col, maintain_order=True).map_groups(self.compute_pair)\n",
    "    \n",
    "#     def compute_pair(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Compute RSI for single pair.\"\"\"\n",
    "#         col_name = f\"rsi_{self.period}\"\n",
    "        \n",
    "#         delta = pl.col(self.price_col).diff()\n",
    "#         gain = pl.when(delta > 0).then(delta).otherwise(0)\n",
    "#         loss = pl.when(delta < 0).then(-delta).otherwise(0)\n",
    "        \n",
    "#         avg_gain = gain.rolling_mean(window_size=self.period)\n",
    "#         avg_loss = loss.rolling_mean(window_size=self.period)\n",
    "        \n",
    "#         rs = avg_gain / avg_loss\n",
    "#         rsi = 100 - (100 / (1 + rs))\n",
    "        \n",
    "#         return df.with_columns(rsi.alias(col_name))\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class GlobalFeature(Feature):\n",
    "#     \"\"\"Base class for features computed across all pairs.\n",
    "    \n",
    "#     Override compute() with custom aggregation logic.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def compute(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Must override - compute global feature across all pairs.\"\"\"\n",
    "#         raise NotImplementedError(f\"{self.__class__.__name__} must implement compute()\")\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# @sf_component(name=\"example/global_rsi\")\n",
    "# class GlobalMeanRsiFeature(GlobalFeature):\n",
    "#     \"\"\"Mean RSI across all pairs per timestamp.\n",
    "    \n",
    "#     1. Compute RSI per pair\n",
    "#     2. Mean across all pairs at time t → global_mean_rsi\n",
    "#     3. Optionally: rsi_diff = pair_rsi - global_mean_rsi\n",
    "    \n",
    "#     Args:\n",
    "#         period: RSI period. Default: 14.\n",
    "#         add_diff: Add per-pair difference column. Default: False.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     period: int = 14\n",
    "#     price_col: str = \"close\"\n",
    "#     add_diff: bool = False\n",
    "    \n",
    "#     requires = [\"{price_col}\"]\n",
    "#     outputs = [\"global_mean_rsi_{period}\"]\n",
    "    \n",
    "#     def output_cols(self, prefix: str = \"\") -> list[str]:\n",
    "#         cols = [f\"{prefix}global_mean_rsi_{self.period}\"]\n",
    "#         if self.add_diff:\n",
    "#             cols.append(f\"{prefix}rsi_{self.period}_diff\")\n",
    "#         return cols\n",
    "    \n",
    "#     def compute(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         rsi_col = f\"rsi_{self.period}\"\n",
    "#         out_col = f\"global_mean_rsi_{self.period}\"\n",
    "        \n",
    "#         has_rsi = rsi_col in df.columns\n",
    "#         if not has_rsi:\n",
    "#             rsi = RsiFeature(period=self.period, price_col=self.price_col)\n",
    "#             df = rsi.compute(df)\n",
    "        \n",
    "#         mean_df = df.group_by(self.ts_col).agg(\n",
    "#             pl.col(rsi_col).mean().alias(out_col)\n",
    "#         )\n",
    "        \n",
    "#         df = df.join(mean_df, on=self.ts_col, how=\"left\")\n",
    "        \n",
    "#         if self.add_diff:\n",
    "#             df = df.with_columns(\n",
    "#                 (pl.col(rsi_col) - pl.col(out_col)).alias(f\"rsi_{self.period}_diff\")\n",
    "#             )\n",
    "        \n",
    "#         if not has_rsi:\n",
    "#             df = df.drop(rsi_col)\n",
    "        \n",
    "#         return df\n",
    "        \n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class OffsetFeature(Feature):\n",
    "#     \"\"\"Multi-timeframe feature via offset resampling.\n",
    "    \n",
    "#     Creates `window` parallel time series with different offsets.\n",
    "#     Each offset computes features as if on `window`-minute bars.\n",
    "    \n",
    "#     For GRU trained on 15m but needing 1m decisions:\n",
    "#     - Creates 15 parallel series (offset 0-14)\n",
    "#     - At signal time t, use offset = t % 15 to get correct features\n",
    "    \n",
    "#     Args:\n",
    "#         feature_cls: Feature class (for Kedro serialization).\n",
    "#         feature_params: Parameters for feature_cls.\n",
    "#         window: Aggregation window in minutes. Default: 15.\n",
    "#         prefix: Prefix for output columns. Default: \"{window}m_\".\n",
    "    \n",
    "#     Example:\n",
    "#         >>> offset = OffsetFeature(\n",
    "#         ...     feature_cls=RsiFeature,\n",
    "#         ...     feature_params={\"period\": 14},\n",
    "#         ...     window=15,\n",
    "#         ... )\n",
    "#         >>> # Outputs: 15m_rsi_14, offset\n",
    "#     \"\"\"\n",
    "    \n",
    "#     feature_cls: type[Feature] = None\n",
    "#     feature_params: dict = field(default_factory=dict)\n",
    "#     window: int = 15\n",
    "#     prefix: str | None = None\n",
    "    \n",
    "#     requires = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"timestamp\"]\n",
    "#     outputs = [\"offset\"]\n",
    "    \n",
    "#     def __post_init__(self):\n",
    "#         if self.feature_cls is None:\n",
    "#             raise ValueError(\"OffsetFeature requires 'feature_cls'\")\n",
    "#         self._base = self.feature_cls(**self.feature_params)\n",
    "#         if self.prefix is None:\n",
    "#             self.prefix = f\"{self.window}m_\"\n",
    "    \n",
    "#     def output_cols(self, prefix: str = \"\") -> list[str]:\n",
    "#         base_cols = self._base.output_cols(prefix=f\"{prefix}{self.prefix}\")\n",
    "#         return base_cols + [f\"{prefix}offset\"]\n",
    "    \n",
    "#     def required_cols(self) -> list[str]:\n",
    "#         return [\"open\", \"high\", \"low\", \"close\", \"volume\", self.ts_col]\n",
    "    \n",
    "#     def _resample_ohlcv(self, df: pl.DataFrame, offset: int) -> pl.DataFrame:\n",
    "#         \"\"\"Resample 1m OHLCV to window-minute bars with given offset.\"\"\"\n",
    "#         df = df.with_row_index(\"_row_idx\")\n",
    "        \n",
    "#         df = df.with_columns(\n",
    "#             ((pl.col(\"_row_idx\").cast(pl.Int64) - offset) // self.window).alias(\"_grp\")\n",
    "#         )\n",
    "        \n",
    "#         agg_exprs = [\n",
    "#             pl.col(self.ts_col).last(),\n",
    "#             pl.col(\"open\").first(),\n",
    "#             pl.col(\"high\").max(),\n",
    "#             pl.col(\"low\").min(),\n",
    "#             pl.col(\"close\").last(),\n",
    "#             pl.col(\"volume\").sum(),\n",
    "#         ]\n",
    "#         if self.group_col in df.columns:\n",
    "#             agg_exprs.append(pl.col(self.group_col).first())\n",
    "        \n",
    "#         return df.group_by(\"_grp\", maintain_order=True).agg(agg_exprs)\n",
    "    \n",
    "#     def compute_pair(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Compute features for all offsets and merge to original timeframe.\"\"\"\n",
    "#         df = df.sort(self.ts_col)\n",
    "#         original_len = len(df)\n",
    "#         df = df.with_row_index(\"_orig_idx\")\n",
    "        \n",
    "#         df = df.with_columns(\n",
    "#             (pl.col(\"_orig_idx\") % self.window).cast(pl.UInt8).alias(\"offset\")\n",
    "#         )\n",
    "        \n",
    "#         offset_results = []\n",
    "#         for offset in range(self.window):\n",
    "#             resampled = self._resample_ohlcv(df.drop([\"_orig_idx\", \"offset\"]), offset)\n",
    "            \n",
    "#             with_feat = self._base.compute_pair(resampled)\n",
    "#             with_feat = with_feat.with_columns(pl.lit(offset).cast(pl.UInt8).alias(\"_offset\"))\n",
    "            \n",
    "#             for col in self._base.output_cols():\n",
    "#                 if col in with_feat.columns:\n",
    "#                     with_feat = with_feat.rename({col: f\"{self.prefix}{col}\"})\n",
    "            \n",
    "#             offset_results.append(with_feat)\n",
    "        \n",
    "#         all_offsets = pl.concat(offset_results)\n",
    "        \n",
    "#         df = df.with_columns(\n",
    "#             ((pl.col(\"_orig_idx\").cast(pl.Int64) - pl.col(\"offset\").cast(pl.Int64)) // self.window).alias(\"_grp\")\n",
    "#         )\n",
    "        \n",
    "#         feature_cols = [f\"{self.prefix}{col}\" for col in self._base.output_cols()]\n",
    "        \n",
    "#         result = df.join(\n",
    "#             all_offsets.select([\"_grp\", \"_offset\"] + feature_cols),\n",
    "#             left_on=[\"_grp\", \"offset\"],\n",
    "#             right_on=[\"_grp\", \"_offset\"],\n",
    "#             how=\"left\",\n",
    "#         )\n",
    "        \n",
    "#         result = result.drop([\"_orig_idx\", \"_grp\"])\n",
    "#         assert len(result) == original_len\n",
    "        \n",
    "#         return result\n",
    "    \n",
    "#     def compute(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         return df.group_by(self.group_col, maintain_order=True).map_groups(self.compute_pair)\n",
    "    \n",
    "#     def to_dict(self) -> dict:\n",
    "#         \"\"\"Serialize for Kedro.\"\"\"\n",
    "#         return {\n",
    "#             \"feature_cls\": self.feature_cls.__name__,\n",
    "#             \"feature_params\": self.feature_params,\n",
    "#             \"window\": self.window,\n",
    "#             \"prefix\": self.prefix,\n",
    "#         }\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_dict(cls, data: dict, registry: dict[str, type[Feature]]) -> \"OffsetFeature\":\n",
    "#         \"\"\"Deserialize from config.\"\"\"\n",
    "#         return cls(\n",
    "#             feature_cls=registry[data[\"feature_cls\"]],\n",
    "#             feature_params=data[\"feature_params\"],\n",
    "#             window=data[\"window\"],\n",
    "#             prefix=data.get(\"prefix\"),\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# @dataclass\n",
    "# class FeaturePipeline(Feature):\n",
    "#     \"\"\"Orchestrates multiple features with optimized execution.\n",
    "    \n",
    "#     Groups consecutive per-pair features into batches for single group_by.\n",
    "    \n",
    "#     Args:\n",
    "#         features: List of features to compute.\n",
    "#         raw_data_type: Type of raw data (defines available columns).\n",
    "    \n",
    "#     Example:\n",
    "#         >>> pipeline = FeaturePipeline(\n",
    "#         ...     features=[\n",
    "#         ...         RsiFeature(period=14),\n",
    "#         ...         SmaFeature(period=20),\n",
    "#         ...         GlobalFeature(base=RsiFeature(period=14), reference_pair=\"BTCUSDT\"),\n",
    "#         ...     ],\n",
    "#         ...     raw_data_type=RawDataType.SPOT,\n",
    "#         ... )\n",
    "#         >>> df = pipeline.run(raw_data_view)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     features: list[Feature] = field(default_factory=list)\n",
    "#     raw_data_type: RawDataType = RawDataType.SPOT\n",
    "    \n",
    "#     requires: ClassVar[list[str]] = []\n",
    "    \n",
    "#     def __post_init__(self):\n",
    "#         if not self.features:\n",
    "#             raise ValueError(\"FeaturePipeline requires at least one feature\")\n",
    "#         self._validate()\n",
    "    \n",
    "#     @property\n",
    "#     def outputs(self) -> list[str]:\n",
    "#         \"\"\"Aggregated outputs from all features.\"\"\"\n",
    "#         result = []\n",
    "#         for f in self.features:\n",
    "#             result.extend(f.output_cols())\n",
    "#         return result\n",
    "    \n",
    "#     def output_cols(self, prefix: str = \"\") -> list[str]:\n",
    "#         return [f\"{prefix}{col}\" for col in self.outputs]\n",
    "    \n",
    "#     def _validate(self):\n",
    "#         \"\"\"Validate all dependencies are satisfied.\"\"\"\n",
    "#         available = self.raw_data_type.columns.copy()\n",
    "        \n",
    "#         for f in self.features:\n",
    "#             required = set(f.required_cols())\n",
    "#             missing = required - available\n",
    "            \n",
    "#             if missing:\n",
    "#                 raise ValueError(\n",
    "#                     f\"{f.__class__.__name__} requires {missing}, \"\n",
    "#                     f\"available: {sorted(available)}\"\n",
    "#                 )\n",
    "            \n",
    "#             available.update(f.output_cols())\n",
    "    \n",
    "#     def _group_into_batches(self) -> list[list[Feature]]:\n",
    "#         \"\"\"Group features: consecutive per-pair → batch, global → separate.\"\"\"\n",
    "#         batches = []\n",
    "#         current_batch = []\n",
    "        \n",
    "#         for f in self.features:\n",
    "#             is_global = isinstance(f, (GlobalFeature, GlobalMeanRsiFeature, FeaturePipeline))\n",
    "            \n",
    "#             if is_global:\n",
    "#                 if current_batch:\n",
    "#                     batches.append(current_batch)\n",
    "#                     current_batch = []\n",
    "#                 batches.append([f])\n",
    "#             else:\n",
    "#                 current_batch.append(f)\n",
    "#         if current_batch:\n",
    "#             batches.append(current_batch)\n",
    "        \n",
    "#         return batches\n",
    "    \n",
    "#     def _is_per_pair_batch(self, batch: list[Feature]) -> bool:\n",
    "#         \"\"\"Check if batch contains only per-pair features.\"\"\"\n",
    "#         return not any(\n",
    "#             isinstance(f, (GlobalFeature, GlobalMeanRsiFeature, FeaturePipeline)) \n",
    "#             for f in batch\n",
    "#         )\n",
    "    \n",
    "#     def compute(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "#         \"\"\"Compute all features with optimized batching.\"\"\"\n",
    "#         df = df.sort([self.group_col, self.ts_col])\n",
    "        \n",
    "#         batches = self._group_into_batches()\n",
    "        \n",
    "#         for batch in batches:\n",
    "#             if self._is_per_pair_batch(batch):\n",
    "#                 def apply_batch(pair_df: pl.DataFrame, features=batch) -> pl.DataFrame:\n",
    "#                     for f in features:\n",
    "#                         pair_df = f.compute_pair(pair_df)\n",
    "#                     return pair_df\n",
    "                \n",
    "#                 df = df.group_by(self.group_col, maintain_order=True).map_groups(apply_batch)\n",
    "#             else:\n",
    "#                 for f in batch:\n",
    "#                     df = f.compute(df)\n",
    "        \n",
    "#         return df\n",
    "    \n",
    "#     def run(self, raw_data_view: sf.RawDataView) -> pl.DataFrame:\n",
    "#         \"\"\"Entry point: load from RawDataView and compute.\"\"\"\n",
    "#         df = raw_data_view.to_polars(self.raw_data_type)\n",
    "#         return self.compute(df)\n",
    "    \n",
    "#     def to_mermaid(self) -> str:\n",
    "#         \"\"\"Generate Mermaid diagram of feature dependencies.\"\"\"\n",
    "#         lines = [\"graph LR\"]\n",
    "#         lines.append(\"    subgraph Input\")\n",
    "#         for col in sorted(self.raw_data_type.columns):\n",
    "#             lines.append(f\"        {col}[{col}]\")\n",
    "#         lines.append(\"    end\")\n",
    "        \n",
    "#         for f in self.features:\n",
    "#             name = f.__class__.__name__\n",
    "#             if hasattr(f, 'period'):\n",
    "#                 name = f\"{name}_{f.period}\"\n",
    "            \n",
    "#             for req in f.required_cols():\n",
    "#                 lines.append(f\"    {req} --> {name}\")\n",
    "#             for out in f.output_cols():\n",
    "#                 lines.append(f\"    {name} --> {out}[{out}]\")\n",
    "        \n",
    "#         return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "import signalflow as sf\n",
    "\n",
    "\n",
    "pipeline = FeaturePipeline(\n",
    "    features=[\n",
    "        sf.feature.ExampleRsiFeature(period=140),\n",
    "        sf.feature.ExampleRsiFeature(period=210),\n",
    "        OffsetFeature(\n",
    "            feature_cls=sf.feature.ExampleRsiFeature,\n",
    "            feature_params={\"period\": 140},\n",
    "            window=60,\n",
    "            prefix=\"ofs_\",\n",
    "        ),\n",
    "        sf.feature.ExampleGlobalMeanRsiFeature(period=140),\n",
    "        sf.feature.ExampleGlobalMeanRsiFeature(period=210),\n",
    "    ],\n",
    "    raw_data_type=sf.RawDataType.SPOT,\n",
    ")\n",
    "\n",
    "df = pipeline.run(raw_data_view)\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"plotting.backend\", \"plotly\")\n",
    "df.filter(pl.col(\"pair\") == \"BTCUSDT\").select([\"rsi_140\", \"rsi_210\", \"global_mean_rsi_140\", \"global_mean_rsi_210\", \"ofs_rsi_140\"]).to_pandas().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6fc4e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (393_120, 14)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>pair</th><th>timestamp</th><th>open</th><th>high</th><th>low</th><th>close</th><th>volume</th><th>trades</th><th>rsi_140</th><th>rsi_210</th><th>ofs_rsi_140</th><th>offset</th><th>global_mean_rsi_140</th><th>global_mean_rsi_210</th></tr><tr><td>str</td><td>datetime[μs]</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>i32</td><td>f64</td><td>f64</td><td>f64</td><td>u8</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;BTCUSDT&quot;</td><td>2025-10-01 00:01:00</td><td>114048.94</td><td>114100.0</td><td>114048.93</td><td>114100.0</td><td>2.0881e6</td><td>1726</td><td>null</td><td>null</td><td>null</td><td>0</td><td>null</td><td>null</td></tr><tr><td>&quot;BTCUSDT&quot;</td><td>2025-10-01 00:02:00</td><td>114100.0</td><td>114222.95</td><td>114099.99</td><td>114172.0</td><td>2.3579e6</td><td>3957</td><td>null</td><td>null</td><td>null</td><td>1</td><td>null</td><td>null</td></tr><tr><td>&quot;BTCUSDT&quot;</td><td>2025-10-01 00:03:00</td><td>114171.99</td><td>114182.87</td><td>114145.1</td><td>114166.99</td><td>486037.318853</td><td>1466</td><td>null</td><td>null</td><td>null</td><td>2</td><td>null</td><td>null</td></tr><tr><td>&quot;BTCUSDT&quot;</td><td>2025-10-01 00:04:00</td><td>114167.0</td><td>114176.25</td><td>114149.09</td><td>114164.48</td><td>741897.103869</td><td>1436</td><td>null</td><td>null</td><td>null</td><td>3</td><td>null</td><td>null</td></tr><tr><td>&quot;BTCUSDT&quot;</td><td>2025-10-01 00:05:00</td><td>114164.48</td><td>114211.4</td><td>114158.34</td><td>114211.4</td><td>328877.124806</td><td>1147</td><td>null</td><td>null</td><td>null</td><td>4</td><td>null</td><td>null</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;SOLUSDT&quot;</td><td>2025-12-30 23:56:00</td><td>125.05</td><td>125.06</td><td>125.04</td><td>125.05</td><td>37020.21681</td><td>125</td><td>58.928571</td><td>52.205128</td><td>null</td><td>1</td><td>56.443025</td><td>52.600838</td></tr><tr><td>&quot;SOLUSDT&quot;</td><td>2025-12-30 23:57:00</td><td>125.06</td><td>125.14</td><td>125.05</td><td>125.13</td><td>91307.62098</td><td>359</td><td>59.292035</td><td>52.44898</td><td>null</td><td>2</td><td>56.770349</td><td>52.906793</td></tr><tr><td>&quot;SOLUSDT&quot;</td><td>2025-12-30 23:58:00</td><td>125.13</td><td>125.14</td><td>125.09</td><td>125.11</td><td>524228.40961</td><td>537</td><td>60.269865</td><td>52.82631</td><td>null</td><td>3</td><td>56.740598</td><td>53.329519</td></tr><tr><td>&quot;SOLUSDT&quot;</td><td>2025-12-30 23:59:00</td><td>125.1</td><td>125.11</td><td>125.05</td><td>125.07</td><td>56596.09677</td><td>274</td><td>60.36036</td><td>53.264249</td><td>null</td><td>4</td><td>57.011225</td><td>53.675403</td></tr><tr><td>&quot;SOLUSDT&quot;</td><td>2025-12-31 00:00:00</td><td>125.07</td><td>125.07</td><td>125.01</td><td>125.01</td><td>150753.14025</td><td>353</td><td>60.179641</td><td>52.989691</td><td>50.54277</td><td>5</td><td>56.549221</td><td>53.208524</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (393_120, 14)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬────────────┬────────┬────────────┬────────────┐\n",
       "│ pair    ┆ timestamp  ┆ open      ┆ high      ┆ … ┆ ofs_rsi_14 ┆ offset ┆ global_mea ┆ global_mea │\n",
       "│ ---     ┆ ---        ┆ ---       ┆ ---       ┆   ┆ 0          ┆ ---    ┆ n_rsi_140  ┆ n_rsi_210  │\n",
       "│ str     ┆ datetime[μ ┆ f64       ┆ f64       ┆   ┆ ---        ┆ u8     ┆ ---        ┆ ---        │\n",
       "│         ┆ s]         ┆           ┆           ┆   ┆ f64        ┆        ┆ f64        ┆ f64        │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪════════════╪════════╪════════════╪════════════╡\n",
       "│ BTCUSDT ┆ 2025-10-01 ┆ 114048.94 ┆ 114100.0  ┆ … ┆ null       ┆ 0      ┆ null       ┆ null       │\n",
       "│         ┆ 00:01:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ BTCUSDT ┆ 2025-10-01 ┆ 114100.0  ┆ 114222.95 ┆ … ┆ null       ┆ 1      ┆ null       ┆ null       │\n",
       "│         ┆ 00:02:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ BTCUSDT ┆ 2025-10-01 ┆ 114171.99 ┆ 114182.87 ┆ … ┆ null       ┆ 2      ┆ null       ┆ null       │\n",
       "│         ┆ 00:03:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ BTCUSDT ┆ 2025-10-01 ┆ 114167.0  ┆ 114176.25 ┆ … ┆ null       ┆ 3      ┆ null       ┆ null       │\n",
       "│         ┆ 00:04:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ BTCUSDT ┆ 2025-10-01 ┆ 114164.48 ┆ 114211.4  ┆ … ┆ null       ┆ 4      ┆ null       ┆ null       │\n",
       "│         ┆ 00:05:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ …       ┆ …          ┆ …         ┆ …         ┆ … ┆ …          ┆ …      ┆ …          ┆ …          │\n",
       "│ SOLUSDT ┆ 2025-12-30 ┆ 125.05    ┆ 125.06    ┆ … ┆ null       ┆ 1      ┆ 56.443025  ┆ 52.600838  │\n",
       "│         ┆ 23:56:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ SOLUSDT ┆ 2025-12-30 ┆ 125.06    ┆ 125.14    ┆ … ┆ null       ┆ 2      ┆ 56.770349  ┆ 52.906793  │\n",
       "│         ┆ 23:57:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ SOLUSDT ┆ 2025-12-30 ┆ 125.13    ┆ 125.14    ┆ … ┆ null       ┆ 3      ┆ 56.740598  ┆ 53.329519  │\n",
       "│         ┆ 23:58:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ SOLUSDT ┆ 2025-12-30 ┆ 125.1     ┆ 125.11    ┆ … ┆ null       ┆ 4      ┆ 57.011225  ┆ 53.675403  │\n",
       "│         ┆ 23:59:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "│ SOLUSDT ┆ 2025-12-31 ┆ 125.07    ┆ 125.07    ┆ … ┆ 50.54277   ┆ 5      ┆ 56.549221  ┆ 53.208524  │\n",
       "│         ┆ 00:00:00   ┆           ┆           ┆   ┆            ┆        ┆            ┆            │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴────────────┴────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6aba86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26401716",
   "metadata": {},
   "outputs": [],
   "source": [
    "abdlfds;f,dlsafgmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad1219e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import polars as pl\n",
    "\n",
    "@dataclass\n",
    "@sf.core.sf_component(name='custom/log_return', override=True)\n",
    "class LogReturnExtractor(sf.feature.FeatureExtractor):\n",
    "    \"\"\"Calculates logarithmic returns: ln(Pt / Pt-1).\"\"\"\n",
    "    \n",
    "    price_col: str = 'close'\n",
    "    out_col: str = 'log_ret'\n",
    "    period: int = 1\n",
    "\n",
    "    def compute_group(self, group_df: pl.DataFrame, data_context: dict | None) -> pl.DataFrame:\n",
    "        if self.price_col not in group_df.columns:\n",
    "            raise ValueError(f\"Missing required column: {self.price_col}\")\n",
    "\n",
    "        log_ret = (\n",
    "            pl.col(self.price_col)\n",
    "            .log()\n",
    "            .diff(n=self.period)\n",
    "            .alias(self.out_col)\n",
    "        )\n",
    "        \n",
    "        return group_df.with_columns(log_ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d60ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set = sf.feature.FeatureSet(\n",
    "    extractors=[\n",
    "        LogReturnExtractor(price_col=\"close\", out_col=\"log_ret_1\", period=1), \n",
    "        LogReturnExtractor(price_col=\"close\", out_col=\"log_ret_2\", period=2), \n",
    "        LogReturnExtractor(price_col=\"close\", out_col=\"log_ret_4\", period=4), \n",
    "        LogReturnExtractor(price_col=\"close\", out_col=\"log_ret_8\", period=8), \n",
    "        LogReturnExtractor(price_col=\"close\", out_col=\"log_ret_16\", period=16), \n",
    "        LogReturnExtractor(price_col=\"close\", out_col=\"log_ret_32\", period=32), \n",
    "    ]\n",
    ")\n",
    "features_df = feature_set.extract(raw_data_view)\n",
    "features_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52874e5",
   "metadata": {},
   "source": [
    "## DETECTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45381502",
   "metadata": {},
   "source": [
    "### Custom Signal Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8995f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "@sf.core.sf_component(name='momentum_breakout', override=True)\n",
    "class MomentumDetector(sf.detector.SignalDetector):\n",
    "    \"\"\"Detects large price moves based on log return thresholds.\"\"\"\n",
    "    \n",
    "    threshold: float = 0.02 \n",
    "    feature_col: str = 'log_ret'\n",
    "    price_col: str = 'close'\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.feature_set = sf.feature.FeatureSet(extractors=[\n",
    "            LogReturnExtractor(\n",
    "                price_col=self.price_col, \n",
    "                out_col=self.feature_col,\n",
    "                period=1\n",
    "            )\n",
    "        ])\n",
    "\n",
    "    def detect(self, features: pl.DataFrame, context: dict | None = None) -> sf.core.Signals:\n",
    "        feat = pl.col(self.feature_col)\n",
    "        \n",
    "        signal_type = (\n",
    "            pl.when(feat > self.threshold).then(pl.lit(sf.core.SignalType.RISE.value))\n",
    "            .when(feat < -self.threshold).then(pl.lit(sf.core.SignalType.FALL.value))\n",
    "            .otherwise(pl.lit(sf.core.SignalType.NONE.value))\n",
    "            .alias('signal_type')\n",
    "        )\n",
    "        \n",
    "        signal_val = (\n",
    "            pl.when(feat > self.threshold).then(1)\n",
    "            .when(feat < -self.threshold).then(-1)\n",
    "            .otherwise(0)\n",
    "            .alias('signal')\n",
    "        )\n",
    "\n",
    "        df = features.with_columns([signal_type, signal_val])\n",
    "        \n",
    "        out = df.filter(pl.col('signal_type') != sf.core.SignalType.NONE.value)\n",
    "        \n",
    "        return sf.core.Signals(out.select([self.pair_col, self.ts_col, 'signal_type', 'signal']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a2fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detector = MomentumDetector(threshold=0.01)\n",
    "\n",
    "print(\"--- Running Pipeline ---\")\n",
    "signals = detector.run(raw_data_view)\n",
    "\n",
    "print(f\"\\nDetected {signals.value.height} signals:\")\n",
    "display(signals.value)\n",
    "\n",
    "print(\"\\nVerifying with Source Data:\")\n",
    "dates_of_interest = signals.value.get_column(\"timestamp\")\n",
    "spot_df = raw_data.get(\"spot\")\n",
    "\n",
    "verification = spot_df.filter(pl.col(\"timestamp\").is_in(dates_of_interest))\n",
    "display(verification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c95dae",
   "metadata": {},
   "source": [
    "### Result ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe57f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import plotly.io as pio\n",
    "\n",
    "def plot_signals(raw_df: pl.DataFrame, signals_df: pl.DataFrame, pair: str = \"BTCUSDT\"):\n",
    "\n",
    "    price_data = raw_df.select([\"timestamp\", \"pair\", \"close\"]).filter(pl.col(\"pair\") == pair)\n",
    "    sig_data = signals_df.select([\"timestamp\", \"pair\", \"signal\"]).filter(pl.col(\"pair\") == pair)\n",
    "\n",
    "    price_data = price_data.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "    sig_data = sig_data.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "\n",
    "    df_plot = price_data.sort(\"timestamp\").to_pandas()\n",
    "    signals_with_price = sig_data.join(price_data, on=[\"timestamp\", \"pair\"], how=\"inner\")\n",
    "    sig_plot = signals_with_price.to_pandas()\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_plot['timestamp'], \n",
    "        y=df_plot['close'], \n",
    "        mode='lines', \n",
    "        name=f'{pair} Price',\n",
    "        line=dict(color='#2E86C1', width=1.5) \n",
    "    ))\n",
    "\n",
    "    buys = sig_plot[sig_plot['signal'] == 1]\n",
    "    if not buys.empty:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=buys['timestamp'],\n",
    "            y=buys['close'],\n",
    "            mode='markers',\n",
    "            name='Buy Signal',\n",
    "            marker=dict(symbol='triangle-up', size=13, color='#00CC96', line=dict(width=1, color='black'))\n",
    "        ))\n",
    "\n",
    "    sells = sig_plot[sig_plot['signal'] == -1]\n",
    "    if not sells.empty:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=sells['timestamp'],\n",
    "            y=sells['close'],\n",
    "            mode='markers',\n",
    "            name='Sell Signal',\n",
    "            marker=dict(symbol='triangle-down', size=13, color='#EF553B', line=dict(width=1, color='black'))\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=f'SignalFlow: {pair} Analysis', font=dict(color='black')),\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Price',\n",
    "        template='plotly_white', \n",
    "        hovermode='x unified',\n",
    "        height=600,\n",
    "        xaxis=dict(showgrid=True, gridcolor='lightgray'),\n",
    "        yaxis=dict(showgrid=True, gridcolor='lightgray')\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "spot_df = raw_data.get(\"spot\")\n",
    "fig = plot_signals(spot_df, signals.value, pair=\"ETHUSDT\")\n",
    "fig.write_image(\"spot_signals.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5348f9",
   "metadata": {},
   "source": [
    "## VALIDATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46661fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from signalflow.target import FixedHorizonLabeler\n",
    "\n",
    "\n",
    "labeler = FixedHorizonLabeler(\n",
    "    price_col='close', \n",
    "    horizon=10, \n",
    "    include_meta=True \n",
    ")\n",
    "labeled_df = labeler.compute(\n",
    "    df=raw_data_view.to_polars(\"spot\"),\n",
    "    signals=signals \n",
    ")\n",
    "\n",
    "print(\"Labeled Data Sample (Signals only):\")\n",
    "labeled_signals = labeled_df.filter(pl.col(\"label\") != \"none\")\n",
    "display(labeled_signals.select([\"timestamp\", \"label\", \"ret\"]).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955a8480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.validator import SklearnSignalValidator\n",
    "\n",
    "validator = SklearnSignalValidator(\n",
    "    model_type='random_forest',\n",
    "    model_params={'n_estimators': 100, 'max_depth': 5, 'random_state': 42}\n",
    ")\n",
    "\n",
    "\n",
    "features_df = detector.preprocess(raw_data_view)\n",
    "\n",
    "split_idx = int(features_df.height * 0.8)\n",
    "X_train = features_df.slice(0, split_idx)\n",
    "X_test = features_df.slice(split_idx, None)\n",
    "\n",
    "y = labeled_df.select(\"label\")\n",
    "y_train = y.slice(0, split_idx)\n",
    "y_test = y.slice(split_idx, None)\n",
    "\n",
    "print(\"--- Training Validator ---\")\n",
    "validator.fit(X_train, y_train)\n",
    "print(\"Model trained successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6706100",
   "metadata": {},
   "outputs": [],
   "source": [
    "validated_signals = validator.validate_signals(signals, features_df)\n",
    "\n",
    "print(\"\\nValidated Signals (with probabilities):\")\n",
    "display(\n",
    "    validated_signals.value\n",
    "    .select([\n",
    "        \"timestamp\", \n",
    "        \"signal_type\", \n",
    "        \"probability_rise\", \n",
    "        \"probability_fall\"\n",
    "    ])\n",
    "    .sort(\"probability_rise\", descending=True)\n",
    "    .head()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2f94f3",
   "metadata": {},
   "source": [
    "### Visualizing Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceba48d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import polars as pl\n",
    "import signalflow as sf \n",
    "\n",
    "def plot_strategy_complete(raw_df: pl.DataFrame, val_signals: sf.core.Signals, pair: str = \"BTCUSDT\"):\n",
    "    raw_sorted = raw_df.filter(pl.col(\"pair\") == pair).sort(\"timestamp\")\n",
    "    raw_sorted = raw_sorted.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "    df_plot = raw_sorted.to_pandas()\n",
    "    \n",
    "    sig_data = val_signals.value.filter(pl.col(\"pair\") == pair)\n",
    "    sig_data = sig_data.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "    \n",
    "    merged = sig_data.join(raw_sorted.select([\"timestamp\", \"close\"]), on=\"timestamp\", how=\"inner\").to_pandas()\n",
    "\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=df_plot['timestamp'], \n",
    "        y=df_plot['close'], \n",
    "        mode='lines', \n",
    "        name='Price', \n",
    "        line=dict(color='#2962FF', width=1.5) \n",
    "    ))\n",
    "\n",
    "\n",
    "    buys = merged[merged['signal_type'] == 'rise']\n",
    "    \n",
    "    if not buys.empty:\n",
    "        buys_noise = buys[buys['probability_rise'] < 0.5]\n",
    "        if not buys_noise.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=buys_noise['timestamp'], y=buys_noise['close'],\n",
    "                mode='markers', name='Buy (Ignored)',\n",
    "                marker=dict(\n",
    "                    symbol='triangle-up', \n",
    "                    size=8, \n",
    "                    color='#B0BEC5', \n",
    "                    line=dict(width=1, color='gray')\n",
    "                ),\n",
    "                hovertemplate=\"<b>Buy (Ignored)</b><br>Price: %{y:.2f}<br>Conf: %{text}<extra></extra>\",\n",
    "                text=[f\"{p:.2f}\" for p in buys_noise['probability_rise']]\n",
    "            ))\n",
    "\n",
    "        buys_signal = buys[buys['probability_rise'] >= 0.5]\n",
    "        if not buys_signal.empty:\n",
    "            sizes = 12 + (buys_signal['probability_rise'] * 15)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=buys_signal['timestamp'], y=buys_signal['close'],\n",
    "                mode='markers', name='Buy (Active)',\n",
    "                marker=dict(\n",
    "                    symbol='triangle-up', \n",
    "                    size=sizes, \n",
    "                    color=buys_signal['probability_rise'],\n",
    "                    colorscale=[[0, '#B9F6CA'], [1, '#00C853']], \n",
    "                    cmin=0.5, cmax=1.0, \n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title=\"Buy Conf\", x=1.02, len=0.4, y=0.8),\n",
    "                    line=dict(width=1, color='black')\n",
    "                ),\n",
    "                hovertemplate=\"<b>Buy (Active)</b><br>Price: %{y:.2f}<br>Conf: %{text}<extra></extra>\",\n",
    "                text=[f\"{p:.2f}\" for p in buys_signal['probability_rise']]\n",
    "            ))\n",
    "\n",
    "\n",
    "    sells = merged[merged['signal_type'] == 'fall']\n",
    "    \n",
    "    if not sells.empty:\n",
    "        sells_noise = sells[sells['probability_fall'] < 0.5]\n",
    "        if not sells_noise.empty:\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=sells_noise['timestamp'], y=sells_noise['close'],\n",
    "                mode='markers', name='Sell (Ignored)',\n",
    "                marker=dict(\n",
    "                    symbol='triangle-down', \n",
    "                    size=8, \n",
    "                    color='#B0BEC5', \n",
    "                    line=dict(width=1, color='gray')\n",
    "                ),\n",
    "                hovertemplate=\"<b>Sell (Ignored)</b><br>Price: %{y:.2f}<br>Conf: %{text}<extra></extra>\",\n",
    "                text=[f\"{p:.2f}\" for p in sells_noise['probability_fall']]\n",
    "            ))\n",
    "\n",
    "        sells_signal = sells[sells['probability_fall'] >= 0.5]\n",
    "        if not sells_signal.empty:\n",
    "            sizes = 12 + (sells_signal['probability_fall'] * 15)\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=sells_signal['timestamp'], y=sells_signal['close'],\n",
    "                mode='markers', name='Sell (Active)',\n",
    "                marker=dict(\n",
    "                    symbol='triangle-down', \n",
    "                    size=sizes, \n",
    "                    color=sells_signal['probability_fall'],\n",
    "                    colorscale=[[0, '#EF9A9A'], [1, '#C62828']], \n",
    "                    cmin=0.5, cmax=1.0,\n",
    "                    showscale=True,\n",
    "                    colorbar=dict(title=\"Sell Conf\", x=1.02, len=0.4, y=0.2),\n",
    "                    line=dict(width=1, color='black')\n",
    "                ),\n",
    "                hovertemplate=\"<b>Sell (Active)</b><br>Price: %{y:.2f}<br>Conf: %{text}<extra></extra>\",\n",
    "                text=[f\"{p:.2f}\" for p in sells_signal['probability_fall']]\n",
    "            ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=dict(text=f'AI Strategy Analysis: {pair}', font=dict(size=20, color='#333')),\n",
    "        template='plotly_white',\n",
    "        height=700,\n",
    "        hovermode='x unified',\n",
    "        xaxis=dict(showgrid=True, gridcolor='#eceff1'),\n",
    "        yaxis=dict(showgrid=True, gridcolor='#eceff1'),\n",
    "        legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"left\", x=0)\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "plot_strategy_complete(raw_data_view.to_polars(\"spot\"), validated_signals, pair=\"SOLUSDT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbee15b",
   "metadata": {},
   "source": [
    "## STRATEGY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2ee400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.strategy.broker import BacktestBroker\n",
    "from signalflow.strategy.broker.executor import VirtualSpotExecutor\n",
    "from signalflow.data.strategy_store import DuckDbStrategyStore\n",
    "from signalflow.strategy.runner import BacktestRunner, OptimizedBacktestRunner\n",
    "from signalflow.strategy.component.entry import SignalEntryRule\n",
    "from signalflow.strategy.component.exit import TakeProfitStopLossExit\n",
    "from signalflow.analytic.strategy import TotalReturnMetric, BalanceAllocationMetric, DrawdownMetric, WinRateMetric, SharpeRatioMetric\n",
    "\n",
    "strategy_store = DuckDbStrategyStore(\"strategy.duckdb\")\n",
    "strategy_store.init()\n",
    "\n",
    "executor = VirtualSpotExecutor(fee_rate=0.001, slippage_pct=0.001) \n",
    "broker = BacktestBroker(executor=executor, store=strategy_store)\n",
    "\n",
    "entry_rule = SignalEntryRule(\n",
    "    base_position_size=1000.0,  \n",
    "    use_probability_sizing=True, \n",
    "    min_probability=0.6,         \n",
    "    max_positions_per_pair=1,\n",
    "    allow_shorts=False        \n",
    ")\n",
    "\n",
    "exit_rule = TakeProfitStopLossExit(\n",
    "    take_profit_pct=0.02, \n",
    "    stop_loss_pct=0.02    \n",
    ")\n",
    "\n",
    "metrics=[\n",
    "        TotalReturnMetric(initial_capital=10000.0),\n",
    "        BalanceAllocationMetric(initial_capital=10000.0),\n",
    "        DrawdownMetric(),\n",
    "        WinRateMetric(),\n",
    "        SharpeRatioMetric(initial_capital=10000.0, window_size=100)\n",
    "]\n",
    "\n",
    "runner = OptimizedBacktestRunner(\n",
    "    strategy_id=\"ml_momentum_strategy\",\n",
    "    broker=broker,\n",
    "    entry_rules=[entry_rule],\n",
    "    exit_rules=[exit_rule],\n",
    "    metrics=metrics,\n",
    "    initial_capital=10000.0,\n",
    "    data_key=\"spot\" \n",
    ")\n",
    "\n",
    "print(\"--- Running Backtest ---\")\n",
    "final_state = runner.run(raw_data, validated_signals)\n",
    "\n",
    "results = runner.get_results()\n",
    "print(f\"\\nFinal Equity: ${results['final_equity']:.2f}\")\n",
    "print(f\"Total Return: {results['final_return']*100:.2f}%\")\n",
    "print(f\"Trades Executed: {results['total_trades']}\")\n",
    "\n",
    "\n",
    "print(\"Recent Trades:\")\n",
    "display(results['trades_df'].tail(10))\n",
    "\n",
    "metrics_df = results['metrics_df']\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203867a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "def plot_backtest_performance(results: dict):\n",
    "\n",
    "    metrics_df = results.get('metrics_df')\n",
    "    \n",
    "    if metrics_df is None or metrics_df.height == 0:\n",
    "        print(\"No metrics to plot\")\n",
    "        return\n",
    "    \n",
    "    if 'timestamp' in metrics_df.columns:\n",
    "        timestamps = metrics_df.select(\n",
    "            pl.from_epoch(pl.col('timestamp').cast(pl.Int64), time_unit='s').alias('datetime')\n",
    "        ).get_column('datetime').to_list()\n",
    "    else:\n",
    "        timestamps = list(range(metrics_df.height))\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.06,\n",
    "        subplot_titles=(\n",
    "            'Strategy Performance', \n",
    "            'Position Metrics',\n",
    "            'Balance Allocation'\n",
    "        ),\n",
    "        row_heights=[0.45, 0.30, 0.25]\n",
    "    )\n",
    "\n",
    "    \n",
    "    if 'total_return' in metrics_df.columns:\n",
    "        returns_pct = (metrics_df.get_column('total_return') * 100).to_list()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=returns_pct,\n",
    "                mode='lines',\n",
    "                name='Strategy Return',\n",
    "                line=dict(color='#1E88E5', width=2.5),\n",
    "                hovertemplate='Return: %{y:.2f}%<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    fig.add_hline(\n",
    "        y=0, \n",
    "        line_dash=\"dash\", \n",
    "        line_color=\"#9E9E9E\", \n",
    "        line_width=1,\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "    if 'open_positions' in metrics_df.columns:\n",
    "        open_pos = metrics_df.get_column('open_positions').to_list()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=open_pos,\n",
    "                mode='lines',\n",
    "                name='Open Positions',\n",
    "                line=dict(color='#43A047', width=2),\n",
    "                fill='tozeroy',\n",
    "                fillcolor='rgba(67, 160, 71, 0.15)',\n",
    "                hovertemplate='Open: %{y}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    if 'closed_positions' in metrics_df.columns:\n",
    "        closed_pos = metrics_df.get_column('closed_positions').to_list()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=closed_pos,\n",
    "                mode='lines',\n",
    "                name='Closed Positions',\n",
    "                line=dict(color='#8E24AA', width=1.5, dash='dot'),\n",
    "                hovertemplate='Closed: %{y}<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "\n",
    "    if 'cash' in metrics_df.columns and 'equity' in metrics_df.columns:\n",
    "        cash = metrics_df.get_column('cash').to_list()\n",
    "        equity = metrics_df.get_column('equity').to_list()\n",
    "        \n",
    "        initial_capital = results.get('initial_capital', equity[0] if equity else 10000)\n",
    "        \n",
    "        allocated_pct = [(eq - c) / eq if eq > 0 else 0 for eq, c in zip(equity, cash)]\n",
    "        free_pct = [c / eq if eq > 0 else 0 for eq, c in zip(equity, cash)]\n",
    "        \n",
    "        total_balance_pct = [(eq / initial_capital - 1) * 100 for eq in equity]\n",
    "        \n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=free_pct,\n",
    "                mode='lines',\n",
    "                name='Free Cash',\n",
    "                line=dict(width=0),\n",
    "                fillcolor='rgba(100, 181, 246, 0.6)', \n",
    "                fill='tozeroy',\n",
    "                stackgroup='balance',\n",
    "                hovertemplate='Free: %{y:.1%}<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=allocated_pct,\n",
    "                mode='lines',\n",
    "                name='In Positions',\n",
    "                line=dict(width=0),\n",
    "                fillcolor='rgba(25, 118, 210, 0.7)',  \n",
    "                fill='tonexty',\n",
    "                stackgroup='balance',\n",
    "                hovertemplate='Allocated: %{y:.1%}<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=total_balance_pct,\n",
    "                mode='lines',\n",
    "                name='Total Return',\n",
    "                line=dict(color='#1565C0', width=2.5),\n",
    "                yaxis='y4', \n",
    "                hovertemplate='Total: %{y:.2f}%<extra></extra>'\n",
    "            ),\n",
    "            row=3, col=1\n",
    "        )\n",
    " \n",
    "    final_return = results.get('final_return', 0) * 100\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=f'Backtest Results | Total Return: {final_return:.2f}%',\n",
    "            font=dict(size=18, color='#212121')\n",
    "        ),\n",
    "        template='plotly_white',\n",
    "        height=900,\n",
    "        hovermode='x unified',\n",
    "        legend=dict(\n",
    "            orientation='h',\n",
    "            yanchor='bottom',\n",
    "            y=1.01,\n",
    "            xanchor='left',\n",
    "            x=0,\n",
    "            font=dict(size=11)\n",
    "        ),\n",
    "        showlegend=True,\n",
    "        plot_bgcolor='#FAFAFA',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text='Date', row=3, col=1)\n",
    "    fig.update_yaxes(title_text='Return (%)', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Count', row=2, col=1)\n",
    "    fig.update_yaxes(\n",
    "        title_text='Allocation', \n",
    "        row=3, col=1, \n",
    "        tickformat='.0%',\n",
    "        range=[0, 1] \n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        yaxis4=dict(\n",
    "            title='Total Return (%)',\n",
    "            overlaying='y3',\n",
    "            side='right',\n",
    "            showgrid=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(\n",
    "        showgrid=True, \n",
    "        gridwidth=1, \n",
    "        gridcolor='#E0E0E0',\n",
    "        showline=True,\n",
    "        linewidth=1,\n",
    "        linecolor='#BDBDBD'\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        showgrid=True, \n",
    "        gridwidth=1, \n",
    "        gridcolor='#E0E0E0',\n",
    "        showline=True,\n",
    "        linewidth=1,\n",
    "        linecolor='#BDBDBD'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_detailed_metrics(results: dict):\n",
    "\n",
    "    metrics_df = results.get('metrics_df')\n",
    "    \n",
    "    if metrics_df is None or metrics_df.height == 0:\n",
    "        print(\"No metrics to plot\")\n",
    "        return\n",
    "    \n",
    "    if 'timestamp' in metrics_df.columns:\n",
    "        timestamps = metrics_df.select(\n",
    "            pl.from_epoch(pl.col('timestamp').cast(pl.Int64), time_unit='s').alias('datetime')\n",
    "        ).get_column('datetime').to_list()\n",
    "    else:\n",
    "        timestamps = list(range(metrics_df.height))\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.08,\n",
    "        subplot_titles=('Drawdown Analysis', 'Capital Utilization'),\n",
    "        row_heights=[0.6, 0.4]\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    if 'current_drawdown' in metrics_df.columns:\n",
    "        drawdown = metrics_df.get_column('current_drawdown').to_list()\n",
    "        drawdown_pct = [-d * 100 for d in drawdown]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=drawdown_pct,\n",
    "                mode='lines',\n",
    "                name='Drawdown',\n",
    "                line=dict(color='#E53935', width=2),\n",
    "                fill='tozeroy',\n",
    "                fillcolor='rgba(229, 57, 53, 0.2)',\n",
    "                hovertemplate='DD: %{y:.2f}%<extra></extra>'\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        max_dd = results.get('max_drawdown', 0) * 100\n",
    "        if max_dd > 0:\n",
    "            fig.add_hline(\n",
    "                y=-max_dd,\n",
    "                line_dash=\"dash\",\n",
    "                line_color=\"#C62828\",\n",
    "                line_width=1.5,\n",
    "                annotation_text=f\"Max DD: {max_dd:.2f}%\",\n",
    "                annotation_position=\"right\",\n",
    "                row=1, col=1\n",
    "            )\n",
    "    \n",
    "    \n",
    "    if 'capital_utilization' in metrics_df.columns:\n",
    "        util = metrics_df.get_column('capital_utilization').to_list()\n",
    "        util_pct = [u * 100 for u in util]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=timestamps,\n",
    "                y=util_pct,\n",
    "                mode='lines',\n",
    "                name='Capital Utilization',\n",
    "                line=dict(color='#FB8C00', width=2),\n",
    "                fill='tozeroy',\n",
    "                fillcolor='rgba(251, 140, 0, 0.15)',\n",
    "                hovertemplate='Util: %{y:.1f}%<extra></extra>'\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_hline(\n",
    "            y=100,\n",
    "            line_dash=\"dot\",\n",
    "            line_color=\"#9E9E9E\",\n",
    "            line_width=1,\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        height=600,\n",
    "        hovermode='x unified',\n",
    "        showlegend=True,\n",
    "        plot_bgcolor='#FAFAFA',\n",
    "        paper_bgcolor='white'\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title_text='Drawdown (%)', row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Utilization (%)', row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Date', row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='#E0E0E0')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='#E0E0E0')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "def print_performance_summary(results: dict):\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"📊 BACKTEST PERFORMANCE SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Performance metrics\n",
    "    final_return = results.get('final_return', 0) * 100\n",
    "    max_dd = results.get('max_drawdown', 0) * 100\n",
    "    sharpe = results.get('sharpe_ratio', 0)\n",
    "    \n",
    "    print(f\"\\n💰 Returns:\")\n",
    "    print(f\"   Total Return:     {final_return:>8.2f}%\")\n",
    "    print(f\"   Max Drawdown:     {max_dd:>8.2f}%\")\n",
    "    print(f\"   Sharpe Ratio:     {sharpe:>8.3f}\")\n",
    "    \n",
    "    if abs(max_dd) > 0.01:\n",
    "        calmar = final_return / abs(max_dd)\n",
    "        print(f\"   Calmar Ratio:     {calmar:>8.3f}\")\n",
    "    \n",
    "    total_trades = results.get('total_trades', 0)\n",
    "    entry_count = results.get('entry_count', 0)\n",
    "    exit_count = results.get('exit_count', 0)\n",
    "    win_rate = results.get('win_rate', 0) * 100 if 'win_rate' in results else 0\n",
    "    \n",
    "    print(f\"\\n📈 Trading Activity:\")\n",
    "    print(f\"   Total Trades:     {total_trades:>8}\")\n",
    "    print(f\"   Entry Trades:     {entry_count:>8}\")\n",
    "    print(f\"   Exit Trades:      {exit_count:>8}\")\n",
    "    if win_rate > 0:\n",
    "        print(f\"   Win Rate:         {win_rate:>8.1f}%\")\n",
    "    \n",
    "    final_equity = results.get('final_equity', 0)\n",
    "    initial_capital = results.get('initial_capital', 10000)\n",
    "    \n",
    "    print(f\"\\n💵 Capital:\")\n",
    "    print(f\"   Initial:          ${initial_capital:>12,.2f}\")\n",
    "    print(f\"   Final:            ${final_equity:>12,.2f}\")\n",
    "    print(f\"   Profit/Loss:      ${final_equity - initial_capital:>12,.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "results = runner.get_results()\n",
    "\n",
    "print_performance_summary(results)\n",
    "\n",
    "fig1 = plot_backtest_performance(results)\n",
    "pio.write_image(fig1, \"backtest_performance.png\") \n",
    "\n",
    "fig1.show()\n",
    "\n",
    "fig2 = plot_detailed_metrics(results)\n",
    "pio.write_image(fig2, \"detailed_metrics.png\") \n",
    "\n",
    "fig2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signalflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
