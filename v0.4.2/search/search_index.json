{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SignalFlow - High-Performance Algorithmic Trading Framework","text":"<p>Current stable version: 0.4.2</p> <p>SignalFlow is a high-performance Python framework for algorithmic trading, quantitative finance, and machine learning-based trading strategies.</p> <ul> <li> <p> High Performance</p> <p>Powered by Polars for blazing-fast processing of large datasets (100+ trading pairs, 500k+ candles)</p> </li> <li> <p> Modular Design</p> <p>Component registry system with pluggable detectors, validators, features, and strategies</p> </li> <li> <p> Production Ready</p> <p>Seamless transition from research to production with unified backtesting and live trading interfaces</p> </li> <li> <p> ML-Powered</p> <p>Built-in support for scikit-learn, XGBoost, LightGBM, and PyTorch-based signal validation</p> </li> </ul>"},{"location":"#the-signal-processing-pipeline","title":"The Signal Processing Pipeline","text":"<p>SignalFlow organizes algorithmic trading into four distinct stages:</p> <pre><code>flowchart LR\n    A[Market Data] --&gt; B[Signal Detection]\n    B --&gt; C[Signal Validation]\n    C --&gt; D[Strategy Execution]\n\n    style A fill:#2563eb,stroke:#3b82f6,stroke-width:2px,color:#fff\n    style B fill:#ea580c,stroke:#f97316,stroke-width:2px,color:#fff\n    style C fill:#16a34a,stroke:#22c55e,stroke-width:2px,color:#fff\n    style D fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#fff</code></pre> <p>SignalFlow implements a four-stage algorithmic trading pipeline: market data ingestion, signal detection, machine learning-based signal validation, and strategy execution with risk management.</p>"},{"location":"#1-data-features","title":"1. Data &amp; Features","text":"<p>Load and process market data with efficient storage and feature engineering:</p> <ul> <li>Flexible data loaders (Binance Spot, Futures, custom sources)</li> <li>DuckDB and Parquet storage backends</li> <li>Technical indicators via pandas-ta and custom Polars extractors</li> </ul>"},{"location":"#2-signal-detection","title":"2. Signal Detection","text":"<p>Identify potential trading opportunities from market patterns:</p> <ul> <li>Classical algorithms (SMA crossover, MACD, RSI thresholds)</li> <li>Pattern recognition (candlestick patterns, chart formations)</li> <li>Neural network outputs (CNN, LSTM, Transformer predictions)</li> </ul>"},{"location":"#3-signal-validation-meta-labeling","title":"3. Signal Validation (Meta-Labeling)","text":"<p>Filter signals using machine learning to predict success probability:</p> <ul> <li>Implements Lopez de Prado's meta-labeling methodology</li> <li>Support for scikit-learn, XGBoost, LightGBM classifiers</li> <li>Deep learning validators via PyTorch Lightning (signalflow-nn)</li> </ul>"},{"location":"#4-strategy-execution","title":"4. Strategy Execution","text":"<p>Convert validated signals into trades with risk management:</p> <ul> <li>Entry/exit rules:</li> <li>Entry: Signal-based, model-driven, fixed-size</li> <li>Exit: Take-profit/stop-loss, trailing stops, volatility-based, composite</li> <li>Advanced position sizing: Kelly Criterion, volatility targeting, risk parity, martingale/grid</li> <li>Entry filters: Regime, volatility, drawdown, correlation, time-of-day, price distance, signal accuracy</li> <li>Signal aggregation: Combine multiple detectors with voting modes (majority, weighted, unanimous, meta-labeling)</li> <li>Real-time execution: Paper trading with <code>RealtimeRunner</code> and monitoring/alerts</li> <li>ML/RL integration: External model support via <code>StrategyModel</code> protocol</li> <li>Unified interface for backtesting, paper trading, and live trading</li> </ul>"},{"location":"#real-time-trading-monitoring","title":"Real-Time Trading &amp; Monitoring","text":"<p>SignalFlow v0.3.7+ includes production-ready infrastructure for paper and live trading:</p>"},{"location":"#paper-trading-virtual-trading","title":"Paper Trading (Virtual Trading)","text":"<ul> <li>RealtimeRunner: Async event loop for real-time bar processing</li> <li>VirtualRealtimeBroker: Simulated execution with order/fill logging</li> <li>Data Sync: Automatic background data updates from exchanges</li> <li>State Persistence: Crash recovery with checkpoint restoration</li> </ul>"},{"location":"#monitoring-alerts","title":"Monitoring &amp; Alerts","text":"<ul> <li>MaxDrawdownAlert: Trigger on excessive drawdown</li> <li>NoSignalsAlert: Detect signal generation failures</li> <li>StuckPositionAlert: Alert on positions not exiting</li> <li>Custom Metrics: Track equity, win rate, Sharpe ratio in real-time</li> </ul>"},{"location":"#external-model-integration","title":"External Model Integration","text":"<ul> <li>StrategyModel Protocol: Integrate ML/RL models for automated decisions</li> <li>ModelEntryRule / ModelExitRule: Model-driven entry and exit logic</li> <li>BacktestExporter: Export training data (Parquet) for model development</li> <li>Decision Caching: Single model call per bar for consistent decisions</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from datetime import datetime\nfrom pathlib import Path\n\nfrom signalflow.data.source import VirtualDataProvider\nfrom signalflow.data.raw_store import DuckDbSpotStore\nfrom signalflow.data import RawDataFactory\nfrom signalflow.detector import ExampleSmaCrossDetector\nfrom signalflow.strategy.runner import BacktestRunner\nfrom signalflow.strategy.component.entry.signal import SignalEntryRule\nfrom signalflow.strategy.component.exit.tp_sl import TakeProfitStopLossExit\nfrom signalflow.strategy.broker import BacktestBroker\nfrom signalflow.strategy.broker.executor import VirtualSpotExecutor\n\n# Generate synthetic data (no API keys needed)\nstore = DuckDbSpotStore(db_path=Path(\"data.duckdb\"))\nVirtualDataProvider(store=store, seed=42).download(\n    pairs=[\"BTCUSDT\"], n_bars=10_000\n)\n\n# Load data\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data.duckdb\"),\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2020, 1, 1),\n    end=datetime(2030, 1, 1),\n)\n\n# Detect SMA crossover signals\ndetector = ExampleSmaCrossDetector(fast_period=20, slow_period=50)\nsignals = detector.run(raw_data.view())\n\n# Backtest with entry/exit rules\nrunner = BacktestRunner(\n    strategy_id=\"quickstart\",\n    broker=BacktestBroker(executor=VirtualSpotExecutor(fee_rate=0.001)),\n    entry_rules=[SignalEntryRule(base_position_size=100.0, use_probability_sizing=False)],\n    exit_rules=[TakeProfitStopLossExit(take_profit_pct=0.02, stop_loss_pct=0.01)],\n    initial_capital=10_000.0,\n)\nstate = runner.run(raw_data=raw_data, signals=signals)\nprint(f\"Trades: {len(runner.trades)}, Final capital: ${state.capital:.2f}\")\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#polars-first-performance","title":"Polars-First Performance","text":"<p>Core data processing uses Polars for extreme efficiency on large datasets, with seamless Pandas compatibility for prototyping.</p>"},{"location":"#component-registry","title":"Component Registry","text":"<p>All components (detectors, validators, features) are registered via <code>@sf_component</code> decorator for easy customization:</p> <pre><code>from signalflow.core import sf_component, SignalDetector\n\n@sf_component(name=\"my_detector\")\nclass CustomDetector(SignalDetector):\n    def detect(self, data):\n        # Your logic here\n        return signals\n</code></pre>"},{"location":"#advanced-labeling","title":"Advanced Labeling","text":"<p>Built-in support for sophisticated labeling strategies:</p> <ul> <li>Triple Barrier Method: Combines take-profit, stop-loss, and time barriers</li> <li>Fixed Horizon: Label signals based on future returns</li> <li>Signal masking: Label only signal timestamps for efficient ML training</li> <li>Numba-accelerated for performance (45s \u2192 0.3s on large datasets)</li> </ul>"},{"location":"#199-technical-indicators","title":"199+ Technical Indicators","text":"<p>The signalflow-ta extension provides production-grade technical analysis:</p> <ul> <li>Momentum, Overlap, Volatility, Volume, Trend, Statistics</li> <li>Physics-based indicators (energy, viscosity, impedance analogs)</li> <li>Preset pipeline factories for rapid feature engineering</li> </ul>"},{"location":"#paper-trading-monitoring","title":"Paper Trading &amp; Monitoring","text":"<p>Production-ready infrastructure for risk-free validation:</p> <ul> <li>RealtimeRunner: Async loop with automatic data sync and signal processing</li> <li>VirtualRealtimeBroker: Simulated execution with comprehensive logging</li> <li>Alert System: Real-time monitoring of drawdown, positions, and signal quality</li> <li>Crash Recovery: State persistence and automatic restoration</li> </ul>"},{"location":"#mlrl-model-integration","title":"ML/RL Model Integration","text":"<p>Protocol-based external model support:</p> <ul> <li>StrategyModel Protocol: Clean interface for ML/RL decision models</li> <li>Model Rules: <code>ModelEntryRule</code> and <code>ModelExitRule</code> for automated trading</li> <li>Training Export: Parquet export of backtest data for model development</li> <li>Decision Caching: Consistent model decisions across entry/exit phases</li> </ul>"},{"location":"#technology-stack","title":"Technology Stack","text":"Data ProcessingMachine LearningTrading ToolsInfrastructure <ul> <li>Polars - High-performance DataFrames</li> <li>Pandas - Legacy compatibility &amp; prototyping</li> <li>DuckDB - Embedded analytics database</li> <li>NumPy - Numerical computing</li> </ul> <ul> <li>scikit-learn - Classical ML models</li> <li>XGBoost - Gradient boosting</li> <li>LightGBM - Fast gradient boosting</li> <li>PyTorch - Deep learning framework</li> <li>Lightning - PyTorch training framework</li> <li>Optuna - Hyperparameter optimization</li> </ul> <ul> <li>signalflow-ta - 199+ technical indicators</li> <li>pandas-ta - Technical analysis foundation</li> <li>Numba - JIT compilation for speed</li> <li>Plotly - Interactive visualizations</li> </ul> <ul> <li>DuckDB - Local data storage</li> <li>SQLite / PostgreSQL - Alternative storage backends</li> <li>Kedro - Pipeline orchestration</li> </ul>"},{"location":"#signalflow-ecosystem","title":"SignalFlow Ecosystem","text":"<p>SignalFlow is a multi-repository ecosystem with specialized extensions:</p>"},{"location":"#signalflow-trading-core","title":"signalflow-trading (Core)","text":"<p>The main library with foundational components:</p> <ul> <li>Core data containers and abstractions (<code>RawData</code>, <code>Signals</code>, <code>RawDataView</code>)</li> <li>Exchange connectors (Binance Spot) and virtual data generation</li> <li>DuckDB, SQLite, and PostgreSQL storage backends</li> <li>Backtesting infrastructure with modular entry/exit rules</li> <li>Component registry system (<code>@sf_component</code>)</li> </ul>"},{"location":"#signalflow-ta-technical-analysis","title":"signalflow-ta (Technical Analysis)","text":"<p>199+ technical indicators across 8 modules:</p> <ul> <li>Momentum, Overlap, Volatility, Volume, Trend, Statistics</li> <li>Physics-based market analogs (energy, viscosity, impedance)</li> <li>Preset pipeline factories for rapid feature engineering</li> <li>AutoFeatureNormalizer for automatic scaling</li> </ul>"},{"location":"#signalflow-nn-neural-networks","title":"signalflow-nn (Neural Networks)","text":"<p>Deep learning validators built on PyTorch Lightning:</p> <ul> <li>Encoder + Head composition (LSTM, GRU + MLP, Attention, Residual)</li> <li>TemporalValidator for seamless signalflow integration</li> <li>Per-asset preprocessing with TimeSeriesPreprocessor</li> <li>Optuna hyperparameter tuning support</li> </ul>"},{"location":"#philosophy","title":"Philosophy","text":"<p>Minimize time from successful experiment to production deployment.</p> <p>SignalFlow bridges the research-production gap by:</p> <ol> <li>Unified API: Same code works for backtesting and live trading</li> <li>Performance: Polars-optimized for production-scale data</li> <li>Modularity: Swap components without rewriting strategies</li> <li>Testability: Every stage independently analyzable</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to build your first trading strategy?</p> <ul> <li> <p> Installation Guide for SignalFlow</p> <p>Install SignalFlow and set up your development environment</p> </li> <li> <p> Quick Start Algorithmic Trading Example</p> <p>Build your first signal detector and backtest a strategy</p> </li> <li> <p> Advanced Strategy Components</p> <p>Position sizing, entry filters, signal aggregation, and exit rules</p> </li> <li> <p> ML/RL Model Integration</p> <p>Integrate external ML/RL models with StrategyModel protocol</p> </li> <li> <p> Ecosystem Extensions</p> <p>signalflow-ta (199 indicators) and signalflow-nn (deep learning)</p> </li> <li> <p> SignalFlow API Reference</p> <p>Detailed documentation for all classes and methods</p> </li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub: github.com/pathway2nothing/signalflow-trading</li> <li>Issues: Report bugs or request features</li> <li>Email: pathway2nothing@gmail.com</li> </ul>"},{"location":"#license","title":"License","text":"<p>SignalFlow is open source software released under the MIT License.</p> <p>Disclaimer</p> <p>SignalFlow is provided for research purposes. Trading financial instruments carries risk. Past performance does not guarantee future results. Use at your own risk.</p>"},{"location":"#faq","title":"FAQ","text":""},{"location":"#what-is-signalflow-used-for","title":"What is SignalFlow used for?","text":"<p>SignalFlow is used to build, validate, and deploy algorithmic trading strategies using classical indicators and machine learning models.</p>"},{"location":"#is-signalflow-suitable-for-production-trading","title":"Is SignalFlow suitable for production trading?","text":"<p>Yes. SignalFlow is designed with production pipelines, reproducibility, and performance in mind, using Polars, DuckDB, and Kedro.</p>"},{"location":"#does-signalflow-support-machine-learning","title":"Does SignalFlow support machine learning?","text":"<p>SignalFlow supports scikit-learn, XGBoost, LightGBM, and PyTorch-based models for signal validation and meta-labeling.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to SignalFlow!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/pathway2nothing/signalflow-trading.git\ncd signalflow-trading\npip install -e \".[dev,docs,nn]\"\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Use Black for formatting</li> <li>Use Ruff for linting</li> <li>Add type hints</li> <li>Write Google-style docstrings</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code>pytest\npytest --cov=signalflow\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"contributing/#contact","title":"Contact","text":"<ul> <li>Email: pathway2nothing@gmail.com</li> <li>Issues: GitHub Issues</li> </ul>"},{"location":"hooks/","title":"Hooks","text":"In\u00a0[\u00a0]: Copied! <pre># docs/hooks.py\nimport os\nimport tomllib\n</pre> # docs/hooks.py import os import tomllib In\u00a0[\u00a0]: Copied! <pre>def define_env(env):\n    \"\"\"\n    \u0426\u0435 \u0445\u0443\u043a \u0434\u043b\u044f mkdocs-macros-plugin\n    \"\"\"\n    toml_path = \"pyproject.toml\"\n\n    try:\n        with open(toml_path, \"rb\") as f:\n            data = tomllib.load(f)\n            version = data[\"project\"][\"version\"]\n\n            env.variables.project_version = version\n\n            env.variables.project_name = data[\"project\"][\"name\"]\n    except FileNotFoundError:\n        env.variables.project_version = \"unknown\"\n</pre> def define_env(env):     \"\"\"     \u0426\u0435 \u0445\u0443\u043a \u0434\u043b\u044f mkdocs-macros-plugin     \"\"\"     toml_path = \"pyproject.toml\"      try:         with open(toml_path, \"rb\") as f:             data = tomllib.load(f)             version = data[\"project\"][\"version\"]              env.variables.project_version = version              env.variables.project_name = data[\"project\"][\"name\"]     except FileNotFoundError:         env.variables.project_version = \"unknown\" In\u00a0[\u00a0]: Copied! <pre>def on_pre_page_macros(env):\n    \"\"\"Skip macro processing for notebook pages to avoid Jinja syntax conflicts.\"\"\"\n    page = env.page\n    if page and page.file.src_path.endswith(\".ipynb\"):\n        return False  # Skip macros for notebooks\n    return True\n</pre> def on_pre_page_macros(env):     \"\"\"Skip macro processing for notebook pages to avoid Jinja syntax conflicts.\"\"\"     page = env.page     if page and page.file.src_path.endswith(\".ipynb\"):         return False  # Skip macros for notebooks     return True In\u00a0[\u00a0]: Copied! <pre>def on_post_page_macros(env):\n    \"\"\"Post-processing hook (no-op).\"\"\"\n    pass\n</pre> def on_post_page_macros(env):     \"\"\"Post-processing hook (no-op).\"\"\"     pass In\u00a0[\u00a0]: Copied! <pre>def on_post_build(env):\n    \"\"\"Post-build hook (no-op).\"\"\"\n    pass\n</pre> def on_post_build(env):     \"\"\"Post-build hook (no-op).\"\"\"     pass"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>Build your first algorithmic trading strategy with SignalFlow.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 or higher</li> <li>Basic understanding of Polars DataFrames</li> <li>Familiarity with trading concepts (OHLCV data, signals)</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install signalflow-trading\n</code></pre> <p>For technical analysis indicators: <pre><code>pip install signalflow-ta\n</code></pre></p>"},{"location":"quickstart/#your-first-strategy-sma-crossover","title":"Your First Strategy: SMA Crossover","text":"<p>We'll build a classic Simple Moving Average crossover strategy step by step, using synthetic data so everything works offline.</p>"},{"location":"quickstart/#step-1-generate-market-data","title":"Step 1: Generate Market Data","text":"<p>Use <code>VirtualDataProvider</code> for self-contained development - no API keys needed:</p> <pre><code>from datetime import datetime\nfrom pathlib import Path\n\nfrom signalflow.data.source import VirtualDataProvider\nfrom signalflow.data.raw_store import DuckDbSpotStore\n\n# Create store and virtual data provider\nstore = DuckDbSpotStore(db_path=Path(\"tutorial.duckdb\"))\nprovider = VirtualDataProvider(\n    store=store,\n    base_prices={\"BTCUSDT\": 40000.0, \"ETHUSDT\": 2200.0},\n    volatility=0.02,\n    seed=42,\n)\n\n# Generate 10,000 bars of 1-minute data per pair\nprovider.download(pairs=[\"BTCUSDT\", \"ETHUSDT\"], n_bars=10_000)\n</code></pre> <p>??? tip \"Production: Binance Data\"     For real market data, swap in <code>BinanceSpotLoader</code>:     <pre><code>import asyncio\nfrom signalflow.data.source import BinanceSpotLoader\n\nloader = BinanceSpotLoader(store=store)\nasyncio.run(loader.download(pairs=[\"BTCUSDT\", \"ETHUSDT\"], days=30))\n</code></pre></p>"},{"location":"quickstart/#step-2-load-data-into-framework","title":"Step 2: Load Data into Framework","text":"<p>Convert stored data to SignalFlow's <code>RawData</code> container:</p> <pre><code>from signalflow.data import RawDataFactory\n\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"tutorial.duckdb\"),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2020, 1, 1),\n    end=datetime(2030, 1, 1),\n)\n\nraw_data_view = raw_data.view()\ndf = raw_data_view.to_polars(\"spot\")\nprint(f\"Loaded {df.height} rows, pairs: {df['pair'].unique().to_list()}\")\n</code></pre>"},{"location":"quickstart/#step-3-extract-features","title":"Step 3: Extract Features","text":"<p>Create features for signal detection and ML validation:</p> <pre><code>from signalflow.feature import FeaturePipeline, ExampleRsiFeature, ExampleSmaFeature\n\npipeline = FeaturePipeline(features=[\n    ExampleRsiFeature(period=14),\n    ExampleSmaFeature(period=20),\n    ExampleSmaFeature(period=50),\n])\n\nfeatures_df = pipeline.compute(df)\nprint(f\"Feature columns: {pipeline.output_cols()}\")\n</code></pre>"},{"location":"quickstart/#step-4-detect-signals","title":"Step 4: Detect Signals","text":"<p>Use the built-in SMA crossover detector:</p> <pre><code>from signalflow.detector import ExampleSmaCrossDetector\n\ndetector = ExampleSmaCrossDetector(fast_period=20, slow_period=50)\nsignals = detector.run(raw_data_view)\n\nsignals_df = signals.value\nactive = signals_df.filter(signals_df[\"signal_type\"] != \"none\")\nprint(f\"Detected {active.height} active signals out of {signals_df.height} rows\")\n</code></pre> <p>Signal Types:</p> <ul> <li><code>rise</code> - Fast SMA crosses above slow SMA (bullish)</li> <li><code>fall</code> - Fast SMA crosses below slow SMA (bearish)</li> <li><code>none</code> - No crossover</li> </ul>"},{"location":"quickstart/#step-5-label-signals-for-ml","title":"Step 5: Label Signals for ML","text":"<p>Add forward-looking labels to evaluate signal quality:</p> <pre><code>from signalflow.target import FixedHorizonLabeler\n\nlabeler = FixedHorizonLabeler(\n    price_col=\"close\",\n    horizon=60,        # look 60 bars ahead\n    include_meta=True, # include t1 and log-return\n)\n\n# Extract signal timestamps for masking\nsignal_keys = signals.value.select([\"pair\", \"timestamp\"])\n\nlabeled_df = labeler.compute(\n    df=raw_data_view.to_polars(\"spot\"),\n    signals=signals,\n    data_context={\"signal_keys\": signal_keys},\n)\n\n# Only signal rows get meaningful labels\nlabeled_signals = labeled_df.filter(labeled_df[\"label\"] != \"none\")\nprint(f\"Labeled signals: {labeled_signals.height}\")\nprint(labeled_signals.group_by(\"label\").len().sort(\"label\"))\n</code></pre> <p>Signal Masking</p> <p>Labels are computed on the full price series but masked to signal timestamps. Pass <code>data_context={\"signal_keys\": ...}</code> to enable masking. Non-signal rows receive <code>label=\"none\"</code>.</p>"},{"location":"quickstart/#step-6-train-signal-validator-meta-labeling","title":"Step 6: Train Signal Validator (Meta-Labeling)","text":"<p>Filter signals using ML to predict success probability:</p> <pre><code>import polars as pl\nfrom signalflow.validator import SklearnSignalValidator\nfrom signalflow.core import Signals\n\n# Prepare training data - join features with labeled signals\nfeature_cols = pipeline.output_cols()\ntrain_data = labeled_signals.join(\n    features_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    on=[\"pair\", \"timestamp\"],\n    how=\"inner\",\n).drop_nulls(subset=feature_cols)\n\n# Time-based train/test split (80/20)\nsplit_idx = int(len(train_data) * 0.8)\ntrain_df = train_data[:split_idx]\ntest_df = train_data[split_idx:]\n\n# Create and train validator\nvalidator = SklearnSignalValidator(model_type=\"random_forest\")\nvalidator.fit(\n    X_train=train_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_train=train_df.select(\"label\"),\n)\n\n# Predict on test set\ntest_signals = Signals(test_df.select([\"pair\", \"timestamp\", \"signal_type\", \"signal\"]))\nvalidated = validator.validate_signals(\n    test_signals,\n    test_df.select([\"pair\", \"timestamp\"] + feature_cols),\n)\n\nprint(validated.value.select(\n    [\"pair\", \"signal_type\", \"probability_rise\", \"probability_fall\"]\n).head())\n</code></pre> <p>Model Selection</p> <p><code>SklearnSignalValidator</code> supports:</p> <ul> <li><code>lightgbm</code> - fast gradient boosting (requires <code>pip install lightgbm</code>)</li> <li><code>xgboost</code> - gradient boosting (requires <code>pip install xgboost</code>)</li> <li><code>random_forest</code> - sklearn Random Forest</li> <li><code>logistic_regression</code> - linear baseline</li> <li><code>auto</code> - automatic selection via cross-validation</li> </ul>"},{"location":"quickstart/#step-7-backtest-strategy","title":"Step 7: Backtest Strategy","text":"<p>Simulate trading with entry/exit rules and a broker:</p> <pre><code>from signalflow.strategy.runner import BacktestRunner\nfrom signalflow.strategy.component.entry.signal import SignalEntryRule\nfrom signalflow.strategy.component.exit.tp_sl import TakeProfitStopLossExit\nfrom signalflow.strategy.broker import BacktestBroker\nfrom signalflow.strategy.broker.executor import VirtualSpotExecutor\n\n# Configure components\nentry_rule = SignalEntryRule(\n    base_position_size=100.0,\n    use_probability_sizing=False,\n    max_positions_per_pair=1,\n    max_total_positions=10,\n)\n\nexit_rule = TakeProfitStopLossExit(\n    take_profit_pct=0.02,\n    stop_loss_pct=0.01,\n)\n\nbroker = BacktestBroker(executor=VirtualSpotExecutor(fee_rate=0.001))\n\n# Create and run backtest\nrunner = BacktestRunner(\n    strategy_id=\"sma_cross_quickstart\",\n    broker=broker,\n    entry_rules=[entry_rule],\n    exit_rules=[exit_rule],\n    initial_capital=10_000.0,\n)\n\nstate = runner.run(raw_data=raw_data, signals=signals)\n\n# Results\nprint(f\"Total trades: {len(runner.trades)}\")\nprint(f\"Final capital: ${state.capital:.2f}\")\n\ntrades_df = runner.trades_df\nif trades_df.height &gt; 0:\n    wins = trades_df.filter(pl.col(\"pnl\") &gt; 0).height\n    print(f\"Win rate: {wins / trades_df.height:.1%}\")\n</code></pre>"},{"location":"quickstart/#complete-example","title":"Complete Example","text":"<p>Here's the full workflow in one script:</p> <pre><code>from datetime import datetime\nfrom pathlib import Path\nimport polars as pl\n\nfrom signalflow.data.source import VirtualDataProvider\nfrom signalflow.data.raw_store import DuckDbSpotStore\nfrom signalflow.data import RawDataFactory\nfrom signalflow.feature import FeaturePipeline, ExampleRsiFeature, ExampleSmaFeature\nfrom signalflow.detector import ExampleSmaCrossDetector\nfrom signalflow.target import FixedHorizonLabeler\nfrom signalflow.validator import SklearnSignalValidator\nfrom signalflow.core import Signals\nfrom signalflow.strategy.runner import BacktestRunner\nfrom signalflow.strategy.component.entry.signal import SignalEntryRule\nfrom signalflow.strategy.component.exit.tp_sl import TakeProfitStopLossExit\nfrom signalflow.strategy.broker import BacktestBroker\nfrom signalflow.strategy.broker.executor import VirtualSpotExecutor\n\n# 1. Generate data\nstore = DuckDbSpotStore(db_path=Path(\"quickstart.duckdb\"))\nprovider = VirtualDataProvider(store=store, seed=42)\nprovider.download(pairs=[\"BTCUSDT\"], n_bars=10_000)\n\n# 2. Load data\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"quickstart.duckdb\"),\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2020, 1, 1),\n    end=datetime(2030, 1, 1),\n)\nview = raw_data.view()\ndf = view.to_polars(\"spot\")\n\n# 3. Features\npipeline = FeaturePipeline(features=[\n    ExampleRsiFeature(period=14),\n    ExampleSmaFeature(period=20),\n    ExampleSmaFeature(period=50),\n])\nfeatures_df = pipeline.compute(df)\n\n# 4. Detect signals\ndetector = ExampleSmaCrossDetector(fast_period=20, slow_period=50)\nsignals = detector.run(view)\n\n# 5. Backtest\nrunner = BacktestRunner(\n    strategy_id=\"quickstart\",\n    broker=BacktestBroker(executor=VirtualSpotExecutor(fee_rate=0.001)),\n    entry_rules=[SignalEntryRule(base_position_size=100.0, use_probability_sizing=False)],\n    exit_rules=[TakeProfitStopLossExit(take_profit_pct=0.02, stop_loss_pct=0.01)],\n    initial_capital=10_000.0,\n)\nstate = runner.run(raw_data=raw_data, signals=signals)\nprint(f\"Trades: {len(runner.trades)}, Final capital: ${state.capital:.2f}\")\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li> <p> User Guide</p> <p>Custom data types, advanced features, and extension points</p> </li> <li> <p> API Reference</p> <p>Complete documentation for all classes and methods</p> </li> <li> <p> Ecosystem</p> <p>signalflow-ta (199 indicators) and signalflow-nn (deep learning validators)</p> </li> <li> <p> GitHub</p> <p>Source code, issues, and contributions</p> </li> </ul>"},{"location":"quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"quickstart/#using-the-component-registry","title":"Using the Component Registry","text":"<p>Register and discover components dynamically:</p> <pre><code>from signalflow.core import sf_component, default_registry, SfComponentType\nfrom signalflow.detector import SignalDetector\n\n@sf_component(name=\"my_detector\")\nclass MyCustomDetector(SignalDetector):\n    def detect(self, features, context=None):\n        # Your signal detection logic\n        ...\n\n# List available detectors\ndetectors = default_registry.list(SfComponentType.DETECTOR)\nprint(f\"Available: {detectors}\")\n</code></pre>"},{"location":"quickstart/#working-with-polars-dataframes","title":"Working with Polars DataFrames","text":"<p>SignalFlow is Polars-first for performance:</p> <pre><code>import polars as pl\n\n# Filter active signals\nactive_signals = signals.value.filter(pl.col(\"signal_type\") != \"none\")\n\n# Group by pair\nstats = signals.value.group_by(\"pair\").agg(\n    pl.col(\"signal_type\").filter(pl.col(\"signal_type\") != \"none\").len().alias(\"signal_count\")\n)\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#no-signals-detected","title":"No signals detected?","text":"<p>Check detector parameters and data quality:</p> <pre><code># Verify data\nprint(df.select(\"close\").describe())\n\n# Try shorter periods for more signals\ndetector = ExampleSmaCrossDetector(fast_period=10, slow_period=20)\n</code></pre>"},{"location":"quickstart/#import-errors","title":"Import errors?","text":"<pre><code># Core installation\npip install signalflow-trading\n\n# With technical analysis indicators\npip install signalflow-ta\n\n# With neural network validators\npip install signalflow-nn\n</code></pre> <p>You're Ready!</p> <p>You now have a working algorithmic trading strategy. Experiment with different detectors, validators, and parameters to improve performance. Backtest thoroughly before live trading!</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for SignalFlow components.</p>"},{"location":"api/#modules","title":"Modules","text":"<ul> <li>Core - Core containers and registries</li> <li>Data - Data loading and storage</li> <li>Detector - Signal detection</li> <li>Feature - Feature extraction</li> <li>Labeler - Signal labeling</li> <li>Validator - Signal validation</li> <li>Strategy - Strategy execution</li> <li>Technical Analysis (ta) - 199+ indicators from <code>signalflow-ta</code></li> </ul>"},{"location":"api/core/","title":"Core Module","text":""},{"location":"api/core/#signalflow.core","title":"signalflow.core","text":"<p>SignalFlow Core Module.</p> <p>Provides fundamental building blocks for SignalFlow trading framework: - Containers: RawData, Signals, Position, Trade, Portfolio, Order, OrderFill - Enums: SignalType, SfComponentType, PositionType, etc. - Registry: Component registration and discovery - Decorators: @sf_component for automatic registration - Transforms: SignalsTransform protocol</p>"},{"location":"api/core/#signalflow.core.RawData","title":"RawData  <code>dataclass</code>","text":"<pre><code>RawData(datetime_start: datetime, datetime_end: datetime, pairs: list[str] = list(), data: dict[str, DataFrame] = dict())\n</code></pre> <p>Immutable container for raw market data.</p> <p>Acts as a unified in-memory bundle for multiple raw datasets (e.g. spot prices, funding, trades, orderbook, signals).</p> Design principles <ul> <li>Canonical storage is dataset-based (dictionary by name)</li> <li>Datasets accessed via string keys (e.g. raw_data[\"spot\"])</li> <li>No business logic or transformations</li> <li>Immutability ensures reproducibility in pipelines</li> </ul> <p>Attributes:</p> Name Type Description <code>datetime_start</code> <code>datetime</code> <p>Start datetime of the data snapshot.</p> <code>datetime_end</code> <code>datetime</code> <p>End datetime of the data snapshot.</p> <code>pairs</code> <code>list[str]</code> <p>List of trading pairs in the snapshot.</p> <code>data</code> <code>dict[str, DataFrame]</code> <p>Dictionary of datasets keyed by name.</p> Example <pre><code>from signalflow.core import RawData\nimport polars as pl\nfrom datetime import datetime\n\n# Create RawData with spot data\nraw_data = RawData(\n    datetime_start=datetime(2024, 1, 1),\n    datetime_end=datetime(2024, 12, 31),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    data={\n        \"spot\": spot_dataframe,\n        \"signals\": signals_dataframe,\n    }\n)\n\n# Access datasets\nspot_df = raw_data[\"spot\"]\nsignals_df = raw_data.get(\"signals\")\n\n# Check if dataset exists\nif \"spot\" in raw_data:\n    print(\"Spot data available\")\n</code></pre> Note <p>Dataset schemas are defined by convention, not enforced. Views (pandas/polars) should be handled by RawDataView wrapper.</p>"},{"location":"api/core/#signalflow.core.RawData.__contains__","title":"__contains__","text":"<pre><code>__contains__(key: str) -&gt; bool\n</code></pre> <p>Check if dataset exists.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Dataset name to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if dataset exists, False otherwise.</p> Example <pre><code>if \"spot\" in raw_data:\n    process_spot_data(raw_data[\"spot\"])\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def __contains__(self, key: str) -&gt; bool:\n    \"\"\"Check if dataset exists.\n\n    Args:\n        key (str): Dataset name to check.\n\n    Returns:\n        bool: True if dataset exists, False otherwise.\n\n    Example:\n        ```python\n        if \"spot\" in raw_data:\n            process_spot_data(raw_data[\"spot\"])\n        ```\n    \"\"\"\n    return key in self.data\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; pl.DataFrame\n</code></pre> <p>Dictionary-style access to datasets.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Dataset name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Dataset as Polars DataFrame.</p> Example <pre><code>spot_df = raw_data[\"spot\"]\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def __getitem__(self, key: str) -&gt; pl.DataFrame:\n    \"\"\"Dictionary-style access to datasets.\n\n    Args:\n        key (str): Dataset name.\n\n    Returns:\n        pl.DataFrame: Dataset as Polars DataFrame.\n\n    Example:\n        ```python\n        spot_df = raw_data[\"spot\"]\n        ```\n    \"\"\"\n    return self.get(key)\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.get","title":"get","text":"<pre><code>get(key: str) -&gt; pl.DataFrame\n</code></pre> <p>Get dataset by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Dataset name (e.g. \"spot\", \"signals\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Polars DataFrame if exists, empty DataFrame otherwise.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If dataset exists but is not a Polars DataFrame.</p> Example <pre><code>spot_df = raw_data.get(\"spot\")\n\n# Returns empty DataFrame if key doesn't exist\nmissing_df = raw_data.get(\"nonexistent\")\nassert missing_df.is_empty()\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def get(self, key: str) -&gt; pl.DataFrame:\n    \"\"\"Get dataset by key.\n\n    Args:\n        key (str): Dataset name (e.g. \"spot\", \"signals\").\n\n    Returns:\n        pl.DataFrame: Polars DataFrame if exists, empty DataFrame otherwise.\n\n    Raises:\n        TypeError: If dataset exists but is not a Polars DataFrame.\n\n    Example:\n        ```python\n        spot_df = raw_data.get(\"spot\")\n\n        # Returns empty DataFrame if key doesn't exist\n        missing_df = raw_data.get(\"nonexistent\")\n        assert missing_df.is_empty()\n        ```\n    \"\"\"\n    obj = self.data.get(key)\n    if obj is None:\n        return pl.DataFrame()\n    if not isinstance(obj, pl.DataFrame):\n        raise TypeError(f\"Dataset '{key}' is not a polars.DataFrame: {type(obj)}\")\n    return obj\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Return (key, dataset) pairs.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <p>Iterator over (key, DataFrame) tuples.</p> Example <pre><code>for name, df in raw_data.items():\n    print(f\"{name}: {df.shape}\")\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def items(self):\n    \"\"\"Return (key, dataset) pairs.\n\n    Returns:\n        Iterator: Iterator over (key, DataFrame) tuples.\n\n    Example:\n        ```python\n        for name, df in raw_data.items():\n            print(f\"{name}: {df.shape}\")\n        ```\n    \"\"\"\n    return self.data.items()\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.keys","title":"keys","text":"<pre><code>keys() -&gt; Iterator[str]\n</code></pre> <p>Return available dataset keys.</p> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>Iterator[str]: Iterator over dataset names.</p> Example <pre><code>for key in raw_data.keys():\n    print(f\"Dataset: {key}\")\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def keys(self) -&gt; Iterator[str]:\n    \"\"\"Return available dataset keys.\n\n    Returns:\n        Iterator[str]: Iterator over dataset names.\n\n    Example:\n        ```python\n        for key in raw_data.keys():\n            print(f\"Dataset: {key}\")\n        ```\n    \"\"\"\n    return self.data.keys()\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Return dataset values.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <p>Iterator over DataFrames.</p> Example <pre><code>for df in raw_data.values():\n    print(df.columns)\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def values(self):\n    \"\"\"Return dataset values.\n\n    Returns:\n        Iterator: Iterator over DataFrames.\n\n    Example:\n        ```python\n        for df in raw_data.values():\n            print(df.columns)\n        ```\n    \"\"\"\n    return self.data.values()\n</code></pre>"},{"location":"api/core/#signalflow.core.Signals","title":"Signals  <code>dataclass</code>","text":"<pre><code>Signals(value: DataFrame)\n</code></pre> <p>Immutable container for trading signals.</p> <p>Canonical in-memory format is a Polars DataFrame with long schema.</p> Required columns <ul> <li>pair (str): Trading pair identifier</li> <li>timestamp (datetime): Signal timestamp</li> <li>signal_type (SignalType | int): Signal type (RISE, FALL, NONE)</li> <li>signal (int | float): Signal value</li> </ul> Optional columns <ul> <li>probability (float): Signal probability (required for merge logic)</li> </ul> <p>Attributes:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>Polars DataFrame containing signal data.</p> Example <pre><code>from signalflow.core import Signals, SignalType\nimport polars as pl\nfrom datetime import datetime\n\n# Create signals\nsignals_df = pl.DataFrame({\n    \"pair\": [\"BTCUSDT\", \"ETHUSDT\"],\n    \"timestamp\": [datetime.now(), datetime.now()],\n    \"signal_type\": [SignalType.RISE.value, SignalType.FALL.value],\n    \"signal\": [1, -1],\n    \"probability\": [0.8, 0.7]\n})\n\nsignals = Signals(signals_df)\n\n# Apply transformation\nfiltered = signals.apply(filter_transform)\n\n# Chain transformations\nprocessed = signals.pipe(\n    transform1,\n    transform2,\n    transform3\n)\n\n# Merge signals\ncombined = signals1 + signals2\n</code></pre> Note <p>All transformations return new Signals instance. No in-place mutation is allowed.</p>"},{"location":"api/core/#signalflow.core.Signals.__add__","title":"__add__","text":"<pre><code>__add__(other: 'Signals') -&gt; 'Signals'\n</code></pre> <p>Merge two Signals objects.</p> Merge rules <ol> <li>Key: (pair, timestamp) or (pair, timestamp, signal_category)    if signal_category column is present.</li> <li>Signal type priority:</li> <li>null signal_type has lowest priority</li> <li>SignalType.NONE (\"none\") has lowest priority (backward compat)</li> <li>Non-null/non-NONE always overrides</li> <li>If both non-null, <code>other</code> wins</li> <li>Low-priority signals normalized to probability = 0</li> <li>Merge is deterministic</li> </ol> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Signals</code> <p>Another Signals object to merge.</p> required <p>Returns:</p> Name Type Description <code>Signals</code> <code>'Signals'</code> <p>New merged Signals instance.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If other is not a Signals instance.</p> Example <pre><code># Detector 1 signals\nsignals1 = detector1.run(data)\n\n# Detector 2 signals\nsignals2 = detector2.run(data)\n\n# Merge with priority to signals2\nmerged = signals1 + signals2\n\n# NONE/null signals overridden by non-NONE\n# Non-NONE conflicts resolved by taking signals2\n</code></pre> Source code in <code>src/signalflow/core/containers/signals.py</code> <pre><code>def __add__(self, other: \"Signals\") -&gt; \"Signals\":\n    \"\"\"Merge two Signals objects.\n\n    Merge rules:\n        1. Key: (pair, timestamp) or (pair, timestamp, signal_category)\n           if signal_category column is present.\n        2. Signal type priority:\n           - null signal_type has lowest priority\n           - SignalType.NONE (\"none\") has lowest priority (backward compat)\n           - Non-null/non-NONE always overrides\n           - If both non-null, ``other`` wins\n        3. Low-priority signals normalized to probability = 0\n        4. Merge is deterministic\n\n    Args:\n        other (Signals): Another Signals object to merge.\n\n    Returns:\n        Signals: New merged Signals instance.\n\n    Raises:\n        TypeError: If other is not a Signals instance.\n\n    Example:\n        ```python\n        # Detector 1 signals\n        signals1 = detector1.run(data)\n\n        # Detector 2 signals\n        signals2 = detector2.run(data)\n\n        # Merge with priority to signals2\n        merged = signals1 + signals2\n\n        # NONE/null signals overridden by non-NONE\n        # Non-NONE conflicts resolved by taking signals2\n        ```\n    \"\"\"\n    if not isinstance(other, Signals):\n        return NotImplemented\n\n    a = self.value\n    b = other.value\n\n    all_cols = list(dict.fromkeys([*a.columns, *b.columns]))\n\n    def align(df: pl.DataFrame) -&gt; pl.DataFrame:\n        return df.with_columns([pl.lit(None).alias(c) for c in all_cols if c not in df.columns]).select(all_cols)\n\n    a = align(a).with_columns(pl.lit(0).alias(\"_src\"))\n    b = align(b).with_columns(pl.lit(1).alias(\"_src\"))\n\n    merged = pl.concat([a, b], how=\"vertical\")\n\n    # Determine merge key: include signal_category if present\n    has_category = \"signal_category\" in all_cols\n    key_cols = [\"pair\", \"timestamp\"]\n    if has_category:\n        key_cols = [\"pair\", \"timestamp\", \"signal_category\"]\n\n    # Priority: null and \"none\" signal_type are lowest priority\n    merged = merged.with_columns(\n        pl.when(pl.col(\"signal_type\").is_null() | (pl.col(\"signal_type\") == _NONE_SIGNAL))\n        .then(pl.lit(0))\n        .otherwise(pl.col(\"probability\"))\n        .alias(\"probability\")\n    )\n\n    merged = merged.with_columns(\n        pl.when(pl.col(\"signal_type\").is_null() | (pl.col(\"signal_type\") == _NONE_SIGNAL))\n        .then(pl.lit(0))\n        .otherwise(pl.lit(1))\n        .alias(\"_priority\")\n    )\n\n    sort_cols = key_cols + [\"_priority\", \"_src\"]\n    sort_desc = [False] * len(key_cols) + [True, True]\n\n    merged = (\n        merged.sort(sort_cols, descending=sort_desc)\n        .unique(subset=key_cols, keep=\"first\")\n        .drop([\"_priority\", \"_src\"])\n        .sort(key_cols[:2])  # always sort output by pair, timestamp\n    )\n\n    return Signals(merged)\n</code></pre>"},{"location":"api/core/#signalflow.core.Signals.apply","title":"apply","text":"<pre><code>apply(transform: SignalsTransform) -&gt; 'Signals'\n</code></pre> <p>Apply a single transformation to signals.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>SignalsTransform</code> <p>Callable transformation implementing SignalsTransform protocol.</p> required <p>Returns:</p> Name Type Description <code>Signals</code> <code>'Signals'</code> <p>New Signals instance with transformed data.</p> Example <pre><code>from signalflow.core import Signals\nimport polars as pl\n\ndef filter_high_probability(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.filter(pl.col(\"probability\") &gt; 0.7)\n\nfiltered = signals.apply(filter_high_probability)\n</code></pre> Source code in <code>src/signalflow/core/containers/signals.py</code> <pre><code>def apply(self, transform: SignalsTransform) -&gt; \"Signals\":\n    \"\"\"Apply a single transformation to signals.\n\n    Args:\n        transform (SignalsTransform): Callable transformation implementing\n            SignalsTransform protocol.\n\n    Returns:\n        Signals: New Signals instance with transformed data.\n\n    Example:\n        ```python\n        from signalflow.core import Signals\n        import polars as pl\n\n        def filter_high_probability(df: pl.DataFrame) -&gt; pl.DataFrame:\n            return df.filter(pl.col(\"probability\") &gt; 0.7)\n\n        filtered = signals.apply(filter_high_probability)\n        ```\n    \"\"\"\n    out = transform(self.value)\n    return Signals(out)\n</code></pre>"},{"location":"api/core/#signalflow.core.Signals.pipe","title":"pipe","text":"<pre><code>pipe(*transforms: SignalsTransform) -&gt; 'Signals'\n</code></pre> <p>Apply multiple transformations sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>*transforms</code> <code>SignalsTransform</code> <p>Sequence of transformations to apply in order.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Signals</code> <code>'Signals'</code> <p>New Signals instance after applying all transformations.</p> Example <pre><code>result = signals.pipe(\n    filter_none_signals,\n    normalize_probabilities,\n    add_metadata\n)\n</code></pre> Source code in <code>src/signalflow/core/containers/signals.py</code> <pre><code>def pipe(self, *transforms: SignalsTransform) -&gt; \"Signals\":\n    \"\"\"Apply multiple transformations sequentially.\n\n    Args:\n        *transforms (SignalsTransform): Sequence of transformations to apply in order.\n\n    Returns:\n        Signals: New Signals instance after applying all transformations.\n\n    Example:\n        ```python\n        result = signals.pipe(\n            filter_none_signals,\n            normalize_probabilities,\n            add_metadata\n        )\n        ```\n    \"\"\"\n    s = self\n    for t in transforms:\n        s = s.apply(t)\n    return s\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalType","title":"SignalType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of price direction signal types.</p> <p>.. deprecated::     <code>SignalType</code> is deprecated and will be removed in a future version.     Use plain string values instead (e.g. <code>\"rise\"</code>, <code>\"fall\"</code>, <code>\"flat\"</code>).     Use <code>null</code> (Polars null) instead of <code>SignalType.NONE</code> for unknown/no signal.</p> <pre><code>For configuring which signal types to trade, use ``signal_type_map`` on\nentry rules or ``DIRECTIONAL_SIGNAL_MAP`` from ``core.signal_registry``.\n</code></pre> <p>Represents the direction of a trading signal detected by signal detectors. This enum covers the <code>PRICE_DIRECTION</code> category. Other categories use free-form string values for <code>signal_type</code> (see <code>SignalCategory</code>).</p> Values <p>NONE: No signal detected. Deprecated -- use <code>null</code> for unknown     or <code>FLAT</code> for sideways market. RISE: Bullish signal indicating potential price increase. FALL: Bearish signal indicating potential price decrease. FLAT: Sideways market / range-bound price action.</p> Example <pre><code># Preferred: use plain strings\nsignal_type = \"rise\"\nif signal_type == \"rise\":\n    print(\"Bullish signal detected\")\n\n# Legacy (deprecated):\nfrom signalflow.core.enums import SignalType\nif signal_type == SignalType.RISE.value:\n    print(\"Bullish signal detected\")\n</code></pre> Note <p>Stored as string values in DataFrames for serialization. For non-PRICE_DIRECTION categories, use string values directly (e.g. <code>\"high_volatility\"</code>, <code>\"local_max\"</code>) instead of this enum.</p>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry","title":"SignalFlowRegistry  <code>dataclass</code>","text":"<pre><code>SignalFlowRegistry(_items: dict[SfComponentType, dict[str, type[Any]]] = dict(), _raw_data_types: dict[str, set[str]] = (lambda: {k: (v.copy()) for k, v in (_BUILTIN_RAW_DATA_TYPES.items())})(), _discovered: bool = False)\n</code></pre> <p>Component registry for dynamic component discovery and instantiation.</p> <p>Provides centralized registration and lookup for SignalFlow components. Components are organized by type (DETECTOR, EXTRACTOR, etc.) and accessed by case-insensitive names.</p> <p>Also manages extensible raw data type definitions - each data type maps to a set of required columns. Built-in types (SPOT, FUTURES, PERPETUAL) are pre-registered; users can add custom types via <code>register_raw_data_type()</code>.</p> Registry structure <p>component_type -&gt; name -&gt; class</p> Supported component types <ul> <li>DETECTOR: Signal detection classes</li> <li>EXTRACTOR: Feature extraction classes</li> <li>LABELER: Signal labeling classes</li> <li>ENTRY_RULE: Position entry rules</li> <li>EXIT_RULE: Position exit rules</li> <li>METRIC: Strategy metrics</li> <li>EXECUTOR: Order execution engines</li> </ul> <p>Attributes:</p> Name Type Description <code>_items</code> <code>dict[SfComponentType, dict[str, Type[Any]]]</code> <p>Internal storage mapping component types to name-class pairs.</p> <code>_raw_data_types</code> <code>dict[str, set[str]]</code> <p>Mapping of raw data type names to their required column sets.</p> Example <pre><code>from signalflow.core.registry import SignalFlowRegistry, default_registry\n\n# Register custom raw data type\ndefault_registry.register_raw_data_type(\n    name=\"lob\",\n    columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\"],\n)\n\n# Get columns for any type\ncols = default_registry.get_raw_data_columns(\"spot\")\ncustom_cols = default_registry.get_raw_data_columns(\"lob\")\n\n# List all registered raw data types\nprint(default_registry.list_raw_data_types())\n</code></pre> Note <p>Component names are stored and looked up in lowercase. Use default_registry singleton for application-wide registration.</p> See Also <p>sf_component: Decorator for automatic component registration.</p>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.autodiscover","title":"autodiscover","text":"<pre><code>autodiscover() -&gt; None\n</code></pre> <p>Scan <code>signalflow.*</code> packages and entry-points for components.</p> <p>Walks all sub-modules of the <code>signalflow</code> package using :func:<code>pkgutil.walk_packages</code> and imports them.  Because :func:<code>sf_component</code> registers classes at import time, importing a module is sufficient to populate the registry.</p> <p>External packages can expose components via the <code>signalflow.components</code> entry-point group.  Each entry-point should reference a module (not a callable); importing it triggers registration through the <code>@sf_component</code> decorator.</p> <p>This method is idempotent - subsequent calls are no-ops once <code>_discovered</code> is <code>True</code>.</p> Example <pre><code>from signalflow.core.registry import default_registry\n\n# Explicit discovery (normally automatic on first get/list)\ndefault_registry.autodiscover()\n\n# All @sf_component-decorated classes are now registered\nprint(default_registry.snapshot())\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def autodiscover(self) -&gt; None:\n    \"\"\"Scan ``signalflow.*`` packages and entry-points for components.\n\n    Walks all sub-modules of the ``signalflow`` package using\n    :func:`pkgutil.walk_packages` and imports them.  Because\n    :func:`sf_component` registers classes at import time, importing\n    a module is sufficient to populate the registry.\n\n    External packages can expose components via the\n    ``signalflow.components`` entry-point group.  Each entry-point\n    should reference a module (not a callable); importing it triggers\n    registration through the ``@sf_component`` decorator.\n\n    This method is idempotent - subsequent calls are no-ops once\n    ``_discovered`` is ``True``.\n\n    Example:\n        ```python\n        from signalflow.core.registry import default_registry\n\n        # Explicit discovery (normally automatic on first get/list)\n        default_registry.autodiscover()\n\n        # All @sf_component-decorated classes are now registered\n        print(default_registry.snapshot())\n        ```\n    \"\"\"\n    if self._discovered:\n        return\n    self._discovered = True\n\n    self._discover_internal_packages()\n    self._discover_entry_points()\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.create","title":"create","text":"<pre><code>create(component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>Instantiate a component by registry key.</p> <p>Convenient method that combines get() and instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to create.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to component constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Instantiated component.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found.</p> <code>TypeError</code> <p>If kwargs don't match component constructor.</p> Example <pre><code># Create detector with params\ndetector = registry.create(\n    SfComponentType.DETECTOR,\n    \"sma_cross\",\n    fast_window=10,\n    slow_window=20\n)\n\n# Create extractor\nextractor = registry.create(\n    SfComponentType.EXTRACTOR,\n    \"rsi\",\n    window=14\n)\n\n# Create with config dict\nconfig = {\"window\": 20, \"threshold\": 0.7}\nlabeler = registry.create(\n    SfComponentType.LABELER,\n    \"fixed\",\n    **config\n)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def create(self, component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Instantiate a component by registry key.\n\n    Convenient method that combines get() and instantiation.\n\n    Args:\n        component_type (SfComponentType): Type of component to create.\n        name (str): Component name (case-insensitive).\n        **kwargs: Arguments to pass to component constructor.\n\n    Returns:\n        Any: Instantiated component.\n\n    Raises:\n        KeyError: If component not found.\n        TypeError: If kwargs don't match component constructor.\n\n    Example:\n        ```python\n        # Create detector with params\n        detector = registry.create(\n            SfComponentType.DETECTOR,\n            \"sma_cross\",\n            fast_window=10,\n            slow_window=20\n        )\n\n        # Create extractor\n        extractor = registry.create(\n            SfComponentType.EXTRACTOR,\n            \"rsi\",\n            window=14\n        )\n\n        # Create with config dict\n        config = {\"window\": 20, \"threshold\": 0.7}\n        labeler = registry.create(\n            SfComponentType.LABELER,\n            \"fixed\",\n            **config\n        )\n        ```\n    \"\"\"\n    cls = self.get(component_type, name)\n    return cls(**kwargs)\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.get","title":"get","text":"<pre><code>get(component_type: SfComponentType, name: str) -&gt; type[Any]\n</code></pre> <p>Get a registered class by key.</p> <p>Lookup is case-insensitive. Raises helpful error with available components if key not found.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to lookup.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <p>Returns:</p> Type Description <code>type[Any]</code> <p>Type[Any]: Registered class.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found. Error message includes available components.</p> Example <pre><code># Get component class\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n# Case-insensitive\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n# Instantiate manually\ndetector = detector_cls(fast_window=10, slow_window=20)\n\n# Handle missing component\ntry:\n    cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\nexcept KeyError as e:\n    print(f\"Component not found: {e}\")\n    # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def get(self, component_type: SfComponentType, name: str) -&gt; type[Any]:\n    \"\"\"Get a registered class by key.\n\n    Lookup is case-insensitive. Raises helpful error with available\n    components if key not found.\n\n    Args:\n        component_type (SfComponentType): Type of component to lookup.\n        name (str): Component name (case-insensitive).\n\n    Returns:\n        Type[Any]: Registered class.\n\n    Raises:\n        KeyError: If component not found. Error message includes available components.\n\n    Example:\n        ```python\n        # Get component class\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # Case-insensitive\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n        # Instantiate manually\n        detector = detector_cls(fast_window=10, slow_window=20)\n\n        # Handle missing component\n        try:\n            cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\n        except KeyError as e:\n            print(f\"Component not found: {e}\")\n            # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n        ```\n    \"\"\"\n    self._discover_if_needed()\n    self._ensure(component_type)\n    key = name.lower()\n    try:\n        return self._items[component_type][key]\n    except KeyError as e:\n        available = \", \".join(sorted(self._items[component_type]))\n        raise KeyError(f\"Component not found: {component_type.value}:{key}. Available: [{available}]\") from e\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.get_raw_data_columns","title":"get_raw_data_columns","text":"<pre><code>get_raw_data_columns(name: str) -&gt; set[str]\n</code></pre> <p>Get required columns for a raw data type.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Data type identifier (case-insensitive). Accepts both <code>RawDataType</code> enum members and plain strings.</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Copy of the column set for the requested type.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If data type is not registered.</p> Example <pre><code>cols = default_registry.get_raw_data_columns(\"spot\")\n# {'pair', 'timestamp', 'open', 'high', 'low', 'close', 'volume'}\n\ncols = default_registry.get_raw_data_columns(\"lob\")\n# {'pair', 'timestamp', 'bid', 'ask', ...}\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def get_raw_data_columns(self, name: str) -&gt; set[str]:\n    \"\"\"Get required columns for a raw data type.\n\n    Args:\n        name: Data type identifier (case-insensitive). Accepts both\n            ``RawDataType`` enum members and plain strings.\n\n    Returns:\n        Copy of the column set for the requested type.\n\n    Raises:\n        KeyError: If data type is not registered.\n\n    Example:\n        ```python\n        cols = default_registry.get_raw_data_columns(\"spot\")\n        # {'pair', 'timestamp', 'open', 'high', 'low', 'close', 'volume'}\n\n        cols = default_registry.get_raw_data_columns(\"lob\")\n        # {'pair', 'timestamp', 'bid', 'ask', ...}\n        ```\n    \"\"\"\n    raw = getattr(name, \"value\", name)\n    key = str(raw).strip().lower()\n    try:\n        return self._raw_data_types[key].copy()\n    except KeyError:\n        available = \", \".join(sorted(self._raw_data_types))\n        raise KeyError(f\"Raw data type '{key}' not registered. Available: [{available}]\") from None\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.list","title":"list","text":"<pre><code>list(component_type: SfComponentType) -&gt; list[str]\n</code></pre> <p>List registered components for a type.</p> <p>Returns sorted list of component names for given type.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of components to list.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Sorted list of registered component names.</p> Example <pre><code># List all detectors\ndetectors = registry.list(SfComponentType.DETECTOR)\nprint(f\"Available detectors: {detectors}\")\n# Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n# Check if component exists\nif \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n    detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n# List all component types\nfrom signalflow.core.enums import SfComponentType\nfor component_type in SfComponentType:\n    components = registry.list(component_type)\n    print(f\"{component_type.value}: {components}\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def list(self, component_type: SfComponentType) -&gt; list[str]:\n    \"\"\"List registered components for a type.\n\n    Returns sorted list of component names for given type.\n\n    Args:\n        component_type (SfComponentType): Type of components to list.\n\n    Returns:\n        list[str]: Sorted list of registered component names.\n\n    Example:\n        ```python\n        # List all detectors\n        detectors = registry.list(SfComponentType.DETECTOR)\n        print(f\"Available detectors: {detectors}\")\n        # Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n        # Check if component exists\n        if \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n            detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # List all component types\n        from signalflow.core.enums import SfComponentType\n        for component_type in SfComponentType:\n            components = registry.list(component_type)\n            print(f\"{component_type.value}: {components}\")\n        ```\n    \"\"\"\n    self._discover_if_needed()\n    self._ensure(component_type)\n    return sorted(self._items[component_type])\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.list_raw_data_types","title":"list_raw_data_types","text":"<pre><code>list_raw_data_types() -&gt; list[str]\n</code></pre> <p>List all registered raw data type names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of registered data type names.</p> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def list_raw_data_types(self) -&gt; list[str]:\n    \"\"\"List all registered raw data type names.\n\n    Returns:\n        Sorted list of registered data type names.\n    \"\"\"\n    return sorted(self._raw_data_types)\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.register","title":"register","text":"<pre><code>register(component_type: SfComponentType, name: str, cls: type[Any], *, override: bool = False) -&gt; None\n</code></pre> <p>Register a class under (component_type, name).</p> <p>Stores class in registry for later lookup and instantiation. Names are normalized to lowercase for case-insensitive lookup.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component (DETECTOR, EXTRACTOR, etc.).</p> required <code>name</code> <code>str</code> <p>Registry name (case-insensitive, will be lowercased).</p> required <code>cls</code> <code>Type[Any]</code> <p>Class to register.</p> required <code>override</code> <code>bool</code> <p>Allow overriding existing registration. Default: False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name is empty or already registered (when override=False).</p> Example <pre><code># Register new component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=MyDetector\n)\n\n# Override existing component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=ImprovedDetector,\n    override=True  # Logs warning\n)\n\n# Register multiple types\nregistry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\nregistry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def register(self, component_type: SfComponentType, name: str, cls: type[Any], *, override: bool = False) -&gt; None:\n    \"\"\"Register a class under (component_type, name).\n\n    Stores class in registry for later lookup and instantiation.\n    Names are normalized to lowercase for case-insensitive lookup.\n\n    Args:\n        component_type (SfComponentType): Type of component (DETECTOR, EXTRACTOR, etc.).\n        name (str): Registry name (case-insensitive, will be lowercased).\n        cls (Type[Any]): Class to register.\n        override (bool): Allow overriding existing registration. Default: False.\n\n    Raises:\n        ValueError: If name is empty or already registered (when override=False).\n\n    Example:\n        ```python\n        # Register new component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=MyDetector\n        )\n\n        # Override existing component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=ImprovedDetector,\n            override=True  # Logs warning\n        )\n\n        # Register multiple types\n        registry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\n        registry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n        ```\n    \"\"\"\n    if not isinstance(name, str) or not name.strip():\n        raise ValueError(\"name must be a non-empty string\")\n\n    key = name.strip().lower()\n    self._ensure(component_type)\n\n    if key in self._items[component_type] and not override:\n        raise ValueError(f\"{component_type.value}:{key} already registered\")\n\n    if key in self._items[component_type] and override:\n        logger.warning(f\"Overriding {component_type.value}:{key} with {cls.__name__}\")\n\n    self._items[component_type][key] = cls\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.register_raw_data_type","title":"register_raw_data_type","text":"<pre><code>register_raw_data_type(name: str, columns: list[str] | set[str], *, override: bool = False) -&gt; None\n</code></pre> <p>Register a custom raw data type with its required columns.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Data type identifier (case-insensitive, stored lowercase).</p> required <code>columns</code> <code>list[str] | set[str]</code> <p>Required column names for this data type.</p> required <code>override</code> <code>bool</code> <p>Allow overriding an existing registration.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name is empty, columns are empty, or name already registered (when override=False).</p> Example <pre><code>default_registry.register_raw_data_type(\n    name=\"lob\",\n    columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\"],\n)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def register_raw_data_type(\n    self,\n    name: str,\n    columns: list[str] | set[str],\n    *,\n    override: bool = False,\n) -&gt; None:\n    \"\"\"Register a custom raw data type with its required columns.\n\n    Args:\n        name: Data type identifier (case-insensitive, stored lowercase).\n        columns: Required column names for this data type.\n        override: Allow overriding an existing registration.\n\n    Raises:\n        ValueError: If name is empty, columns are empty, or name already\n            registered (when override=False).\n\n    Example:\n        ```python\n        default_registry.register_raw_data_type(\n            name=\"lob\",\n            columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\"],\n        )\n        ```\n    \"\"\"\n    if not isinstance(name, str) or not name.strip():\n        raise ValueError(\"name must be a non-empty string\")\n\n    cols = set(columns)\n    if not cols:\n        raise ValueError(\"columns must be a non-empty collection\")\n\n    key = name.strip().lower()\n\n    if key in self._raw_data_types and not override:\n        raise ValueError(f\"Raw data type '{key}' already registered\")\n\n    if key in self._raw_data_types and override:\n        logger.warning(f\"Overriding raw data type '{key}'\")\n\n    self._raw_data_types[key] = cols\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.snapshot","title":"snapshot","text":"<pre><code>snapshot() -&gt; dict[str, list[str]]\n</code></pre> <p>Snapshot of registry for debugging.</p> <p>Returns complete registry state organized by component type.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: Dictionary mapping component type names to sorted lists of registered component names.</p> Example <pre><code># Get full registry snapshot\nsnapshot = registry.snapshot()\nprint(snapshot)\n# Output:\n# {\n#     'DETECTOR': ['ema_cross', 'sma_cross'],\n#     'EXTRACTOR': ['rsi', 'sma'],\n#     'LABELER': ['fixed', 'triple_barrier'],\n#     'ENTRY_RULE': ['fixed_size'],\n#     'EXIT_RULE': ['take_profit', 'time_based']\n# }\n\n# Use for debugging\nimport json\nprint(json.dumps(registry.snapshot(), indent=2))\n\n# Check registration status\nsnapshot = registry.snapshot()\nif 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n    print(\"SMA detector is registered\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def snapshot(self) -&gt; dict[str, list[str]]:\n    \"\"\"Snapshot of registry for debugging.\n\n    Returns complete registry state organized by component type.\n\n    Returns:\n        dict[str, list[str]]: Dictionary mapping component type names\n            to sorted lists of registered component names.\n\n    Example:\n        ```python\n        # Get full registry snapshot\n        snapshot = registry.snapshot()\n        print(snapshot)\n        # Output:\n        # {\n        #     'DETECTOR': ['ema_cross', 'sma_cross'],\n        #     'EXTRACTOR': ['rsi', 'sma'],\n        #     'LABELER': ['fixed', 'triple_barrier'],\n        #     'ENTRY_RULE': ['fixed_size'],\n        #     'EXIT_RULE': ['take_profit', 'time_based']\n        # }\n\n        # Use for debugging\n        import json\n        print(json.dumps(registry.snapshot(), indent=2))\n\n        # Check registration status\n        snapshot = registry.snapshot()\n        if 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n            print(\"SMA detector is registered\")\n        ```\n    \"\"\"\n    self._discover_if_needed()\n    return {t.value: sorted(v.keys()) for t, v in self._items.items()}\n</code></pre>"},{"location":"api/core/#signalflow.core.sf_component","title":"sf_component","text":"<pre><code>sf_component(*, name: str, override: bool = True)\n</code></pre> <p>Register class as SignalFlow component.</p> <p>Decorator that registers a class in the global component registry, making it discoverable by name for dynamic instantiation.</p> <p>The decorated class must have a <code>component_type</code> class attribute of type <code>SfComponentType</code> to indicate what kind of component it is (e.g., DETECTOR, FEATURE, LABELER, STRATEGY_ENTRY_RULE, STRATEGY_EXIT_RULE).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Registry name for the component (case-insensitive).</p> required <code>override</code> <code>bool</code> <p>Allow overriding existing registration. Default: True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>Callable</code> <p>Decorator function that registers and returns the class unchanged.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If class doesn't define component_type attribute.</p> <code>ValueError</code> <p>If name already registered and override=False.</p> Example <pre><code>from signalflow.core import sf_component\nfrom signalflow.core.enums import SfComponentType\nfrom signalflow.detector import SignalDetector\n\n@sf_component(name=\"my_detector\")\nclass MyDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n\n    def detect(self, df):\n        # Detection logic\n        return signals\n\n# Later, instantiate by name\nfrom signalflow.core.registry import default_registry\n\ndetector_cls = default_registry.get(\n    SfComponentType.DETECTOR,\n    \"my_detector\"\n)\ndetector = detector_cls(params={\"window\": 20})\n\n# Override existing registration\n@sf_component(name=\"my_detector\", override=True)\nclass ImprovedDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n    # ... improved implementation\n</code></pre> Example <pre><code># Register multiple component types\n\n@sf_component(name=\"sma_cross\")\nclass SmaCrossDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n    # ...\n\n@sf_component(name=\"rsi\")\nclass RsiExtractor(FeatureExtractor):\n    component_type = SfComponentType.FEATURE\n    # ...\n\n@sf_component(name=\"fixed_size\")\nclass FixedSizeEntry(SignalEntryRule):\n    component_type = SfComponentType.STRATEGY_ENTRY_RULE\n    # ...\n\n@sf_component(name=\"take_profit\")\nclass TakeProfitExit(ExitRule):\n    component_type = SfComponentType.STRATEGY_EXIT_RULE\n    # ...\n</code></pre> Note <p>Component names are case-insensitive for lookup. The class itself is not modified - only registered. Use override=True carefully to avoid accidental overrides.</p> Source code in <code>src/signalflow/core/decorators.py</code> <pre><code>def sf_component(*, name: str, override: bool = True):\n    \"\"\"Register class as SignalFlow component.\n\n    Decorator that registers a class in the global component registry,\n    making it discoverable by name for dynamic instantiation.\n\n    The decorated class must have a `component_type` class attribute\n    of type `SfComponentType` to indicate what kind of component it is\n    (e.g., DETECTOR, FEATURE, LABELER, STRATEGY_ENTRY_RULE, STRATEGY_EXIT_RULE).\n\n    Args:\n        name (str): Registry name for the component (case-insensitive).\n        override (bool): Allow overriding existing registration. Default: True.\n\n    Returns:\n        Callable: Decorator function that registers and returns the class unchanged.\n\n    Raises:\n        ValueError: If class doesn't define component_type attribute.\n        ValueError: If name already registered and override=False.\n\n    Example:\n        ```python\n        from signalflow.core import sf_component\n        from signalflow.core.enums import SfComponentType\n        from signalflow.detector import SignalDetector\n\n        @sf_component(name=\"my_detector\")\n        class MyDetector(SignalDetector):\n            component_type = SfComponentType.DETECTOR\n\n            def detect(self, df):\n                # Detection logic\n                return signals\n\n        # Later, instantiate by name\n        from signalflow.core.registry import default_registry\n\n        detector_cls = default_registry.get(\n            SfComponentType.DETECTOR,\n            \"my_detector\"\n        )\n        detector = detector_cls(params={\"window\": 20})\n\n        # Override existing registration\n        @sf_component(name=\"my_detector\", override=True)\n        class ImprovedDetector(SignalDetector):\n            component_type = SfComponentType.DETECTOR\n            # ... improved implementation\n        ```\n\n    Example:\n        ```python\n        # Register multiple component types\n\n        @sf_component(name=\"sma_cross\")\n        class SmaCrossDetector(SignalDetector):\n            component_type = SfComponentType.DETECTOR\n            # ...\n\n        @sf_component(name=\"rsi\")\n        class RsiExtractor(FeatureExtractor):\n            component_type = SfComponentType.FEATURE\n            # ...\n\n        @sf_component(name=\"fixed_size\")\n        class FixedSizeEntry(SignalEntryRule):\n            component_type = SfComponentType.STRATEGY_ENTRY_RULE\n            # ...\n\n        @sf_component(name=\"take_profit\")\n        class TakeProfitExit(ExitRule):\n            component_type = SfComponentType.STRATEGY_EXIT_RULE\n            # ...\n        ```\n\n    Note:\n        Component names are case-insensitive for lookup.\n        The class itself is not modified - only registered.\n        Use override=True carefully to avoid accidental overrides.\n    \"\"\"\n\n    def decorator(cls: Type[Any]) -&gt; Type[Any]:\n        component_type = getattr(cls, \"component_type\", None)\n        if not isinstance(component_type, SfComponentType):\n            raise ValueError(f\"{cls.__name__} must define class attribute 'component_type: SfComponentType'\")\n\n        default_registry.register(\n            component_type,\n            name=name,\n            cls=cls,\n            override=override,\n        )\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/core/#signalflow.core.enums","title":"signalflow.core.enums","text":""},{"location":"api/core/#signalflow.core.enums.SfComponentType","title":"SfComponentType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of SignalFlow component types.</p> <p>Defines all component types that can be registered in the component registry. Used by sf_component decorator and SignalFlowRegistry for type-safe registration.</p> Component categories <ul> <li>Data: Raw data loading and storage</li> <li>Feature: Feature extraction</li> <li>Signals: Signal detection, transformation, labeling, validation</li> <li>Strategy: Execution, rules, metrics</li> </ul> Values <p>RAW_DATA_STORE: Raw data storage backends (e.g., DuckDB, Parquet). RAW_DATA_SOURCE: Raw data sources (e.g., Binance API). RAW_DATA_LOADER: Raw data loaders combining source + store. FEATURE: Feature extraction classes (e.g., RSI, SMA). SIGNALS_TRANSFORM: Signal transformation functions. LABELER: Signal labeling strategies (e.g., triple barrier). DETECTOR: Signal detection algorithms (e.g., SMA cross). VALIDATOR: Signal validation models. TORCH_MODULE: PyTorch neural network modules. VALIDATOR_MODEL: Pre-trained validator models. STRATEGY_STORE: Strategy state persistence backends. STRATEGY_RUNNER: Backtest/live runner implementations. STRATEGY_BROKER: Order management and position tracking. STRATEGY_EXECUTOR: Order execution engines (backtest/live). STRATEGY_EXIT_RULE: Position exit rules (e.g., take profit, stop loss). STRATEGY_ENTRY_RULE: Position entry rules (e.g., fixed size). STRATEGY_METRIC: Strategy performance metrics. STRATEGY_ALERT: Strategy monitoring alerts (e.g., max drawdown, stuck positions).</p> Example <pre><code>from signalflow.core import sf_component\nfrom signalflow.core.enums import SfComponentType\nfrom signalflow.detector import SignalDetector\n\n# Register detector\n@sf_component(name=\"my_detector\")\nclass MyDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n    # ... implementation\n\n# Register extractor\n@sf_component(name=\"my_feature\")\nclass MyExtractor(FeatureExtractor):\n    component_type = SfComponentType.FEATURE_EXTRACTOR\n    # ... implementation\n\n# Register exit rule\n@sf_component(name=\"my_exit\")\nclass MyExit(ExitRule):\n    component_type = SfComponentType.STRATEGY_EXIT_RULE\n    # ... implementation\n\n# Use in registry\nfrom signalflow.core.registry import default_registry\n\ndetector = default_registry.create(\n    SfComponentType.DETECTOR,\n    \"my_detector\"\n)\n</code></pre> Note <p>All registered components must have component_type class attribute. Component types are organized hierarchically (category/subcategory).</p>"},{"location":"api/core/#signalflow.core.enums.DataFrameType","title":"DataFrameType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported DataFrame backends.</p> <p>Specifies which DataFrame library to use for data processing. Used by FeatureExtractor and other components to determine input/output format.</p> Values <p>POLARS: Polars DataFrame (faster, modern). PANDAS: Pandas DataFrame (legacy compatibility).</p> Example <pre><code>from signalflow.core.enums import DataFrameType\nfrom signalflow.feature import FeatureExtractor\n\n# Polars-based extractor\nclass MyExtractor(FeatureExtractor):\n    df_type = DataFrameType.POLARS\n\n    def extract(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        return df.with_columns(\n            pl.col(\"close\").rolling_mean(20).alias(\"sma_20\")\n        )\n\n# Pandas-based extractor\nclass LegacyExtractor(FeatureExtractor):\n    df_type = DataFrameType.PANDAS\n\n    def extract(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df[\"sma_20\"] = df[\"close\"].rolling(20).mean()\n        return df\n\n# Use in RawDataView\nfrom signalflow.core import RawDataView\n\nview = RawDataView(raw=raw_data)\n\n# Get data in required format\ndf_polars = view.get_data(\"spot\", DataFrameType.POLARS)\ndf_pandas = view.get_data(\"spot\", DataFrameType.PANDAS)\n</code></pre> Note <p>New code should prefer POLARS for better performance. PANDAS supported for backward compatibility and legacy libraries.</p>"},{"location":"api/core/#signalflow.core.enums.RawDataType","title":"RawDataType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Built-in raw data types.</p> <p>Defines types of market data that can be loaded and processed. Column definitions are stored in :class:<code>SignalFlowRegistry</code> and can be extended with custom types via <code>default_registry.register_raw_data_type()</code>.</p> Values <p>SPOT: Spot trading data (OHLCV). FUTURES: Futures trading data (OHLCV + open_interest). PERPETUAL: Perpetual swaps data (OHLCV + funding_rate + open_interest).</p> Example <pre><code>from signalflow.core.enums import RawDataType\n\n# Built-in types\nspot_cols = RawDataType.SPOT.columns\n# {'pair', 'timestamp', 'open', 'high', 'low', 'close', 'volume'}\n\n# Custom types - register via registry\nfrom signalflow.core.registry import default_registry\n\ndefault_registry.register_raw_data_type(\n    name=\"lob\",\n    columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\"],\n)\ncols = default_registry.get_raw_data_columns(\"lob\")\n</code></pre> Note <p>Use <code>default_registry.register_raw_data_type()</code> to add custom types. Use <code>default_registry.get_raw_data_columns(name)</code> to look up columns for any type (built-in or custom).</p>"},{"location":"api/core/#signalflow.core.enums.RawDataType.columns","title":"columns  <code>property</code>","text":"<pre><code>columns: set[str]\n</code></pre> <p>Columns guaranteed to be present (looked up from registry).</p>"},{"location":"api/core/#signalflow.core.registry","title":"signalflow.core.registry","text":""},{"location":"api/core/#signalflow.core.registry.default_registry","title":"default_registry  <code>module-attribute</code>","text":"<pre><code>default_registry = SignalFlowRegistry()\n</code></pre> <p>Global default registry instance.</p> <p>Use this singleton for application-wide component registration.</p> Example <pre><code>from signalflow.core.registry import default_registry\nfrom signalflow.core.enums import SfComponentType\n\n# Register to default registry\ndefault_registry.register(\n    SfComponentType.DETECTOR,\n    \"my_detector\",\n    MyDetector\n)\n\n# Access from anywhere\ndetector = default_registry.create(\n    SfComponentType.DETECTOR,\n    \"my_detector\"\n)\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry","title":"SignalFlowRegistry  <code>dataclass</code>","text":"<pre><code>SignalFlowRegistry(_items: dict[SfComponentType, dict[str, type[Any]]] = dict(), _raw_data_types: dict[str, set[str]] = (lambda: {k: (v.copy()) for k, v in (_BUILTIN_RAW_DATA_TYPES.items())})(), _discovered: bool = False)\n</code></pre> <p>Component registry for dynamic component discovery and instantiation.</p> <p>Provides centralized registration and lookup for SignalFlow components. Components are organized by type (DETECTOR, EXTRACTOR, etc.) and accessed by case-insensitive names.</p> <p>Also manages extensible raw data type definitions - each data type maps to a set of required columns. Built-in types (SPOT, FUTURES, PERPETUAL) are pre-registered; users can add custom types via <code>register_raw_data_type()</code>.</p> Registry structure <p>component_type -&gt; name -&gt; class</p> Supported component types <ul> <li>DETECTOR: Signal detection classes</li> <li>EXTRACTOR: Feature extraction classes</li> <li>LABELER: Signal labeling classes</li> <li>ENTRY_RULE: Position entry rules</li> <li>EXIT_RULE: Position exit rules</li> <li>METRIC: Strategy metrics</li> <li>EXECUTOR: Order execution engines</li> </ul> <p>Attributes:</p> Name Type Description <code>_items</code> <code>dict[SfComponentType, dict[str, Type[Any]]]</code> <p>Internal storage mapping component types to name-class pairs.</p> <code>_raw_data_types</code> <code>dict[str, set[str]]</code> <p>Mapping of raw data type names to their required column sets.</p> Example <pre><code>from signalflow.core.registry import SignalFlowRegistry, default_registry\n\n# Register custom raw data type\ndefault_registry.register_raw_data_type(\n    name=\"lob\",\n    columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\"],\n)\n\n# Get columns for any type\ncols = default_registry.get_raw_data_columns(\"spot\")\ncustom_cols = default_registry.get_raw_data_columns(\"lob\")\n\n# List all registered raw data types\nprint(default_registry.list_raw_data_types())\n</code></pre> Note <p>Component names are stored and looked up in lowercase. Use default_registry singleton for application-wide registration.</p> See Also <p>sf_component: Decorator for automatic component registration.</p>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.autodiscover","title":"autodiscover","text":"<pre><code>autodiscover() -&gt; None\n</code></pre> <p>Scan <code>signalflow.*</code> packages and entry-points for components.</p> <p>Walks all sub-modules of the <code>signalflow</code> package using :func:<code>pkgutil.walk_packages</code> and imports them.  Because :func:<code>sf_component</code> registers classes at import time, importing a module is sufficient to populate the registry.</p> <p>External packages can expose components via the <code>signalflow.components</code> entry-point group.  Each entry-point should reference a module (not a callable); importing it triggers registration through the <code>@sf_component</code> decorator.</p> <p>This method is idempotent - subsequent calls are no-ops once <code>_discovered</code> is <code>True</code>.</p> Example <pre><code>from signalflow.core.registry import default_registry\n\n# Explicit discovery (normally automatic on first get/list)\ndefault_registry.autodiscover()\n\n# All @sf_component-decorated classes are now registered\nprint(default_registry.snapshot())\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def autodiscover(self) -&gt; None:\n    \"\"\"Scan ``signalflow.*`` packages and entry-points for components.\n\n    Walks all sub-modules of the ``signalflow`` package using\n    :func:`pkgutil.walk_packages` and imports them.  Because\n    :func:`sf_component` registers classes at import time, importing\n    a module is sufficient to populate the registry.\n\n    External packages can expose components via the\n    ``signalflow.components`` entry-point group.  Each entry-point\n    should reference a module (not a callable); importing it triggers\n    registration through the ``@sf_component`` decorator.\n\n    This method is idempotent - subsequent calls are no-ops once\n    ``_discovered`` is ``True``.\n\n    Example:\n        ```python\n        from signalflow.core.registry import default_registry\n\n        # Explicit discovery (normally automatic on first get/list)\n        default_registry.autodiscover()\n\n        # All @sf_component-decorated classes are now registered\n        print(default_registry.snapshot())\n        ```\n    \"\"\"\n    if self._discovered:\n        return\n    self._discovered = True\n\n    self._discover_internal_packages()\n    self._discover_entry_points()\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.create","title":"create","text":"<pre><code>create(component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>Instantiate a component by registry key.</p> <p>Convenient method that combines get() and instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to create.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to component constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Instantiated component.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found.</p> <code>TypeError</code> <p>If kwargs don't match component constructor.</p> Example <pre><code># Create detector with params\ndetector = registry.create(\n    SfComponentType.DETECTOR,\n    \"sma_cross\",\n    fast_window=10,\n    slow_window=20\n)\n\n# Create extractor\nextractor = registry.create(\n    SfComponentType.EXTRACTOR,\n    \"rsi\",\n    window=14\n)\n\n# Create with config dict\nconfig = {\"window\": 20, \"threshold\": 0.7}\nlabeler = registry.create(\n    SfComponentType.LABELER,\n    \"fixed\",\n    **config\n)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def create(self, component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Instantiate a component by registry key.\n\n    Convenient method that combines get() and instantiation.\n\n    Args:\n        component_type (SfComponentType): Type of component to create.\n        name (str): Component name (case-insensitive).\n        **kwargs: Arguments to pass to component constructor.\n\n    Returns:\n        Any: Instantiated component.\n\n    Raises:\n        KeyError: If component not found.\n        TypeError: If kwargs don't match component constructor.\n\n    Example:\n        ```python\n        # Create detector with params\n        detector = registry.create(\n            SfComponentType.DETECTOR,\n            \"sma_cross\",\n            fast_window=10,\n            slow_window=20\n        )\n\n        # Create extractor\n        extractor = registry.create(\n            SfComponentType.EXTRACTOR,\n            \"rsi\",\n            window=14\n        )\n\n        # Create with config dict\n        config = {\"window\": 20, \"threshold\": 0.7}\n        labeler = registry.create(\n            SfComponentType.LABELER,\n            \"fixed\",\n            **config\n        )\n        ```\n    \"\"\"\n    cls = self.get(component_type, name)\n    return cls(**kwargs)\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.get","title":"get","text":"<pre><code>get(component_type: SfComponentType, name: str) -&gt; type[Any]\n</code></pre> <p>Get a registered class by key.</p> <p>Lookup is case-insensitive. Raises helpful error with available components if key not found.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to lookup.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <p>Returns:</p> Type Description <code>type[Any]</code> <p>Type[Any]: Registered class.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found. Error message includes available components.</p> Example <pre><code># Get component class\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n# Case-insensitive\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n# Instantiate manually\ndetector = detector_cls(fast_window=10, slow_window=20)\n\n# Handle missing component\ntry:\n    cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\nexcept KeyError as e:\n    print(f\"Component not found: {e}\")\n    # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def get(self, component_type: SfComponentType, name: str) -&gt; type[Any]:\n    \"\"\"Get a registered class by key.\n\n    Lookup is case-insensitive. Raises helpful error with available\n    components if key not found.\n\n    Args:\n        component_type (SfComponentType): Type of component to lookup.\n        name (str): Component name (case-insensitive).\n\n    Returns:\n        Type[Any]: Registered class.\n\n    Raises:\n        KeyError: If component not found. Error message includes available components.\n\n    Example:\n        ```python\n        # Get component class\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # Case-insensitive\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n        # Instantiate manually\n        detector = detector_cls(fast_window=10, slow_window=20)\n\n        # Handle missing component\n        try:\n            cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\n        except KeyError as e:\n            print(f\"Component not found: {e}\")\n            # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n        ```\n    \"\"\"\n    self._discover_if_needed()\n    self._ensure(component_type)\n    key = name.lower()\n    try:\n        return self._items[component_type][key]\n    except KeyError as e:\n        available = \", \".join(sorted(self._items[component_type]))\n        raise KeyError(f\"Component not found: {component_type.value}:{key}. Available: [{available}]\") from e\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.get_raw_data_columns","title":"get_raw_data_columns","text":"<pre><code>get_raw_data_columns(name: str) -&gt; set[str]\n</code></pre> <p>Get required columns for a raw data type.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Data type identifier (case-insensitive). Accepts both <code>RawDataType</code> enum members and plain strings.</p> required <p>Returns:</p> Type Description <code>set[str]</code> <p>Copy of the column set for the requested type.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If data type is not registered.</p> Example <pre><code>cols = default_registry.get_raw_data_columns(\"spot\")\n# {'pair', 'timestamp', 'open', 'high', 'low', 'close', 'volume'}\n\ncols = default_registry.get_raw_data_columns(\"lob\")\n# {'pair', 'timestamp', 'bid', 'ask', ...}\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def get_raw_data_columns(self, name: str) -&gt; set[str]:\n    \"\"\"Get required columns for a raw data type.\n\n    Args:\n        name: Data type identifier (case-insensitive). Accepts both\n            ``RawDataType`` enum members and plain strings.\n\n    Returns:\n        Copy of the column set for the requested type.\n\n    Raises:\n        KeyError: If data type is not registered.\n\n    Example:\n        ```python\n        cols = default_registry.get_raw_data_columns(\"spot\")\n        # {'pair', 'timestamp', 'open', 'high', 'low', 'close', 'volume'}\n\n        cols = default_registry.get_raw_data_columns(\"lob\")\n        # {'pair', 'timestamp', 'bid', 'ask', ...}\n        ```\n    \"\"\"\n    raw = getattr(name, \"value\", name)\n    key = str(raw).strip().lower()\n    try:\n        return self._raw_data_types[key].copy()\n    except KeyError:\n        available = \", \".join(sorted(self._raw_data_types))\n        raise KeyError(f\"Raw data type '{key}' not registered. Available: [{available}]\") from None\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.list","title":"list","text":"<pre><code>list(component_type: SfComponentType) -&gt; list[str]\n</code></pre> <p>List registered components for a type.</p> <p>Returns sorted list of component names for given type.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of components to list.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Sorted list of registered component names.</p> Example <pre><code># List all detectors\ndetectors = registry.list(SfComponentType.DETECTOR)\nprint(f\"Available detectors: {detectors}\")\n# Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n# Check if component exists\nif \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n    detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n# List all component types\nfrom signalflow.core.enums import SfComponentType\nfor component_type in SfComponentType:\n    components = registry.list(component_type)\n    print(f\"{component_type.value}: {components}\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def list(self, component_type: SfComponentType) -&gt; list[str]:\n    \"\"\"List registered components for a type.\n\n    Returns sorted list of component names for given type.\n\n    Args:\n        component_type (SfComponentType): Type of components to list.\n\n    Returns:\n        list[str]: Sorted list of registered component names.\n\n    Example:\n        ```python\n        # List all detectors\n        detectors = registry.list(SfComponentType.DETECTOR)\n        print(f\"Available detectors: {detectors}\")\n        # Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n        # Check if component exists\n        if \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n            detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # List all component types\n        from signalflow.core.enums import SfComponentType\n        for component_type in SfComponentType:\n            components = registry.list(component_type)\n            print(f\"{component_type.value}: {components}\")\n        ```\n    \"\"\"\n    self._discover_if_needed()\n    self._ensure(component_type)\n    return sorted(self._items[component_type])\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.list_raw_data_types","title":"list_raw_data_types","text":"<pre><code>list_raw_data_types() -&gt; list[str]\n</code></pre> <p>List all registered raw data type names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of registered data type names.</p> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def list_raw_data_types(self) -&gt; list[str]:\n    \"\"\"List all registered raw data type names.\n\n    Returns:\n        Sorted list of registered data type names.\n    \"\"\"\n    return sorted(self._raw_data_types)\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.register","title":"register","text":"<pre><code>register(component_type: SfComponentType, name: str, cls: type[Any], *, override: bool = False) -&gt; None\n</code></pre> <p>Register a class under (component_type, name).</p> <p>Stores class in registry for later lookup and instantiation. Names are normalized to lowercase for case-insensitive lookup.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component (DETECTOR, EXTRACTOR, etc.).</p> required <code>name</code> <code>str</code> <p>Registry name (case-insensitive, will be lowercased).</p> required <code>cls</code> <code>Type[Any]</code> <p>Class to register.</p> required <code>override</code> <code>bool</code> <p>Allow overriding existing registration. Default: False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name is empty or already registered (when override=False).</p> Example <pre><code># Register new component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=MyDetector\n)\n\n# Override existing component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=ImprovedDetector,\n    override=True  # Logs warning\n)\n\n# Register multiple types\nregistry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\nregistry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def register(self, component_type: SfComponentType, name: str, cls: type[Any], *, override: bool = False) -&gt; None:\n    \"\"\"Register a class under (component_type, name).\n\n    Stores class in registry for later lookup and instantiation.\n    Names are normalized to lowercase for case-insensitive lookup.\n\n    Args:\n        component_type (SfComponentType): Type of component (DETECTOR, EXTRACTOR, etc.).\n        name (str): Registry name (case-insensitive, will be lowercased).\n        cls (Type[Any]): Class to register.\n        override (bool): Allow overriding existing registration. Default: False.\n\n    Raises:\n        ValueError: If name is empty or already registered (when override=False).\n\n    Example:\n        ```python\n        # Register new component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=MyDetector\n        )\n\n        # Override existing component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=ImprovedDetector,\n            override=True  # Logs warning\n        )\n\n        # Register multiple types\n        registry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\n        registry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n        ```\n    \"\"\"\n    if not isinstance(name, str) or not name.strip():\n        raise ValueError(\"name must be a non-empty string\")\n\n    key = name.strip().lower()\n    self._ensure(component_type)\n\n    if key in self._items[component_type] and not override:\n        raise ValueError(f\"{component_type.value}:{key} already registered\")\n\n    if key in self._items[component_type] and override:\n        logger.warning(f\"Overriding {component_type.value}:{key} with {cls.__name__}\")\n\n    self._items[component_type][key] = cls\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.register_raw_data_type","title":"register_raw_data_type","text":"<pre><code>register_raw_data_type(name: str, columns: list[str] | set[str], *, override: bool = False) -&gt; None\n</code></pre> <p>Register a custom raw data type with its required columns.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Data type identifier (case-insensitive, stored lowercase).</p> required <code>columns</code> <code>list[str] | set[str]</code> <p>Required column names for this data type.</p> required <code>override</code> <code>bool</code> <p>Allow overriding an existing registration.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name is empty, columns are empty, or name already registered (when override=False).</p> Example <pre><code>default_registry.register_raw_data_type(\n    name=\"lob\",\n    columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\"],\n)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def register_raw_data_type(\n    self,\n    name: str,\n    columns: list[str] | set[str],\n    *,\n    override: bool = False,\n) -&gt; None:\n    \"\"\"Register a custom raw data type with its required columns.\n\n    Args:\n        name: Data type identifier (case-insensitive, stored lowercase).\n        columns: Required column names for this data type.\n        override: Allow overriding an existing registration.\n\n    Raises:\n        ValueError: If name is empty, columns are empty, or name already\n            registered (when override=False).\n\n    Example:\n        ```python\n        default_registry.register_raw_data_type(\n            name=\"lob\",\n            columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\"],\n        )\n        ```\n    \"\"\"\n    if not isinstance(name, str) or not name.strip():\n        raise ValueError(\"name must be a non-empty string\")\n\n    cols = set(columns)\n    if not cols:\n        raise ValueError(\"columns must be a non-empty collection\")\n\n    key = name.strip().lower()\n\n    if key in self._raw_data_types and not override:\n        raise ValueError(f\"Raw data type '{key}' already registered\")\n\n    if key in self._raw_data_types and override:\n        logger.warning(f\"Overriding raw data type '{key}'\")\n\n    self._raw_data_types[key] = cols\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.snapshot","title":"snapshot","text":"<pre><code>snapshot() -&gt; dict[str, list[str]]\n</code></pre> <p>Snapshot of registry for debugging.</p> <p>Returns complete registry state organized by component type.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: Dictionary mapping component type names to sorted lists of registered component names.</p> Example <pre><code># Get full registry snapshot\nsnapshot = registry.snapshot()\nprint(snapshot)\n# Output:\n# {\n#     'DETECTOR': ['ema_cross', 'sma_cross'],\n#     'EXTRACTOR': ['rsi', 'sma'],\n#     'LABELER': ['fixed', 'triple_barrier'],\n#     'ENTRY_RULE': ['fixed_size'],\n#     'EXIT_RULE': ['take_profit', 'time_based']\n# }\n\n# Use for debugging\nimport json\nprint(json.dumps(registry.snapshot(), indent=2))\n\n# Check registration status\nsnapshot = registry.snapshot()\nif 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n    print(\"SMA detector is registered\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def snapshot(self) -&gt; dict[str, list[str]]:\n    \"\"\"Snapshot of registry for debugging.\n\n    Returns complete registry state organized by component type.\n\n    Returns:\n        dict[str, list[str]]: Dictionary mapping component type names\n            to sorted lists of registered component names.\n\n    Example:\n        ```python\n        # Get full registry snapshot\n        snapshot = registry.snapshot()\n        print(snapshot)\n        # Output:\n        # {\n        #     'DETECTOR': ['ema_cross', 'sma_cross'],\n        #     'EXTRACTOR': ['rsi', 'sma'],\n        #     'LABELER': ['fixed', 'triple_barrier'],\n        #     'ENTRY_RULE': ['fixed_size'],\n        #     'EXIT_RULE': ['take_profit', 'time_based']\n        # }\n\n        # Use for debugging\n        import json\n        print(json.dumps(registry.snapshot(), indent=2))\n\n        # Check registration status\n        snapshot = registry.snapshot()\n        if 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n            print(\"SMA detector is registered\")\n        ```\n    \"\"\"\n    self._discover_if_needed()\n    return {t.value: sorted(v.keys()) for t, v in self._items.items()}\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.get_component","title":"get_component","text":"<pre><code>get_component(type: SfComponentType, name: str) -&gt; type[Any]\n</code></pre> <p>Get a registered component by type and name.</p> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def get_component(type: SfComponentType, name: str) -&gt; type[Any]:\n    \"\"\"Get a registered component by type and name.\"\"\"\n    return default_registry.get(type, name)\n</code></pre>"},{"location":"api/data/","title":"Data Module","text":""},{"location":"api/data/#loaders","title":"Loaders","text":""},{"location":"api/data/#signalflow.data.source.binance.BinanceSpotLoader","title":"signalflow.data.source.binance.BinanceSpotLoader  <code>dataclass</code>","text":"<pre><code>BinanceSpotLoader(store: DuckDbSpotStore = (lambda: DuckDbSpotStore(db_path=(Path('raw_data.duckdb'))))(), timeframe: str = '1m')\n</code></pre> <p>               Bases: <code>RawDataLoader</code></p> <p>Downloads and stores Binance spot OHLCV data for fixed timeframe.</p> <p>Combines BinanceClient (source) and DuckDbSpotStore (storage) to provide complete data pipeline with gap filling and incremental updates.</p> <p>Attributes:</p> Name Type Description <code>store</code> <code>DuckDbSpotStore</code> <p>Storage backend. Default: raw_data.duckdb.</p> <code>timeframe</code> <code>str</code> <p>Fixed timeframe for all data. Default: \"1m\".</p>"},{"location":"api/data/#signalflow.data.source.binance.BinanceSpotLoader.download","title":"download  <code>async</code>","text":"<pre><code>download(pairs: list[str], days: Optional[int] = None, start: Optional[datetime] = None, end: Optional[datetime] = None, fill_gaps: bool = True) -&gt; None\n</code></pre> <p>Download historical data with intelligent range detection.</p> Automatically determines what to download <ul> <li>If no existing data: download full range</li> <li>If data exists: download before/after existing range</li> <li>If fill_gaps=True: detect and fill gaps in existing range</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>list[str]</code> <p>Trading pairs to download.</p> required <code>days</code> <code>int | None</code> <p>Number of days back from end. Default: 7.</p> <code>None</code> <code>start</code> <code>datetime | None</code> <p>Range start (overrides days).</p> <code>None</code> <code>end</code> <code>datetime | None</code> <p>Range end. Default: now.</p> <code>None</code> <code>fill_gaps</code> <code>bool</code> <p>Detect and fill gaps. Default: True.</p> <code>True</code> Note <p>Runs async download for all pairs concurrently. Logs progress for large downloads. Errors logged but don't stop other pairs.</p> Source code in <code>src/signalflow/data/source/binance.py</code> <pre><code>async def download(\n    self,\n    pairs: list[str],\n    days: Optional[int] = None,\n    start: Optional[datetime] = None,\n    end: Optional[datetime] = None,\n    fill_gaps: bool = True,\n) -&gt; None:\n    \"\"\"Download historical data with intelligent range detection.\n\n    Automatically determines what to download:\n        - If no existing data: download full range\n        - If data exists: download before/after existing range\n        - If fill_gaps=True: detect and fill gaps in existing range\n\n    Args:\n        pairs (list[str]): Trading pairs to download.\n        days (int | None): Number of days back from end. Default: 7.\n        start (datetime | None): Range start (overrides days).\n        end (datetime | None): Range end. Default: now.\n        fill_gaps (bool): Detect and fill gaps. Default: True.\n\n    Note:\n        Runs async download for all pairs concurrently.\n        Logs progress for large downloads.\n        Errors logged but don't stop other pairs.\n    \"\"\"\n\n    now = datetime.now(timezone.utc).replace(tzinfo=None)\n    if end is None:\n        end = now\n    else:\n        end = ensure_utc_naive(end)\n\n    if start is None:\n        start = end - timedelta(days=days if days else 7)\n    else:\n        start = ensure_utc_naive(start)\n\n    tf_minutes = {\n        \"1m\": 1,\n        \"3m\": 3,\n        \"5m\": 5,\n        \"15m\": 15,\n        \"30m\": 30,\n        \"1h\": 60,\n        \"2h\": 120,\n        \"4h\": 240,\n        \"6h\": 360,\n        \"8h\": 480,\n        \"12h\": 720,\n        \"1d\": 1440,\n    }.get(self.timeframe, 1)\n\n    async def download_pair(client: BinanceClient, pair: str) -&gt; None:\n        logger.info(f\"Processing {pair} from {start} to {end}\")\n\n        db_min, db_max = self.store.get_time_bounds(pair)\n        ranges_to_download: list[tuple[datetime, datetime]] = []\n\n        if db_min is None:\n            ranges_to_download.append((start, end))\n        else:\n            if start &lt; db_min:\n                pre_end = min(end, db_min - timedelta(minutes=tf_minutes))\n                if start &lt; pre_end:\n                    ranges_to_download.append((start, pre_end))\n            if end &gt; db_max:\n                post_start = max(start, db_max + timedelta(minutes=tf_minutes))\n                if post_start &lt; end:\n                    ranges_to_download.append((post_start, end))\n\n            if fill_gaps:\n                overlap_start = max(start, db_min)\n                overlap_end = min(end, db_max)\n                if overlap_start &lt; overlap_end:\n                    gaps = self.store.find_gaps(pair, overlap_start, overlap_end, tf_minutes)\n                    ranges_to_download.extend(gaps)\n\n        for range_start, range_end in ranges_to_download:\n            if range_start &gt;= range_end:\n                continue\n\n            logger.info(f\"{pair}: downloading {range_start} -&gt; {range_end}\")\n\n            try:\n                klines = await client.get_klines_range(\n                    pair=pair,\n                    timeframe=self.timeframe,\n                    start_time=range_start,\n                    end_time=range_end,\n                )\n                self.store.insert_klines(pair, klines)\n            except Exception as e:\n                logger.error(f\"Error downloading {pair}: {e}\")\n\n    async with BinanceClient() as client:\n        await asyncio.gather(*[download_pair(client, pair) for pair in pairs])\n\n    self.store.close()\n</code></pre>"},{"location":"api/data/#signalflow.data.source.binance.BinanceSpotLoader.sync","title":"sync  <code>async</code>","text":"<pre><code>sync(pairs: list[str], update_interval_sec: int = 60) -&gt; None\n</code></pre> <p>Real-time sync - continuously update with latest data.</p> <p>Runs indefinitely, fetching latest candles at specified interval. Useful for live trading or monitoring.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>list[str]</code> <p>Trading pairs to sync.</p> required <code>update_interval_sec</code> <code>int</code> <p>Update interval in seconds. Default: 60.</p> <code>60</code> Note <p>Runs forever - use Ctrl+C to stop or run in background task. Fetches last 5 candles per update (ensures no gaps). Errors logged but sync continues.</p> Source code in <code>src/signalflow/data/source/binance.py</code> <pre><code>async def sync(\n    self,\n    pairs: list[str],\n    update_interval_sec: int = 60,\n) -&gt; None:\n    \"\"\"Real-time sync - continuously update with latest data.\n\n    Runs indefinitely, fetching latest candles at specified interval.\n    Useful for live trading or monitoring.\n\n    Args:\n        pairs (list[str]): Trading pairs to sync.\n        update_interval_sec (int): Update interval in seconds. Default: 60.\n\n    Note:\n        Runs forever - use Ctrl+C to stop or run in background task.\n        Fetches last 5 candles per update (ensures no gaps).\n        Errors logged but sync continues.\n    \"\"\"\n\n    logger.info(f\"Starting real-time sync for {pairs}\")\n    logger.info(f\"Update interval: {update_interval_sec}s (timeframe={self.timeframe})\")\n\n    async def fetch_and_store(client: BinanceClient, pair: str) -&gt; None:\n        try:\n            klines = await client.get_klines(pair=pair, timeframe=self.timeframe, limit=5)\n            self.store.insert_klines(pair, klines)\n        except Exception as e:\n            logger.error(f\"Error syncing {pair}: {e}\")\n\n    async with BinanceClient() as client:\n        while True:\n            await asyncio.gather(*[fetch_and_store(client, pair) for pair in pairs])\n            logger.debug(f\"Synced {len(pairs)} pairs\")\n            await asyncio.sleep(update_interval_sec)\n</code></pre>"},{"location":"api/data/#storage","title":"Storage","text":""},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore","title":"signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore  <code>module-attribute</code>","text":"<pre><code>DuckDbSpotStore = DuckDbRawStore\n</code></pre>"},{"location":"api/data/#factory","title":"Factory","text":""},{"location":"api/data/#signalflow.data.raw_data_factory.RawDataFactory","title":"signalflow.data.raw_data_factory.RawDataFactory","text":"<p>Factory for creating RawData instances from various sources.</p> <p>Provides static methods to construct RawData objects from different storage backends (DuckDB, Parquet, etc.) with proper validation and schema normalization.</p> Key features <ul> <li>Automatic schema validation</li> <li>Duplicate detection</li> <li>Timezone normalization</li> <li>Column cleanup (remove unnecessary columns)</li> <li>Proper sorting by (pair, timestamp)</li> </ul> Example <pre><code>from signalflow.data import RawDataFactory\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Load spot data from DuckDB\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/binance_spot.duckdb\"),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n    data_types=[\"spot\"]\n)\n\n# Access loaded data\nspot_df = raw_data[\"spot\"]\nprint(f\"Loaded {len(spot_df)} bars\")\nprint(f\"Pairs: {raw_data.pairs}\")\nprint(f\"Date range: {raw_data.datetime_start} to {raw_data.datetime_end}\")\n\n# Use in detector\nfrom signalflow.detector import SmaCrossSignalDetector\n\ndetector = SmaCrossSignalDetector(fast_window=10, slow_window=20)\nsignals = detector.detect(raw_data)\n</code></pre> See Also <p>RawData: Immutable container for raw market data. DuckDbSpotStore: DuckDB storage backend for spot data.</p>"},{"location":"api/data/#signalflow.data.raw_data_factory.RawDataFactory.from_duckdb_spot_store","title":"from_duckdb_spot_store  <code>staticmethod</code>","text":"<pre><code>from_duckdb_spot_store(spot_store_path: Path, pairs: list[str], start: datetime, end: datetime, data_types: list[str] | None = None) -&gt; RawData\n</code></pre> <p>Create RawData from DuckDB spot store.</p> <p>Loads spot trading data from DuckDB storage with validation, deduplication checks, and schema normalization.</p> Processing steps <ol> <li>Load data from DuckDB for specified pairs and date range</li> <li>Validate required columns (pair, timestamp)</li> <li>Remove unnecessary columns (timeframe)</li> <li>Normalize timestamps (microseconds, timezone-naive)</li> <li>Check for duplicates (pair, timestamp)</li> <li>Sort by (pair, timestamp)</li> <li>Package into RawData container</li> </ol> <p>Parameters:</p> Name Type Description Default <code>spot_store_path</code> <code>Path</code> <p>Path to DuckDB file.</p> required <code>pairs</code> <code>list[str]</code> <p>List of trading pairs to load (e.g., [\"BTCUSDT\", \"ETHUSDT\"]).</p> required <code>start</code> <code>datetime</code> <p>Start datetime (inclusive).</p> required <code>end</code> <code>datetime</code> <p>End datetime (inclusive).</p> required <code>data_types</code> <code>list[str] | None</code> <p>Data types to load. Default: None. Currently supports: [\"spot\"].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>Immutable container with loaded and validated data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns missing (pair, timestamp).</p> <code>ValueError</code> <p>If duplicate (pair, timestamp) combinations detected.</p> Example <pre><code>from pathlib import Path\nfrom datetime import datetime\nfrom signalflow.data import RawDataFactory\n\n# Load single pair\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/binance.duckdb\"),\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31),\n    data_types=[\"spot\"]\n)\n\n# Load multiple pairs\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/binance.duckdb\"),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n    data_types=[\"spot\"]\n)\n\n# Check loaded data\nspot_df = raw_data[\"spot\"]\nprint(f\"Shape: {spot_df.shape}\")\nprint(f\"Columns: {spot_df.columns}\")\nprint(f\"Pairs: {spot_df['pair'].unique().to_list()}\")\n\n# Verify no duplicates\ndup_check = (\n    spot_df.group_by([\"pair\", \"timestamp\"])\n    .len()\n    .filter(pl.col(\"len\") &gt; 1)\n)\nassert dup_check.is_empty()\n\n# Use in pipeline\nfrom signalflow.core import RawDataView\nview = RawDataView(raw=raw_data)\nspot_pandas = view.to_pandas(\"spot\")\n</code></pre> Example <pre><code># Handle missing data gracefully\ntry:\n    raw_data = RawDataFactory.from_duckdb_spot_store(\n        spot_store_path=Path(\"data/binance.duckdb\"),\n        pairs=[\"BTCUSDT\"],\n        start=datetime(2024, 1, 1),\n        end=datetime(2024, 1, 31),\n        data_types=[\"spot\"]\n    )\nexcept ValueError as e:\n    if \"missing columns\" in str(e):\n        print(\"Data schema invalid\")\n    elif \"Duplicate\" in str(e):\n        print(\"Data contains duplicates\")\n    raise\n\n# Validate date range\nassert raw_data.datetime_start == datetime(2024, 1, 1)\nassert raw_data.datetime_end == datetime(2024, 1, 31)\n\n# Check data quality\nspot_df = raw_data[\"spot\"]\n\n# Verify timestamps are sorted\nassert spot_df[\"timestamp\"].is_sorted()\n\n# Verify timezone-naive\nassert spot_df[\"timestamp\"].dtype == pl.Datetime(\"us\")\n\n# Verify no nulls in key columns\nassert spot_df[\"pair\"].null_count() == 0\nassert spot_df[\"timestamp\"].null_count() == 0\n</code></pre> Note <p>Store connection is automatically closed via finally block. Timestamps are normalized to timezone-naive microseconds. Duplicate detection shows first 10 examples if found. All data sorted by (pair, timestamp) for consistent ordering.</p> Source code in <code>src/signalflow/data/raw_data_factory.py</code> <pre><code>@staticmethod\ndef from_duckdb_spot_store(\n    spot_store_path: Path,\n    pairs: list[str],\n    start: datetime,\n    end: datetime,\n    data_types: list[str] | None = None,\n) -&gt; RawData:\n    \"\"\"Create RawData from DuckDB spot store.\n\n    Loads spot trading data from DuckDB storage with validation,\n    deduplication checks, and schema normalization.\n\n    Processing steps:\n        1. Load data from DuckDB for specified pairs and date range\n        2. Validate required columns (pair, timestamp)\n        3. Remove unnecessary columns (timeframe)\n        4. Normalize timestamps (microseconds, timezone-naive)\n        5. Check for duplicates (pair, timestamp)\n        6. Sort by (pair, timestamp)\n        7. Package into RawData container\n\n    Args:\n        spot_store_path (Path): Path to DuckDB file.\n        pairs (list[str]): List of trading pairs to load (e.g., [\"BTCUSDT\", \"ETHUSDT\"]).\n        start (datetime): Start datetime (inclusive).\n        end (datetime): End datetime (inclusive).\n        data_types (list[str] | None): Data types to load. Default: None.\n            Currently supports: [\"spot\"].\n\n    Returns:\n        RawData: Immutable container with loaded and validated data.\n\n    Raises:\n        ValueError: If required columns missing (pair, timestamp).\n        ValueError: If duplicate (pair, timestamp) combinations detected.\n\n    Example:\n        ```python\n        from pathlib import Path\n        from datetime import datetime\n        from signalflow.data import RawDataFactory\n\n        # Load single pair\n        raw_data = RawDataFactory.from_duckdb_spot_store(\n            spot_store_path=Path(\"data/binance.duckdb\"),\n            pairs=[\"BTCUSDT\"],\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 1, 31),\n            data_types=[\"spot\"]\n        )\n\n        # Load multiple pairs\n        raw_data = RawDataFactory.from_duckdb_spot_store(\n            spot_store_path=Path(\"data/binance.duckdb\"),\n            pairs=[\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\"],\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 12, 31),\n            data_types=[\"spot\"]\n        )\n\n        # Check loaded data\n        spot_df = raw_data[\"spot\"]\n        print(f\"Shape: {spot_df.shape}\")\n        print(f\"Columns: {spot_df.columns}\")\n        print(f\"Pairs: {spot_df['pair'].unique().to_list()}\")\n\n        # Verify no duplicates\n        dup_check = (\n            spot_df.group_by([\"pair\", \"timestamp\"])\n            .len()\n            .filter(pl.col(\"len\") &gt; 1)\n        )\n        assert dup_check.is_empty()\n\n        # Use in pipeline\n        from signalflow.core import RawDataView\n        view = RawDataView(raw=raw_data)\n        spot_pandas = view.to_pandas(\"spot\")\n        ```\n\n    Example:\n        ```python\n        # Handle missing data gracefully\n        try:\n            raw_data = RawDataFactory.from_duckdb_spot_store(\n                spot_store_path=Path(\"data/binance.duckdb\"),\n                pairs=[\"BTCUSDT\"],\n                start=datetime(2024, 1, 1),\n                end=datetime(2024, 1, 31),\n                data_types=[\"spot\"]\n            )\n        except ValueError as e:\n            if \"missing columns\" in str(e):\n                print(\"Data schema invalid\")\n            elif \"Duplicate\" in str(e):\n                print(\"Data contains duplicates\")\n            raise\n\n        # Validate date range\n        assert raw_data.datetime_start == datetime(2024, 1, 1)\n        assert raw_data.datetime_end == datetime(2024, 1, 31)\n\n        # Check data quality\n        spot_df = raw_data[\"spot\"]\n\n        # Verify timestamps are sorted\n        assert spot_df[\"timestamp\"].is_sorted()\n\n        # Verify timezone-naive\n        assert spot_df[\"timestamp\"].dtype == pl.Datetime(\"us\")\n\n        # Verify no nulls in key columns\n        assert spot_df[\"pair\"].null_count() == 0\n        assert spot_df[\"timestamp\"].null_count() == 0\n        ```\n\n    Note:\n        Store connection is automatically closed via finally block.\n        Timestamps are normalized to timezone-naive microseconds.\n        Duplicate detection shows first 10 examples if found.\n        All data sorted by (pair, timestamp) for consistent ordering.\n    \"\"\"\n    data: dict[str, pl.DataFrame] = {}\n    store = DuckDbSpotStore(spot_store_path)\n    try:\n        if \"spot\" in data_types:\n            spot = store.load_many(pairs=pairs, start=start, end=end)\n\n            required = {\"pair\", \"timestamp\"}\n            missing = required - set(spot.columns)\n            if missing:\n                raise ValueError(f\"Spot df missing columns: {sorted(missing)}\")\n\n            if \"timeframe\" in spot.columns:\n                spot = spot.drop(\"timeframe\")\n\n            spot = spot.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")).dt.replace_time_zone(None))\n\n            dup_count = spot.group_by([\"pair\", \"timestamp\"]).len().filter(pl.col(\"len\") &gt; 1)\n            if dup_count.height &gt; 0:\n                dups = (\n                    spot.join(\n                        dup_count.select([\"pair\", \"timestamp\"]),\n                        on=[\"pair\", \"timestamp\"],\n                    )\n                    .select([\"pair\", \"timestamp\"])\n                    .head(10)\n                )\n                raise ValueError(f\"Duplicate (pair, timestamp) detected. Examples:\\n{dups}\")\n\n            spot = spot.sort([\"pair\", \"timestamp\"])\n            data[\"spot\"] = spot\n\n        return RawData(\n            datetime_start=start,\n            datetime_end=end,\n            pairs=pairs,\n            data=data,\n        )\n    finally:\n        store.close()\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_data_factory.RawDataFactory.from_stores","title":"from_stores  <code>staticmethod</code>","text":"<pre><code>from_stores(stores: Sequence[RawDataStore], pairs: list[str], start: datetime, end: datetime) -&gt; RawData\n</code></pre> <p>Create RawData from multiple stores.</p> <p>Loads data from each store and merges into a single RawData container. Each store's data_type becomes the key in RawData.data dict.</p> <p>Parameters:</p> Name Type Description Default <code>stores</code> <code>Sequence[RawDataStore]</code> <p>Sequence of RawDataStore instances.</p> required <code>pairs</code> <code>list[str]</code> <p>List of trading pairs to load.</p> required <code>start</code> <code>datetime</code> <p>Start datetime (inclusive).</p> required <code>end</code> <code>datetime</code> <p>End datetime (inclusive).</p> required <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>Container with merged data from all stores.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If stores have duplicate data_type keys.</p> Example <pre><code>from signalflow.data import RawDataFactory, StoreFactory\n\n# Create stores\nspot_store = StoreFactory.create_raw_store(\n    backend=\"duckdb\",\n    data_type=\"spot\",\n    db_path=\"data/spot.duckdb\",\n)\nfutures_store = StoreFactory.create_raw_store(\n    backend=\"duckdb\",\n    data_type=\"futures\",\n    db_path=\"data/futures.duckdb\",\n)\n\n# Load from multiple stores\nraw_data = RawDataFactory.from_stores(\n    stores=[spot_store, futures_store],\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n)\n\n# Access data by type\nspot_df = raw_data[\"spot\"]\nfutures_df = raw_data[\"futures\"]\n</code></pre> Source code in <code>src/signalflow/data/raw_data_factory.py</code> <pre><code>@staticmethod\ndef from_stores(\n    stores: Sequence[RawDataStore],\n    pairs: list[str],\n    start: datetime,\n    end: datetime,\n) -&gt; RawData:\n    \"\"\"Create RawData from multiple stores.\n\n    Loads data from each store and merges into a single RawData container.\n    Each store's data_type becomes the key in RawData.data dict.\n\n    Args:\n        stores: Sequence of RawDataStore instances.\n        pairs: List of trading pairs to load.\n        start: Start datetime (inclusive).\n        end: End datetime (inclusive).\n\n    Returns:\n        RawData: Container with merged data from all stores.\n\n    Raises:\n        ValueError: If stores have duplicate data_type keys.\n\n    Example:\n        ```python\n        from signalflow.data import RawDataFactory, StoreFactory\n\n        # Create stores\n        spot_store = StoreFactory.create_raw_store(\n            backend=\"duckdb\",\n            data_type=\"spot\",\n            db_path=\"data/spot.duckdb\",\n        )\n        futures_store = StoreFactory.create_raw_store(\n            backend=\"duckdb\",\n            data_type=\"futures\",\n            db_path=\"data/futures.duckdb\",\n        )\n\n        # Load from multiple stores\n        raw_data = RawDataFactory.from_stores(\n            stores=[spot_store, futures_store],\n            pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 12, 31),\n        )\n\n        # Access data by type\n        spot_df = raw_data[\"spot\"]\n        futures_df = raw_data[\"futures\"]\n        ```\n    \"\"\"\n    if not stores:\n        return RawData(\n            datetime_start=start,\n            datetime_end=end,\n            pairs=pairs,\n            data={},\n        )\n\n    merged_data: dict[str, pl.DataFrame] = {}\n\n    for store in stores:\n        raw = store.to_raw_data(pairs=pairs, start=start, end=end)\n        for key, df in raw.data.items():\n            if key in merged_data:\n                raise ValueError(f\"Duplicate data key '{key}' from multiple stores\")\n            merged_data[key] = df\n\n    return RawData(\n        datetime_start=start,\n        datetime_end=end,\n        pairs=pairs,\n        data=merged_data,\n    )\n</code></pre>"},{"location":"api/detector/","title":"Detector Module","text":"<p>Signal detectors and event detectors for real-time market analysis.</p> <p>Module Name</p> <p>The detector functionality is implemented in the <code>signalflow.detector</code> module.</p>"},{"location":"api/detector/#signal-detection","title":"Signal Detection","text":""},{"location":"api/detector/#signalflow.detector.base.SignalDetector","title":"signalflow.detector.base.SignalDetector  <code>dataclass</code>","text":"<pre><code>SignalDetector(signal_category: SignalCategory = SignalCategory.PRICE_DIRECTION, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False)\n</code></pre> <p>               Bases: <code>KwargsTolerantMixin</code>, <code>ABC</code></p> <p>Base class for Polars-first signal detection.</p> Provides standardized pipeline for detecting trading signals from raw data <ol> <li>preprocess: Extract features from raw data</li> <li>detect: Generate signals from features</li> <li>validate: Ensure data quality</li> </ol> Key features <ul> <li>Polars-native for performance</li> <li>Automatic feature extraction via FeaturePipeline</li> <li>Built-in validation (schema, duplicates, timezones)</li> <li>Optional probability requirement</li> <li>Keep latest signal per pair option</li> </ul> Public API <ul> <li>run(): Complete pipeline (preprocess \u2192 detect \u2192 validate)</li> <li>preprocess(): Feature extraction (delegates to FeaturePipeline)</li> <li>detect(): Signal generation (must implement)</li> </ul> <p>Attributes:</p> Name Type Description <code>component_type</code> <code>ClassVar[SfComponentType]</code> <p>Always DETECTOR for registry.</p> <code>pair_col</code> <code>str</code> <p>Trading pair column name. Default: \"pair\".</p> <code>ts_col</code> <code>str</code> <p>Timestamp column name. Default: \"timestamp\".</p> <code>raw_data_type</code> <code>RawDataType</code> <p>Type of raw data to process. Default: SPOT.</p> <code>features</code> <code>Feature | list[Feature] | FeaturePipeline | None</code> <p>Features to compute. Can be a single Feature, list of Features, or FeaturePipeline. If None, preprocess() returns raw OHLCV data. Default: None.</p> <code>require_probability</code> <code>bool</code> <p>Require probability column in signals. Default: False.</p> <code>keep_only_latest_per_pair</code> <code>bool</code> <p>Keep only latest signal per pair. Default: False.</p> Example <pre><code>from signalflow.detector import SignalDetector\nfrom signalflow.core import Signals, SignalType\nimport polars as pl\n\nclass SmaCrossDetector(SignalDetector):\n    '''Simple SMA crossover detector'''\n\n    def __init__(self, fast_window: int = 10, slow_window: int = 20):\n        super().__init__()\n        # Auto-generate features\n        from signalflow.feature import FeaturePipeline, SmaExtractor\n        # Can be FeaturePipeline, list of features, or single feature\n        self.features = FeaturePipeline([\n            SmaExtractor(window=fast_window, column=\"close\"),\n            SmaExtractor(window=slow_window, column=\"close\")\n        ])\n\n    def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n        signals = features.with_columns([\n            # Detect crossover\n            (pl.col(\"sma_10\") &gt; pl.col(\"sma_20\")).alias(\"is_bull\"),\n            (pl.col(\"sma_10\") &lt; pl.col(\"sma_20\")).alias(\"is_bear\")\n        ]).with_columns([\n            # Assign signal type\n            pl.when(pl.col(\"is_bull\"))\n            .then(pl.lit(SignalType.RISE.value))\n            .when(pl.col(\"is_bear\"))\n            .then(pl.lit(SignalType.FALL.value))\n            .otherwise(pl.lit(SignalType.NONE.value))\n            .alias(\"signal_type\")\n        ]).select([\n            self.pair_col,\n            self.ts_col,\n            \"signal_type\",\n            pl.lit(1).alias(\"signal\")\n        ])\n\n        return Signals(signals)\n\n# Usage\ndetector = SmaCrossDetector(fast_window=10, slow_window=20)\nsignals = detector.run(raw_data_view)\n</code></pre> Note <p>Subclasses must implement detect() method. All DataFrames must use timezone-naive timestamps. Duplicate (pair, timestamp) combinations are rejected.</p> See Also <p>FeaturePipeline: Orchestrates feature extraction. Signals: Container for signal output.</p>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.__call__","title":"__call__  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>__call__ = run\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = DETECTOR\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.features","title":"features  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>features: Feature | list[Feature] | FeaturePipeline | None = None\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.keep_only_latest_per_pair","title":"keep_only_latest_per_pair  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keep_only_latest_per_pair: bool = False\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.raw_data_type","title":"raw_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_type: RawDataType | str = SPOT\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.require_probability","title":"require_probability  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>require_probability: bool = False\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = PRICE_DIRECTION\n</code></pre> <p>Signal category this detector produces. Default: PRICE_DIRECTION.</p>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._keep_only_latest","title":"_keep_only_latest","text":"<pre><code>_keep_only_latest(signals: Signals) -&gt; Signals\n</code></pre> <p>Keep only latest signal per pair.</p> <p>Useful for strategies that only trade most recent signal.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals.</p> required <p>Returns:</p> Name Type Description <code>Signals</code> <code>Signals</code> <p>Filtered signals with one per pair.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _keep_only_latest(self, signals: Signals) -&gt; Signals:\n    \"\"\"Keep only latest signal per pair.\n\n    Useful for strategies that only trade most recent signal.\n\n    Args:\n        signals (Signals): Input signals.\n\n    Returns:\n        Signals: Filtered signals with one per pair.\n    \"\"\"\n    s = signals.value\n    out = (\n        s.sort([self.pair_col, self.ts_col])\n        .group_by(self.pair_col, maintain_order=True)\n        .tail(1)\n        .sort([self.pair_col, self.ts_col])\n    )\n    return Signals(out)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._normalize_index","title":"_normalize_index","text":"<pre><code>_normalize_index(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Normalize timestamps to timezone-naive.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame with timezone-naive timestamps.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not pl.DataFrame.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _normalize_index(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Normalize timestamps to timezone-naive.\n\n    Args:\n        df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n        pl.DataFrame: DataFrame with timezone-naive timestamps.\n\n    Raises:\n        TypeError: If df is not pl.DataFrame.\n    \"\"\"\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"Expected pl.DataFrame, got {type(df)}\")\n\n    if self.ts_col in df.columns:\n        ts_dtype = df.schema.get(self.ts_col)\n        if isinstance(ts_dtype, pl.Datetime) and ts_dtype.time_zone is not None:\n            df = df.with_columns(pl.col(self.ts_col).dt.replace_time_zone(None))\n    return df\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._validate_features","title":"_validate_features","text":"<pre><code>_validate_features(df: DataFrame) -&gt; None\n</code></pre> <p>Validate feature DataFrame.</p> Checks <ul> <li>Is pl.DataFrame</li> <li>Has required columns (pair, timestamp)</li> <li>Timestamps are timezone-naive</li> <li>No duplicate (pair, timestamp) combinations</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Features to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If not pl.DataFrame.</p> <code>ValueError</code> <p>If validation fails.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _validate_features(self, df: pl.DataFrame) -&gt; None:\n    \"\"\"Validate feature DataFrame.\n\n    Checks:\n        - Is pl.DataFrame\n        - Has required columns (pair, timestamp)\n        - Timestamps are timezone-naive\n        - No duplicate (pair, timestamp) combinations\n\n    Args:\n        df (pl.DataFrame): Features to validate.\n\n    Raises:\n        TypeError: If not pl.DataFrame.\n        ValueError: If validation fails.\n    \"\"\"\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"preprocess must return polars.DataFrame, got {type(df)}\")\n\n    missing = [c for c in (self.pair_col, self.ts_col) if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Features missing required columns: {missing}\")\n\n    ts_dtype = df.schema.get(self.ts_col)\n    if isinstance(ts_dtype, pl.Datetime) and ts_dtype.time_zone is not None:\n        raise ValueError(\n            f\"Features column '{self.ts_col}' must be timezone-naive, got tz={ts_dtype.time_zone}. \"\n            f\"Use .dt.replace_time_zone(None).\"\n        )\n\n    dup = df.group_by([self.pair_col, self.ts_col]).len().filter(pl.col(\"len\") &gt; 1)\n    if dup.height &gt; 0:\n        raise ValueError(\n            \"Features contain duplicate keys (pair,timestamp). \"\n            f\"Examples:\\n{dup.select([self.pair_col, self.ts_col]).head(10)}\"\n        )\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._validate_signals","title":"_validate_signals","text":"<pre><code>_validate_signals(signals: Signals) -&gt; None\n</code></pre> <p>Validate signal output.</p> Checks <ul> <li>Is Signals instance with pl.DataFrame value</li> <li>Has required columns (pair, timestamp, signal_type)</li> <li>signal_type values are valid SignalType enums</li> <li>Timestamps are timezone-naive</li> <li>No duplicate (pair, timestamp) combinations</li> <li>(optional) Has probability column if required</li> </ul> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Signals to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If not Signals or value not pl.DataFrame.</p> <code>ValueError</code> <p>If validation fails.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _validate_signals(self, signals: Signals) -&gt; None:\n    \"\"\"Validate signal output.\n\n    Checks:\n        - Is Signals instance with pl.DataFrame value\n        - Has required columns (pair, timestamp, signal_type)\n        - signal_type values are valid SignalType enums\n        - Timestamps are timezone-naive\n        - No duplicate (pair, timestamp) combinations\n        - (optional) Has probability column if required\n\n    Args:\n        signals (Signals): Signals to validate.\n\n    Raises:\n        TypeError: If not Signals or value not pl.DataFrame.\n        ValueError: If validation fails.\n    \"\"\"\n    if not isinstance(signals, Signals):\n        raise TypeError(f\"detect must return Signals, got {type(signals)}\")\n\n    s = signals.value\n    if not isinstance(s, pl.DataFrame):\n        raise TypeError(f\"Signals.value must be polars.DataFrame, got {type(s)}\")\n\n    required = {self.pair_col, self.ts_col, \"signal_type\"}\n    missing = sorted(required - set(s.columns))\n    if missing:\n        raise ValueError(f\"Signals missing required columns: {missing}\")\n\n    allowed = getattr(self, \"allowed_signal_types\", None)\n    if allowed:\n        non_null = s.filter(pl.col(\"signal_type\").is_not_null())\n        bad = non_null.select(pl.col(\"signal_type\")).unique().filter(~pl.col(\"signal_type\").is_in(list(allowed)))\n        if bad.height &gt; 0:\n            raise ValueError(\n                f\"Signals contain unknown signal_type values: {bad.get_column('signal_type').to_list()}\"\n            )\n\n    if self.require_probability and \"probability\" not in s.columns:\n        raise ValueError(\"Signals must contain 'probability' column (require_probability=True)\")\n\n    ts_dtype = s.schema.get(self.ts_col)\n    if isinstance(ts_dtype, pl.Datetime) and ts_dtype.time_zone is not None:\n        raise ValueError(f\"Signals column '{self.ts_col}' must be timezone-naive, got tz={ts_dtype.time_zone}.\")\n\n    # optional: hard guarantee no duplicates in signals\n    dup = s.group_by([self.pair_col, self.ts_col]).len().filter(pl.col(\"len\") &gt; 1)\n    if dup.height &gt; 0:\n        raise ValueError(\n            \"Signals contain duplicate keys (pair,timestamp). \"\n            f\"Examples:\\n{dup.select([self.pair_col, self.ts_col]).head(10)}\"\n        )\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.detect","title":"detect  <code>abstractmethod</code>","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Generate signals from features.</p> <p>Core detection logic - must be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Preprocessed features.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Signals</code> <code>Signals</code> <p>Detected signals with columns: - pair (str): Trading pair - timestamp (datetime): Signal timestamp (timezone-naive) - signal_type (int): SignalType enum value - signal (int | float): Signal value - probability (float, optional): Signal probability</p> Example <pre><code>def detect(self, features, context=None):\n    # Simple threshold detector\n    signals = features.filter(\n        pl.col(\"rsi\") &gt; 70  # Overbought\n    ).with_columns([\n        pl.lit(SignalType.FALL.value).alias(\"signal_type\"),\n        pl.lit(-1).alias(\"signal\"),\n        pl.lit(0.8).alias(\"probability\")\n    ]).select([\n        self.pair_col,\n        self.ts_col,\n        \"signal_type\",\n        \"signal\",\n        \"probability\"\n    ])\n\n    return Signals(signals)\n</code></pre> Note <p>Must return Signals with at minimum: pair, timestamp, signal_type. Timestamps must be timezone-naive. No duplicate (pair, timestamp) combinations allowed.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>@abstractmethod\ndef detect(self, features: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; Signals:\n    \"\"\"Generate signals from features.\n\n    Core detection logic - must be implemented by subclasses.\n\n    Args:\n        features (pl.DataFrame): Preprocessed features.\n        context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        Signals: Detected signals with columns:\n            - pair (str): Trading pair\n            - timestamp (datetime): Signal timestamp (timezone-naive)\n            - signal_type (int): SignalType enum value\n            - signal (int | float): Signal value\n            - probability (float, optional): Signal probability\n\n    Example:\n        ```python\n        def detect(self, features, context=None):\n            # Simple threshold detector\n            signals = features.filter(\n                pl.col(\"rsi\") &gt; 70  # Overbought\n            ).with_columns([\n                pl.lit(SignalType.FALL.value).alias(\"signal_type\"),\n                pl.lit(-1).alias(\"signal\"),\n                pl.lit(0.8).alias(\"probability\")\n            ]).select([\n                self.pair_col,\n                self.ts_col,\n                \"signal_type\",\n                \"signal\",\n                \"probability\"\n            ])\n\n            return Signals(signals)\n        ```\n\n    Note:\n        Must return Signals with at minimum: pair, timestamp, signal_type.\n        Timestamps must be timezone-naive.\n        No duplicate (pair, timestamp) combinations allowed.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Extract features from raw data.</p> Base implementation <ol> <li>Load raw OHLCV data from raw_data_view</li> <li>Apply features_pipe if provided</li> </ol> <p>Subclasses can override to add helper columns for their detection method.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Features with at minimum pair and timestamp columns.</p> Example <pre><code># Base: returns OHLCV (if features is None)\n# or OHLCV + computed features (if features is provided)\nfeats = detector.preprocess(raw_data_view)\n\n# Custom override to add helper columns\nclass ZScoreDetector(SignalDetector):\n    target_feature: str = \"RSI_14\"\n    rolling_window: int = 100\n\n    def preprocess(self, raw_data_view, context=None):\n        # 1. Base preprocessing (OHLCV + features)\n        df = super().preprocess(raw_data_view, context)\n\n        # 2. Add helper columns for z-score method\n        df = df.with_columns([\n            pl.col(self.target_feature)\n                .rolling_mean(window_size=self.rolling_window)\n                .over(self.pair_col)\n                .alias(\"_target_rol_mean\"),\n            pl.col(self.target_feature)\n                .rolling_std(window_size=self.rolling_window)\n                .over(self.pair_col)\n                .alias(\"_target_rol_std\"),\n        ])\n        return df\n</code></pre> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def preprocess(self, raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Extract features from raw data.\n\n    Base implementation:\n        1. Load raw OHLCV data from raw_data_view\n        2. Apply features_pipe if provided\n\n    Subclasses can override to add helper columns for their detection method.\n\n    Args:\n        raw_data_view (RawDataView): View to raw market data.\n        context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Features with at minimum pair and timestamp columns.\n\n    Example:\n        ```python\n        # Base: returns OHLCV (if features is None)\n        # or OHLCV + computed features (if features is provided)\n        feats = detector.preprocess(raw_data_view)\n\n        # Custom override to add helper columns\n        class ZScoreDetector(SignalDetector):\n            target_feature: str = \"RSI_14\"\n            rolling_window: int = 100\n\n            def preprocess(self, raw_data_view, context=None):\n                # 1. Base preprocessing (OHLCV + features)\n                df = super().preprocess(raw_data_view, context)\n\n                # 2. Add helper columns for z-score method\n                df = df.with_columns([\n                    pl.col(self.target_feature)\n                        .rolling_mean(window_size=self.rolling_window)\n                        .over(self.pair_col)\n                        .alias(\"_target_rol_mean\"),\n                    pl.col(self.target_feature)\n                        .rolling_std(window_size=self.rolling_window)\n                        .over(self.pair_col)\n                        .alias(\"_target_rol_std\"),\n                ])\n                return df\n        ```\n    \"\"\"\n    # 1. Load raw OHLCV data\n    key = self.raw_data_type.value if hasattr(self.raw_data_type, \"value\") else str(self.raw_data_type)\n    df = raw_data_view.to_polars(key).sort([self.pair_col, self.ts_col])\n\n    # 2. Apply features if provided\n    if self.features is not None:\n        if isinstance(self.features, FeaturePipeline):\n            df = self.features.compute(df, context=context)\n        elif isinstance(self.features, list):\n            for feat in self.features:\n                df = feat.compute(df, context=context)\n        elif isinstance(self.features, Feature):\n            df = self.features.compute(df, context=context)\n        else:\n            raise TypeError(\n                f\"features must be Feature, list[Feature], or FeaturePipeline, got {type(self.features)}\"\n            )\n\n        if not isinstance(df, pl.DataFrame):\n            raise TypeError(f\"{self.__class__.__name__}.features.compute must return pl.DataFrame, got {type(df)}\")\n\n    return df\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.run","title":"run","text":"<pre><code>run(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Execute complete detection pipeline.</p> Pipeline steps <ol> <li>preprocess: Extract features</li> <li>normalize: Ensure timezone-naive timestamps</li> <li>validate features: Check schema and duplicates</li> <li>detect: Generate signals</li> <li>validate signals: Check output quality</li> <li>(optional) keep latest: Filter to latest per pair</li> </ol> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context for detection.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Signals</code> <code>Signals</code> <p>Detected signals.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If preprocess doesn't return pl.DataFrame.</p> <code>ValueError</code> <p>If features/signals fail validation.</p> Example <pre><code>from signalflow.core import RawData, RawDataView\n\n# Create view\nview = RawDataView(raw=raw_data)\n\n# Run detection\nsignals = detector.run(view)\n\n# With context\nsignals = detector.run(view, context={\"threshold\": 0.7})\n</code></pre> Note <p>Can also be called directly: detector(raw_data_view). All validation errors include helpful diagnostic information.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def run(self, raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; Signals:\n    \"\"\"Execute complete detection pipeline.\n\n    Pipeline steps:\n        1. preprocess: Extract features\n        2. normalize: Ensure timezone-naive timestamps\n        3. validate features: Check schema and duplicates\n        4. detect: Generate signals\n        5. validate signals: Check output quality\n        6. (optional) keep latest: Filter to latest per pair\n\n    Args:\n        raw_data_view (RawDataView): View to raw market data.\n        context (dict[str, Any] | None): Additional context for detection.\n\n    Returns:\n        Signals: Detected signals.\n\n    Raises:\n        TypeError: If preprocess doesn't return pl.DataFrame.\n        ValueError: If features/signals fail validation.\n\n    Example:\n        ```python\n        from signalflow.core import RawData, RawDataView\n\n        # Create view\n        view = RawDataView(raw=raw_data)\n\n        # Run detection\n        signals = detector.run(view)\n\n        # With context\n        signals = detector.run(view, context={\"threshold\": 0.7})\n        ```\n\n    Note:\n        Can also be called directly: detector(raw_data_view).\n        All validation errors include helpful diagnostic information.\n    \"\"\"\n    feats = self.preprocess(raw_data_view, context=context)\n    feats = self._normalize_index(feats)\n    self._validate_features(feats)\n\n    signals = self.detect(feats, context=context)\n    self._validate_signals(signals)\n\n    if self.keep_only_latest_per_pair:\n        signals = self._keep_only_latest(signals)\n\n    return signals\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.ExampleSmaCrossDetector","title":"signalflow.detector.sma_cross.ExampleSmaCrossDetector  <code>dataclass</code>","text":"<pre><code>ExampleSmaCrossDetector(signal_category: SignalCategory = SignalCategory.PRICE_DIRECTION, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = None, fast_period: int = 20, slow_period: int = 50, price_col: str = 'close')\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>SMA crossover signal detector.</p> Signals <ul> <li>\"rise\": fast crosses above slow</li> <li>\"fall\": fast crosses below slow</li> </ul>"},{"location":"api/detector/#signalflow.detector.sma_cross.ExampleSmaCrossDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = None\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.ExampleSmaCrossDetector.fast_period","title":"fast_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fast_period: int = 20\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.ExampleSmaCrossDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.ExampleSmaCrossDetector.slow_period","title":"slow_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>slow_period: int = 50\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.ExampleSmaCrossDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/sma_cross.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.fast_period &gt;= self.slow_period:\n        raise ValueError(f\"fast_period must be &lt; slow_period\")\n\n    self.fast_col = f\"sma_{self.fast_period}\"\n    self.slow_col = f\"sma_{self.slow_period}\"\n    self.allowed_signal_types = {\"rise\", \"fall\"}\n\n    self.features = [\n        ExampleSmaFeature(period=self.fast_period, price_col=self.price_col),\n        ExampleSmaFeature(period=self.slow_period, price_col=self.price_col),\n    ]\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.ExampleSmaCrossDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> Source code in <code>src/signalflow/detector/sma_cross.py</code> <pre><code>def detect(self, features: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; Signals:\n    df = features.sort([self.pair_col, self.ts_col])\n\n    df = df.filter(pl.col(self.fast_col).is_not_null() &amp; pl.col(self.slow_col).is_not_null())\n\n    fast = pl.col(self.fast_col)\n    slow = pl.col(self.slow_col)\n    fast_prev = fast.shift(1).over(self.pair_col)\n    slow_prev = slow.shift(1).over(self.pair_col)\n\n    cross_up = (fast &gt; slow) &amp; (fast_prev &lt;= slow_prev)\n    cross_down = (fast &lt; slow) &amp; (fast_prev &gt;= slow_prev)\n\n    out = df.select(\n        [\n            self.pair_col,\n            self.ts_col,\n            pl.when(cross_up)\n            .then(pl.lit(\"rise\"))\n            .when(cross_down)\n            .then(pl.lit(\"fall\"))\n            .otherwise(pl.lit(None, dtype=pl.Utf8))\n            .alias(\"signal_type\"),\n            pl.when(cross_up).then(1).when(cross_down).then(-1).otherwise(0).alias(\"signal\"),\n        ]\n    ).filter(pl.col(\"signal_type\").is_not_null())\n\n    return Signals(out)\n</code></pre>"},{"location":"api/detector/#real-time-detectors","title":"Real-Time Detectors","text":""},{"location":"api/detector/#anomaly-detector","title":"Anomaly Detector","text":""},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector","title":"signalflow.detector.anomaly_detector.AnomalyDetector  <code>dataclass</code>","text":"<pre><code>AnomalyDetector(signal_category: SignalCategory = SignalCategory.ANOMALY, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'extreme_positive_anomaly', 'extreme_negative_anomaly'})(), price_col: str = 'close', vol_window: int = 1440, threshold_return_std: float = 4.0)\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Detects anomalous price movements in real-time (backward-looking only).</p> <p>Unlike <code>AnomalyLabeler</code>, this detector uses only past data and is safe for live trading. It flags the current bar as anomalous when the current return exceeds a multiple of rolling volatility.</p> Algorithm <ol> <li>Compute log returns: log(close[t] / close[t-1])</li> <li>Compute rolling std of returns over <code>vol_window</code> bars</li> <li>Current bar return magnitude: |log_return[t]|</li> <li>If magnitude &gt; threshold_return_std * rolling_std[t] -&gt; \"extreme_positive_anomaly\"</li> <li>If magnitude &gt; threshold AND return is negative -&gt; \"extreme_negative_anomaly\"</li> <li>Otherwise: row is skipped (no signal emitted)</li> </ol> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column name. Default: \"close\".</p> <code>vol_window</code> <code>int</code> <p>Rolling window for volatility estimation. Default: 1440.</p> <code>threshold_return_std</code> <code>float</code> <p>Number of standard deviations for anomaly threshold. Default: 4.0.</p> Example <pre><code>from signalflow.core import RawData, RawDataView\nfrom signalflow.detector.anomaly_detector import AnomalyDetector\n\ndetector = AnomalyDetector(\n    vol_window=1440,\n    threshold_return_std=4.0,\n)\nsignals = detector.run(raw_data_view)\n</code></pre> Note <p>This detector overrides <code>preprocess()</code> to work directly with raw OHLCV data and does not require a FeaturePipeline.</p>"},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'extreme_positive_anomaly', 'extreme_negative_anomaly'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = ANOMALY\n</code></pre>"},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector.threshold_return_std","title":"threshold_return_std  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>threshold_return_std: float = 4.0\n</code></pre>"},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector.vol_window","title":"vol_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_window: int = 1440\n</code></pre>"},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect anomalous price movements on the current bar.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>OHLCV data with pair and timestamp columns.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Signals</code> <code>Signals</code> <p>Detected anomaly signals with columns: pair, timestamp, signal_type, signal, probability.</p> Source code in <code>src/signalflow/detector/anomaly_detector.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect anomalous price movements on the current bar.\n\n    Args:\n        features (pl.DataFrame): OHLCV data with pair and timestamp columns.\n        context (dict[str, Any] | None): Additional context (unused).\n\n    Returns:\n        Signals: Detected anomaly signals with columns:\n            pair, timestamp, signal_type, signal, probability.\n    \"\"\"\n    price = pl.col(self.price_col)\n\n    # Step 1: log returns (per pair)\n    df = features.with_columns(\n        (price / price.shift(1).over(self.pair_col)).log().alias(\"_log_ret\"),\n    )\n\n    # Step 2: rolling std of returns (per pair)\n    df = df.with_columns(\n        pl.col(\"_log_ret\")\n        .rolling_std(window_size=self.vol_window, min_samples=max(2, self.vol_window // 4))\n        .over(self.pair_col)\n        .alias(\"_rolling_vol\"),\n    )\n\n    # Step 3: current bar return magnitude\n    df = df.with_columns(\n        pl.col(\"_log_ret\").abs().alias(\"_ret_abs\"),\n    )\n\n    # Step 4-5: classify\n    threshold = pl.col(\"_rolling_vol\") * self.threshold_return_std\n\n    is_anomaly = (\n        pl.col(\"_ret_abs\").is_not_null()\n        &amp; pl.col(\"_rolling_vol\").is_not_null()\n        &amp; (pl.col(\"_rolling_vol\") &gt; 0)\n        &amp; (pl.col(\"_ret_abs\") &gt; threshold)\n    )\n\n    is_flash_crash = is_anomaly &amp; (pl.col(\"_log_ret\") &lt; 0)\n\n    signal_type_expr = (\n        pl.when(is_flash_crash)\n        .then(pl.lit(\"extreme_negative_anomaly\"))\n        .when(is_anomaly)\n        .then(pl.lit(\"extreme_positive_anomaly\"))\n        .otherwise(pl.lit(None, dtype=pl.Utf8))\n        .alias(\"signal_type\")\n    )\n\n    # Compute probability as ratio of return magnitude to threshold\n    probability_expr = (\n        pl.when(is_anomaly)\n        .then((pl.col(\"_ret_abs\") / threshold).clip(0.0, 1.0))\n        .otherwise(pl.lit(None, dtype=pl.Float64))\n        .alias(\"probability\")\n    )\n\n    df = df.with_columns([signal_type_expr, probability_expr])\n\n    # Step 6: filter to only anomalous bars (skip rows with no signal)\n    signals_df = df.filter(pl.col(\"signal_type\").is_not_null()).select(\n        [\n            self.pair_col,\n            self.ts_col,\n            \"signal_type\",\n            pl.lit(1).alias(\"signal\"),\n            \"probability\",\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.anomaly_detector.AnomalyDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Extract raw OHLCV data without feature pipeline.</p> <p>Overrides base <code>preprocess()</code> to bypass FeaturePipeline and return the raw spot data directly.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Raw OHLCV data sorted by (pair, timestamp).</p> Source code in <code>src/signalflow/detector/anomaly_detector.py</code> <pre><code>def preprocess(\n    self,\n    raw_data_view: RawDataView,\n    context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Extract raw OHLCV data without feature pipeline.\n\n    Overrides base ``preprocess()`` to bypass FeaturePipeline and return\n    the raw spot data directly.\n\n    Args:\n        raw_data_view (RawDataView): View to raw market data.\n        context (dict[str, Any] | None): Additional context (unused).\n\n    Returns:\n        pl.DataFrame: Raw OHLCV data sorted by (pair, timestamp).\n    \"\"\"\n    key = self.raw_data_type.value if hasattr(self.raw_data_type, \"value\") else str(self.raw_data_type)\n    return raw_data_view.to_polars(key).sort([self.pair_col, self.ts_col])\n</code></pre>"},{"location":"api/detector/#volatility-detector","title":"Volatility Detector","text":""},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector","title":"signalflow.detector.volatility_detector.VolatilityDetector  <code>dataclass</code>","text":"<pre><code>VolatilityDetector(signal_category: SignalCategory = SignalCategory.VOLATILITY, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'high_volatility', 'low_volatility'})(), price_col: str = 'close', vol_window: int = 60, lookback_window: int = 1440, upper_quantile: float = 0.67, lower_quantile: float = 0.33)\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Detects volatility regime shifts in real-time (backward-looking only).</p> <p>Unlike <code>VolatilityRegimeLabeler</code>, this detector uses only past and current data and is safe for live trading.</p> Algorithm <ol> <li>Compute log returns: log(close[t] / close[t-1])</li> <li>Backward realized volatility: std of last <code>vol_window</code> returns</li> <li>Rolling percentile of realized vol over <code>lookback_window</code></li> <li>If percentile &gt; upper_quantile -&gt; \"high_volatility\"</li> <li>If percentile &lt; lower_quantile -&gt; \"low_volatility\"</li> <li>Otherwise: no signal emitted</li> </ol> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column name. Default: \"close\".</p> <code>vol_window</code> <code>int</code> <p>Window for realized vol calculation. Default: 60.</p> <code>lookback_window</code> <code>int</code> <p>Window for percentile computation. Default: 1440.</p> <code>upper_quantile</code> <code>float</code> <p>Upper percentile threshold. Default: 0.67.</p> <code>lower_quantile</code> <code>float</code> <p>Lower percentile threshold. Default: 0.33.</p> Example <pre><code>from signalflow.detector.volatility_detector import VolatilityDetector\n\ndetector = VolatilityDetector(\n    vol_window=60,\n    upper_quantile=0.67,\n    lower_quantile=0.33,\n)\nsignals = detector.run(raw_data_view)\n</code></pre> Note <p>This detector overrides <code>preprocess()</code> to work directly with raw OHLCV data and does not require a FeaturePipeline.</p>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'high_volatility', 'low_volatility'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.lookback_window","title":"lookback_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookback_window: int = 1440\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.lower_quantile","title":"lower_quantile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lower_quantile: float = 0.33\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = VOLATILITY\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.upper_quantile","title":"upper_quantile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>upper_quantile: float = 0.67\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.vol_window","title":"vol_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_window: int = 60\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect volatility regime from backward-looking realized vol.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>OHLCV data with pair and timestamp columns.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with high_volatility/low_volatility signal types.</p> Source code in <code>src/signalflow/detector/volatility_detector.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect volatility regime from backward-looking realized vol.\n\n    Args:\n        features: OHLCV data with pair and timestamp columns.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with high_volatility/low_volatility signal types.\n    \"\"\"\n    price = pl.col(self.price_col)\n\n    # Step 1: log returns (per pair)\n    df = features.with_columns(\n        (price / price.shift(1).over(self.pair_col)).log().alias(\"_log_ret\"),\n    )\n\n    # Step 2: backward realized volatility (rolling std of returns)\n    df = df.with_columns(\n        pl.col(\"_log_ret\")\n        .rolling_std(window_size=self.vol_window, min_samples=max(2, self.vol_window // 4))\n        .over(self.pair_col)\n        .alias(\"_realized_vol\"),\n    )\n\n    # Step 3-5: compute rolling percentile and classify per group\n    # Polars doesn't have rolling_quantile with a rank, so we compute via numpy per group\n    results = []\n    for pair_name, group in df.group_by(self.pair_col, maintain_order=True):\n        vol_arr = group[\"_realized_vol\"].to_numpy().astype(np.float64)\n        n = len(vol_arr)\n\n        signal_types = [None] * n\n        probabilities = [None] * n\n\n        for t in range(n):\n            if np.isnan(vol_arr[t]):\n                continue\n\n            lb_start = max(0, t - self.lookback_window + 1)\n            window = vol_arr[lb_start : t + 1]\n            valid = window[~np.isnan(window)]\n            if len(valid) &lt; 2:\n                continue\n\n            percentile = float(np.mean(valid &lt;= vol_arr[t]))\n\n            if percentile &gt; self.upper_quantile:\n                signal_types[t] = \"high_volatility\"\n                probabilities[t] = percentile\n            elif percentile &lt; self.lower_quantile:\n                signal_types[t] = \"low_volatility\"\n                probabilities[t] = 1.0 - percentile\n\n        group = group.with_columns(\n            [\n                pl.Series(name=\"signal_type\", values=signal_types, dtype=pl.Utf8),\n                pl.Series(name=\"probability\", values=probabilities, dtype=pl.Float64),\n            ]\n        )\n        results.append(group)\n\n    if not results:\n        return Signals(pl.DataFrame())\n\n    combined = pl.concat(results, how=\"vertical_relaxed\")\n\n    # Filter to only bars with signals\n    signals_df = combined.filter(pl.col(\"signal_type\").is_not_null()).select(\n        [\n            self.pair_col,\n            self.ts_col,\n            \"signal_type\",\n            pl.lit(1).alias(\"signal\"),\n            \"probability\",\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.volatility_detector.VolatilityDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Extract raw OHLCV data without feature pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Raw OHLCV data sorted by (pair, timestamp).</p> Source code in <code>src/signalflow/detector/volatility_detector.py</code> <pre><code>def preprocess(\n    self,\n    raw_data_view: RawDataView,\n    context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Extract raw OHLCV data without feature pipeline.\n\n    Args:\n        raw_data_view: View to raw market data.\n        context: Additional context (unused).\n\n    Returns:\n        Raw OHLCV data sorted by (pair, timestamp).\n    \"\"\"\n    key = self.raw_data_type.value if hasattr(self.raw_data_type, \"value\") else str(self.raw_data_type)\n    return raw_data_view.to_polars(key).sort([self.pair_col, self.ts_col])\n</code></pre>"},{"location":"api/detector/#structure-detector-local-extrema","title":"Structure Detector (Local Extrema)","text":""},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector","title":"signalflow.detector.structure_detector.StructureDetector  <code>dataclass</code>","text":"<pre><code>StructureDetector(signal_category: SignalCategory = SignalCategory.PRICE_STRUCTURE, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'local_max', 'local_min'})(), price_col: str = 'close', lookback: int = 60, confirmation_bars: int = 10, min_swing_pct: float = 0.02)\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Detects local price structure (tops/bottoms) in real-time.</p> <p>Unlike <code>StructureLabeler</code>, this detector uses only past data and requires a confirmation delay -- a local extremum is only confirmed after <code>confirmation_bars</code> bars have passed showing the reversal.</p> Algorithm <ol> <li>For each bar t, look back <code>lookback</code> bars</li> <li>Find the max and min in the lookback window</li> <li>A local_max is confirmed when:</li> <li>The max occurred at bar (t - confirmation_bars) or earlier</li> <li>Price has dropped &gt;= min_swing_pct from the max</li> <li>Current price &lt; max price</li> <li>A local_min is confirmed when:</li> <li>The min occurred at bar (t - confirmation_bars) or earlier</li> <li>Price has risen &gt;= min_swing_pct from the min</li> <li>Current price &gt; min price</li> <li>Only emit the signal once at the confirmation bar</li> </ol> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column name. Default: \"close\".</p> <code>lookback</code> <code>int</code> <p>Backward window for extrema search. Default: 60.</p> <code>confirmation_bars</code> <code>int</code> <p>Bars of reversal needed for confirmation. Default: 10.</p> <code>min_swing_pct</code> <code>float</code> <p>Minimum swing percentage. Default: 0.02.</p> Example <pre><code>from signalflow.detector.structure_detector import StructureDetector\n\ndetector = StructureDetector(\n    lookback=60,\n    confirmation_bars=10,\n    min_swing_pct=0.02,\n)\nsignals = detector.run(raw_data_view)\n</code></pre> Note <p>This detector overrides <code>preprocess()</code> to work directly with raw OHLCV data and does not require a FeaturePipeline.</p>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'local_max', 'local_min'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.confirmation_bars","title":"confirmation_bars  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confirmation_bars: int = 10\n</code></pre>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.lookback","title":"lookback  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookback: int = 60\n</code></pre>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.min_swing_pct","title":"min_swing_pct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_swing_pct: float = 0.02\n</code></pre>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = PRICE_STRUCTURE\n</code></pre>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect local tops/bottoms with confirmation delay.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>OHLCV data with pair and timestamp columns.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with local_max/local_min signal types.</p> Source code in <code>src/signalflow/detector/structure_detector.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect local tops/bottoms with confirmation delay.\n\n    Args:\n        features: OHLCV data with pair and timestamp columns.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with local_max/local_min signal types.\n    \"\"\"\n    results = []\n\n    for pair_name, group in features.group_by(self.pair_col, maintain_order=True):\n        prices = group[self.price_col].to_numpy().astype(np.float64)\n        n = len(prices)\n\n        signal_types = [None] * n\n        probabilities = [None] * n\n\n        # Track last emitted extremum to avoid duplicates\n        last_emitted_type = None\n        last_emitted_idx = -self.lookback\n\n        for t in range(self.lookback + self.confirmation_bars, n):\n            p_current = prices[t]\n            if np.isnan(p_current):\n                continue\n\n            # Search window: [t - lookback, t - confirmation_bars]\n            search_start = t - self.lookback\n            search_end = t - self.confirmation_bars + 1\n\n            if search_end &lt;= search_start:\n                continue\n\n            search_window = prices[search_start:search_end]\n            valid_mask = ~np.isnan(search_window)\n            if not np.any(valid_mask):\n                continue\n\n            valid_prices = search_window[valid_mask]\n\n            max_val = np.max(valid_prices)\n            min_val = np.min(valid_prices)\n\n            # Check local_max: max in search window, price dropped since\n            if max_val &gt; 0 and p_current &lt; max_val:\n                swing = (max_val - p_current) / max_val\n                if swing &gt;= self.min_swing_pct:\n                    if last_emitted_type != \"local_max\" or (t - last_emitted_idx) &gt; self.lookback:\n                        signal_types[t] = \"local_max\"\n                        probabilities[t] = min(1.0, swing / (self.min_swing_pct * 3))\n                        last_emitted_type = \"local_max\"\n                        last_emitted_idx = t\n                        continue\n\n            # Check local_min: min in search window, price risen since\n            if min_val &gt; 0 and p_current &gt; min_val:\n                swing = (p_current - min_val) / min_val\n                if swing &gt;= self.min_swing_pct:\n                    if last_emitted_type != \"local_min\" or (t - last_emitted_idx) &gt; self.lookback:\n                        signal_types[t] = \"local_min\"\n                        probabilities[t] = min(1.0, swing / (self.min_swing_pct * 3))\n                        last_emitted_type = \"local_min\"\n                        last_emitted_idx = t\n\n        group = group.with_columns(\n            [\n                pl.Series(name=\"signal_type\", values=signal_types, dtype=pl.Utf8),\n                pl.Series(name=\"probability\", values=probabilities, dtype=pl.Float64),\n            ]\n        )\n        results.append(group)\n\n    if not results:\n        return Signals(pl.DataFrame())\n\n    combined = pl.concat(results, how=\"vertical_relaxed\")\n\n    signals_df = combined.filter(pl.col(\"signal_type\").is_not_null()).select(\n        [\n            self.pair_col,\n            self.ts_col,\n            \"signal_type\",\n            pl.lit(1).alias(\"signal\"),\n            \"probability\",\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.structure_detector.StructureDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Extract raw OHLCV data without feature pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Raw OHLCV data sorted by (pair, timestamp).</p> Source code in <code>src/signalflow/detector/structure_detector.py</code> <pre><code>def preprocess(\n    self,\n    raw_data_view: RawDataView,\n    context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Extract raw OHLCV data without feature pipeline.\n\n    Args:\n        raw_data_view: View to raw market data.\n        context: Additional context (unused).\n\n    Returns:\n        Raw OHLCV data sorted by (pair, timestamp).\n    \"\"\"\n    key = self.raw_data_type.value if hasattr(self.raw_data_type, \"value\") else str(self.raw_data_type)\n    return raw_data_view.to_polars(key).sort([self.pair_col, self.ts_col])\n</code></pre>"},{"location":"api/detector/#market-wide-detection","title":"Market-Wide Detection","text":"<p>Exogenous market-wide signals (regulatory news, rate decisions, black swans) cause correlated price moves that no feature could predict. Market-wide detectors identify these timestamps so that labels near them can be masked (set to null), preventing MI estimate pollution.</p> <p>All detectors extend SignalDetector with <code>signal_category=MARKET_WIDE</code>.</p> <pre><code>SignalDetector (signal_category=MARKET_WIDE)\n    \u251c\u2500\u2500 AgreementDetector         @sf_component(\"market/agreement\")\n    \u251c\u2500\u2500 MarketZScoreDetector      @sf_component(\"market/zscore\")\n    \u2514\u2500\u2500 MarketCusumDetector       @sf_component(\"market/cusum\")\n</code></pre>"},{"location":"api/detector/#usage","title":"Usage","text":"<pre><code>from signalflow.detector.market import MarketZScoreDetector, MarketCusumDetector\nfrom signalflow.target.utils import mask_targets_by_signals\n\n# Z-Score: detects sudden shocks (z-score of aggregate cross-pair return)\nzscore_det = MarketZScoreDetector(z_threshold=6.0, rolling_window=500)\nsignals = zscore_det.run(raw_data_view)\n# Default signal_type_name: \"aggregate_outlier\"\n\n# CUSUM: detects sustained regime shifts (cumulative sum of deviations)\ncusum_det = MarketCusumDetector(drift=0.005, cusum_threshold=0.05)\nsignals = cusum_det.run(raw_data_view)\n# Default signal_type_name: \"structural_break\"\n\n# Mask labels near detected signals\ndf_masked = mask_targets_by_signals(\n    df=df,\n    signals=signals,\n    mask_signal_types={\"aggregate_outlier\", \"structural_break\"},\n    horizon_bars=60,\n)\n</code></pre>"},{"location":"api/detector/#agreement-based-detector","title":"Agreement-Based Detector","text":""},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector","title":"signalflow.detector.market.agreement_detector.AgreementDetector  <code>dataclass</code>","text":"<pre><code>AgreementDetector(signal_category: SignalCategory = SignalCategory.MARKET_WIDE, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'synchronization'})(), agreement_threshold: float = 0.8, min_pairs: int = 5, return_window: int = 1, price_col: str = 'close', signal_type_name: str = 'synchronization')\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Detects timestamps where cross-pair return agreement is abnormally high.</p> <p>A market-wide detector that signals when a high fraction of trading pairs move in the same direction simultaneously. This indicates exogenous macro events (interest rate decisions, regulatory news, etc.) that cannot be predicted from individual pair features.</p> Algorithm <ol> <li>Compute log-return for each pair at each timestamp.</li> <li>At each timestamp, compute the fraction of pairs with    same-sign return (majority sign).</li> <li>If fraction &gt;= <code>agreement_threshold</code>, emit signal.</li> </ol> <p>Attributes:</p> Name Type Description <code>agreement_threshold</code> <code>float</code> <p>Fraction of pairs that must agree for detection.</p> <code>min_pairs</code> <code>int</code> <p>Minimum number of active pairs at a timestamp.</p> <code>return_window</code> <code>int</code> <p>Bars for return computation.</p> <code>signal_type_name</code> <code>str</code> <p>Signal type name for detected signals.</p> Example <pre><code>from signalflow.detector import AgreementDetector\nfrom signalflow.target.utils import mask_targets_by_signals\n\n# Detect market-wide agreement\ndetector = AgreementDetector(agreement_threshold=0.8)\nsignals = detector.run(raw_data_view)\n\n# Mask labels overlapping with detected signals\nlabeled_df = mask_targets_by_signals(\n    df=labeled_df,\n    signals=signals,\n    mask_signal_types={\"synchronization\"},\n    horizon_bars=60,\n)\n</code></pre> Note <p>Returns one signal per timestamp (not per pair) since market-wide signals affect all pairs simultaneously. The signal has a synthetic \"ALL\" pair.</p>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.agreement_threshold","title":"agreement_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>agreement_threshold: float = 0.8\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'synchronization'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = DETECTOR\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.min_pairs","title":"min_pairs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_pairs: int = 5\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.return_window","title":"return_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>return_window: int = 1\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = MARKET_WIDE\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.signal_type_name","title":"signal_type_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_type_name: str = 'synchronization'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/market/agreement_detector.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.allowed_signal_types = {self.signal_type_name}\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect market-wide agreement timestamps.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Multi-pair OHLCV DataFrame with _ret column.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with synchronization signal type for detected timestamps.</p> Source code in <code>src/signalflow/detector/market/agreement_detector.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect market-wide agreement timestamps.\n\n    Args:\n        features: Multi-pair OHLCV DataFrame with _ret column.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with synchronization signal type for detected timestamps.\n    \"\"\"\n    agreement = (\n        features.filter(pl.col(\"_ret\").is_not_null() &amp; pl.col(\"_ret\").is_finite())\n        .group_by(self.ts_col)\n        .agg(\n            pl.col(\"_ret\").count().alias(\"_n_pairs\"),\n            (pl.col(\"_ret\") &gt; 0).sum().alias(\"_n_positive\"),\n            (pl.col(\"_ret\") &lt; 0).sum().alias(\"_n_negative\"),\n        )\n        .filter(pl.col(\"_n_pairs\") &gt;= self.min_pairs)\n        .with_columns(\n            (\n                pl.max_horizontal(\"_n_positive\", \"_n_negative\").cast(pl.Float64)\n                / pl.col(\"_n_pairs\").cast(pl.Float64)\n            ).alias(\"_agreement\")\n        )\n        .filter(pl.col(\"_agreement\") &gt;= self.agreement_threshold)\n    )\n\n    n_signals = agreement.height\n    logger.info(f\"AgreementDetector: detected {n_signals} timestamps\")\n\n    if n_signals == 0:\n        return Signals(\n            pl.DataFrame(\n                schema={\n                    self.pair_col: pl.Utf8,\n                    self.ts_col: pl.Datetime,\n                    \"signal_type\": pl.Utf8,\n                    \"signal\": pl.Int64,\n                    \"probability\": pl.Float64,\n                }\n            )\n        )\n\n    # Create signals with synthetic \"ALL\" pair for market-wide signals\n    signals_df = agreement.select(\n        [\n            pl.lit(\"ALL\").alias(self.pair_col),\n            self.ts_col,\n            pl.lit(self.signal_type_name).alias(\"signal_type\"),\n            pl.lit(1).alias(\"signal\"),\n            pl.col(\"_agreement\").alias(\"probability\"),\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.agreement_detector.AgreementDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Preprocess raw data: compute log returns.</p> <p>Returns raw OHLCV with _ret column added.</p> Source code in <code>src/signalflow/detector/market/agreement_detector.py</code> <pre><code>def preprocess(\n    self,\n    raw_data_view: RawDataView,\n    context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Preprocess raw data: compute log returns.\n\n    Returns raw OHLCV with _ret column added.\n    \"\"\"\n    df = super().preprocess(raw_data_view, context)\n\n    df = df.sort([self.pair_col, self.ts_col]).with_columns(\n        (pl.col(self.price_col) / pl.col(self.price_col).shift(self.return_window))\n        .log()\n        .over(self.pair_col)\n        .alias(\"_ret\")\n    )\n\n    return df\n</code></pre>"},{"location":"api/detector/#z-score-detector","title":"Z-Score Detector","text":""},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector","title":"signalflow.detector.market.zscore_detector.MarketZScoreDetector  <code>dataclass</code>","text":"<pre><code>MarketZScoreDetector(signal_category: SignalCategory = SignalCategory.MARKET_WIDE, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'aggregate_outlier'})(), z_threshold: float = 3.0, rolling_window: int = 100, min_pairs: int = 5, return_window: int = 1, price_col: str = 'close', signal_type_name: str = 'aggregate_outlier')\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Detects market-wide signals via z-score of aggregate cross-pair return.</p> <p>More robust than agreement-based detection on correlated markets because it adapts to the current volatility regime.</p> Algorithm <ol> <li>Compute log-return per pair per timestamp.</li> <li>Compute cross-pair mean return at each timestamp.</li> <li>Compute rolling mean and std of the aggregate return over    <code>rolling_window</code> bars.</li> <li>z_score = (agg_return - rolling_mean) / rolling_std</li> <li>Signal if |z_score| &gt; <code>z_threshold</code>.</li> </ol> <p>Attributes:</p> Name Type Description <code>z_threshold</code> <code>float</code> <p>Absolute z-score threshold for detection.</p> <code>rolling_window</code> <code>int</code> <p>Window size for rolling statistics.</p> <code>min_pairs</code> <code>int</code> <p>Minimum number of active pairs at a timestamp.</p> <code>return_window</code> <code>int</code> <p>Bars for return computation.</p> <code>signal_type_name</code> <code>str</code> <p>Signal type name for detected signals.</p> Example <pre><code>from signalflow.detector import MarketZScoreDetector\nfrom signalflow.target.utils import mask_targets_by_signals\n\n# Detect market-wide z-score outliers\ndetector = MarketZScoreDetector(z_threshold=3.0)\nsignals = detector.run(raw_data_view)\n\n# Mask labels overlapping with detected signals\nlabeled_df = mask_targets_by_signals(\n    df=labeled_df,\n    signals=signals,\n    mask_signal_types={\"aggregate_outlier\"},\n    horizon_bars=60,\n)\n</code></pre> Note <p>Returns one signal per timestamp (not per pair) since market-wide signals affect all pairs simultaneously. The signal has a synthetic \"ALL\" pair.</p>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'aggregate_outlier'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = DETECTOR\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.min_pairs","title":"min_pairs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_pairs: int = 5\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.return_window","title":"return_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>return_window: int = 1\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.rolling_window","title":"rolling_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rolling_window: int = 100\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = MARKET_WIDE\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.signal_type_name","title":"signal_type_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_type_name: str = 'aggregate_outlier'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.z_threshold","title":"z_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>z_threshold: float = 3.0\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/market/zscore_detector.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.allowed_signal_types = {self.signal_type_name}\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect market-wide signals via z-score.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Multi-pair OHLCV DataFrame with _ret column.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with aggregate_outlier signal type for detected timestamps.</p> Source code in <code>src/signalflow/detector/market/zscore_detector.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect market-wide signals via z-score.\n\n    Args:\n        features: Multi-pair OHLCV DataFrame with _ret column.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with aggregate_outlier signal type for detected timestamps.\n    \"\"\"\n    min_samples = self.rolling_window\n\n    # Cross-pair mean return per timestamp\n    agg_return = (\n        features.filter(pl.col(\"_ret\").is_not_null() &amp; pl.col(\"_ret\").is_finite())\n        .group_by(self.ts_col)\n        .agg(\n            pl.col(\"_ret\").mean().alias(\"_agg_return\"),\n            pl.col(\"_ret\").count().alias(\"_n_pairs\"),\n        )\n        .filter(pl.col(\"_n_pairs\") &gt;= self.min_pairs)\n        .sort(self.ts_col)\n    )\n\n    # Rolling mean/std and z-score\n    result = (\n        agg_return.with_columns(\n            [\n                pl.col(\"_agg_return\")\n                .rolling_mean(window_size=self.rolling_window, min_samples=min_samples)\n                .alias(\"_rolling_mean\"),\n                pl.col(\"_agg_return\")\n                .rolling_std(window_size=self.rolling_window, min_samples=min_samples)\n                .alias(\"_rolling_std\"),\n            ]\n        )\n        .with_columns(\n            pl.when(pl.col(\"_rolling_std\") &gt; 1e-12)\n            .then((pl.col(\"_agg_return\") - pl.col(\"_rolling_mean\")) / pl.col(\"_rolling_std\"))\n            .otherwise(pl.lit(0.0))\n            .alias(\"_z_score\")\n        )\n        .filter(pl.col(\"_z_score\").abs() &gt; self.z_threshold)\n    )\n\n    n_signals = result.height\n    logger.info(f\"MarketZScoreDetector: detected {n_signals} timestamps\")\n\n    if n_signals == 0:\n        return Signals(\n            pl.DataFrame(\n                schema={\n                    self.pair_col: pl.Utf8,\n                    self.ts_col: pl.Datetime,\n                    \"signal_type\": pl.Utf8,\n                    \"signal\": pl.Int64,\n                    \"probability\": pl.Float64,\n                }\n            )\n        )\n\n    # Create signals with synthetic \"ALL\" pair for market-wide signals\n    # Probability based on normalized z-score\n    signals_df = result.select(\n        [\n            pl.lit(\"ALL\").alias(self.pair_col),\n            self.ts_col,\n            pl.lit(self.signal_type_name).alias(\"signal_type\"),\n            pl.lit(1).alias(\"signal\"),\n            (pl.col(\"_z_score\").abs() / self.z_threshold).clip(0.0, 1.0).alias(\"probability\"),\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.zscore_detector.MarketZScoreDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Preprocess raw data: compute log returns.</p> <p>Returns raw OHLCV with _ret column added.</p> Source code in <code>src/signalflow/detector/market/zscore_detector.py</code> <pre><code>def preprocess(\n    self,\n    raw_data_view: RawDataView,\n    context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Preprocess raw data: compute log returns.\n\n    Returns raw OHLCV with _ret column added.\n    \"\"\"\n    df = super().preprocess(raw_data_view, context)\n\n    df = df.sort([self.pair_col, self.ts_col]).with_columns(\n        (pl.col(self.price_col) / pl.col(self.price_col).shift(self.return_window))\n        .log()\n        .over(self.pair_col)\n        .alias(\"_ret\")\n    )\n\n    return df\n</code></pre>"},{"location":"api/detector/#cusum-detector","title":"CUSUM Detector","text":""},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector","title":"signalflow.detector.market.cusum_detector.MarketCusumDetector  <code>dataclass</code>","text":"<pre><code>MarketCusumDetector(signal_category: SignalCategory = SignalCategory.MARKET_WIDE, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'structural_break'})(), drift: float = 0.005, cusum_threshold: float = 0.05, rolling_window: int = 100, min_pairs: int = 5, return_window: int = 1, price_col: str = 'close', signal_type_name: str = 'structural_break')\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Detects market-wide signals via CUSUM of cross-pair aggregate return.</p> <p>Unlike point-in-time z-score detection, CUSUM accumulates evidence over multiple bars, making it better at detecting gradual structural changes.</p> Algorithm <ol> <li>Compute cross-pair mean return at each timestamp.</li> <li>Compute rolling mean <code>mu</code> (expected return) over <code>rolling_window</code>.</li> <li>S_pos = max(0, S_pos + (x - mu - drift))</li> <li>S_neg = max(0, S_neg + (-x + mu - drift))</li> <li>Signal if S_pos &gt; cusum_threshold or S_neg &gt; cusum_threshold.</li> <li>Reset S_pos, S_neg to 0 after signal detection.</li> </ol> <p>Attributes:</p> Name Type Description <code>drift</code> <code>float</code> <p>Slack parameter (allowance for normal variation).</p> <code>cusum_threshold</code> <code>float</code> <p>Decision interval for CUSUM alarm.</p> <code>rolling_window</code> <code>int</code> <p>Window for estimating expected return (mu).</p> <code>min_pairs</code> <code>int</code> <p>Minimum number of active pairs at a timestamp.</p> <code>return_window</code> <code>int</code> <p>Bars for return computation.</p> <code>signal_type_name</code> <code>str</code> <p>Signal type name for detected signals.</p> Example <pre><code>from signalflow.detector import MarketCusumDetector\nfrom signalflow.target.utils import mask_targets_by_signals\n\n# Detect market-wide regime shifts\ndetector = MarketCusumDetector(cusum_threshold=0.05)\nsignals = detector.run(raw_data_view)\n\n# Mask labels overlapping with detected signals\nlabeled_df = mask_targets_by_signals(\n    df=labeled_df,\n    signals=signals,\n    mask_signal_types={\"structural_break\"},\n    horizon_bars=60,\n)\n</code></pre> Note <p>Returns one signal per timestamp (not per pair) since market-wide signals affect all pairs simultaneously. The signal has a synthetic \"ALL\" pair.</p> Reference <p>Page, E. S. (1954) - \"Continuous Inspection Schemes\"</p>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'structural_break'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = DETECTOR\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.cusum_threshold","title":"cusum_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>cusum_threshold: float = 0.05\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.drift","title":"drift  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>drift: float = 0.005\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.min_pairs","title":"min_pairs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_pairs: int = 5\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.return_window","title":"return_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>return_window: int = 1\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.rolling_window","title":"rolling_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rolling_window: int = 100\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = MARKET_WIDE\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.signal_type_name","title":"signal_type_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_type_name: str = 'structural_break'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/market/cusum_detector.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.allowed_signal_types = {self.signal_type_name}\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect market-wide signals via CUSUM.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Multi-pair OHLCV DataFrame with _ret column.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with structural_break signal type for detected timestamps.</p> Source code in <code>src/signalflow/detector/market/cusum_detector.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect market-wide signals via CUSUM.\n\n    Args:\n        features: Multi-pair OHLCV DataFrame with _ret column.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with structural_break signal type for detected timestamps.\n    \"\"\"\n    min_samples = self.rolling_window\n\n    # Cross-pair mean return per timestamp\n    agg_return = (\n        features.filter(pl.col(\"_ret\").is_not_null() &amp; pl.col(\"_ret\").is_finite())\n        .group_by(self.ts_col)\n        .agg(\n            pl.col(\"_ret\").mean().alias(\"_agg_return\"),\n            pl.col(\"_ret\").count().alias(\"_n_pairs\"),\n        )\n        .filter(pl.col(\"_n_pairs\") &gt;= self.min_pairs)\n        .sort(self.ts_col)\n    )\n\n    # Compute rolling mean (expected return mu)\n    agg_return = agg_return.with_columns(\n        pl.col(\"_agg_return\").rolling_mean(window_size=self.rolling_window, min_samples=min_samples).alias(\"_mu\")\n    )\n\n    # CUSUM with reset (sequential \u2014 inherently stateful)\n    x_arr = agg_return.get_column(\"_agg_return\").to_numpy()\n    mu_arr = agg_return.get_column(\"_mu\").to_numpy()\n\n    n = len(x_arr)\n    is_signal = np.zeros(n, dtype=bool)\n    cusum_values = np.zeros(n, dtype=np.float64)\n    s_pos = 0.0\n    s_neg = 0.0\n\n    for i in range(n):\n        if np.isnan(mu_arr[i]) or np.isnan(x_arr[i]):\n            continue\n\n        deviation = x_arr[i] - mu_arr[i]\n        s_pos = max(0.0, s_pos + deviation - self.drift)\n        s_neg = max(0.0, s_neg - deviation - self.drift)\n\n        cusum_values[i] = max(s_pos, s_neg)\n\n        if s_pos &gt; self.cusum_threshold or s_neg &gt; self.cusum_threshold:\n            is_signal[i] = True\n            s_pos = 0.0\n            s_neg = 0.0\n\n    n_signals = int(is_signal.sum())\n    logger.info(f\"MarketCusumDetector: detected {n_signals} timestamps\")\n\n    if n_signals == 0:\n        return Signals(\n            pl.DataFrame(\n                schema={\n                    self.pair_col: pl.Utf8,\n                    self.ts_col: pl.Datetime,\n                    \"signal_type\": pl.Utf8,\n                    \"signal\": pl.Int64,\n                    \"probability\": pl.Float64,\n                }\n            )\n        )\n\n    # Filter to signal timestamps only\n    signal_df = agg_return.with_columns(\n        [\n            pl.Series(\"_is_signal\", is_signal),\n            pl.Series(\"_cusum\", cusum_values),\n        ]\n    ).filter(pl.col(\"_is_signal\"))\n\n    # Create signals with synthetic \"ALL\" pair for market-wide signals\n    # Probability based on normalized cusum value\n    signals_df = signal_df.select(\n        [\n            pl.lit(\"ALL\").alias(self.pair_col),\n            self.ts_col,\n            pl.lit(self.signal_type_name).alias(\"signal_type\"),\n            pl.lit(1).alias(\"signal\"),\n            (pl.col(\"_cusum\") / self.cusum_threshold).clip(0.0, 1.0).alias(\"probability\"),\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.market.cusum_detector.MarketCusumDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Preprocess raw data: compute log returns.</p> <p>Returns raw OHLCV with _ret column added.</p> Source code in <code>src/signalflow/detector/market/cusum_detector.py</code> <pre><code>def preprocess(\n    self,\n    raw_data_view: RawDataView,\n    context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Preprocess raw data: compute log returns.\n\n    Returns raw OHLCV with _ret column added.\n    \"\"\"\n    df = super().preprocess(raw_data_view, context)\n\n    df = df.sort([self.pair_col, self.ts_col]).with_columns(\n        (pl.col(self.price_col) / pl.col(self.price_col).shift(self.return_window))\n        .log()\n        .over(self.pair_col)\n        .alias(\"_ret\")\n    )\n\n    return df\n</code></pre>"},{"location":"api/detector/#generic-detectors","title":"Generic Detectors","text":""},{"location":"api/detector/#z-score-anomaly-detector","title":"Z-Score Anomaly Detector","text":""},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector","title":"signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector  <code>dataclass</code>","text":"<pre><code>ZScoreAnomalyDetector(signal_category: SignalCategory = SignalCategory.ANOMALY, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'positive_anomaly', 'negative_anomaly'})(), target_feature: str = 'close', rolling_window: int = 1440, threshold: float = 4.0, signal_high: str = 'positive_anomaly', signal_low: str = 'negative_anomaly')\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Z-score based anomaly detector on any feature.</p> <p>Detects anomalies when a feature value deviates significantly from its rolling mean, measured in standard deviations (z-score).</p> Algorithm <ol> <li>Compute rolling mean of target_feature over rolling_window</li> <li>Compute rolling std of target_feature over rolling_window</li> <li>Calculate z-score: (value - rolling_mean) / rolling_std</li> <li>Signal if |z-score| &gt; threshold</li> </ol> <p>Attributes:</p> Name Type Description <code>target_feature</code> <code>str</code> <p>Column name to analyze for anomalies.</p> <code>rolling_window</code> <code>int</code> <p>Window size for rolling mean/std calculation.</p> <code>threshold</code> <code>float</code> <p>Z-score threshold for anomaly detection.</p> <code>signal_high</code> <code>str</code> <p>Signal type when z-score &gt; threshold.</p> <code>signal_low</code> <code>str</code> <p>Signal type when z-score &lt; -threshold.</p> Example <pre><code>from signalflow.detector import ZScoreAnomalyDetector\nfrom signalflow.feature import RsiExtractor\n\n# Detect anomalies on RSI\ndetector = ZScoreAnomalyDetector(\n    features=[RsiExtractor(period=14)],\n    target_feature=\"RSI_14\",\n    threshold=3.0,\n)\nsignals = detector.run(raw_data_view)\n\n# Detect anomalies on log returns\ndetector = ZScoreAnomalyDetector(\n    target_feature=\"close\",  # Will compute on raw close prices\n    threshold=4.0,\n    signal_high=\"extreme_positive_anomaly\",\n    signal_low=\"extreme_negative_anomaly\",\n)\n</code></pre> Note <p>This detector overrides preprocess() to add helper columns (_target_rol_mean, _target_rol_std) for z-score calculation.</p>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'positive_anomaly', 'negative_anomaly'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.rolling_window","title":"rolling_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rolling_window: int = 1440\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = ANOMALY\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.signal_high","title":"signal_high  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_high: str = 'positive_anomaly'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.signal_low","title":"signal_low  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_low: str = 'negative_anomaly'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.target_feature","title":"target_feature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_feature: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.threshold","title":"threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>threshold: float = 4.0\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/zscore_anomaly.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    # Update allowed_signal_types based on configured signal names\n    self.allowed_signal_types = {self.signal_high, self.signal_low}\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect anomalies using z-score method.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Preprocessed DataFrame with _target_rol_mean, _target_rol_std.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with positive_anomaly/negative_anomaly signal types.</p> Source code in <code>src/signalflow/detector/zscore_anomaly.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect anomalies using z-score method.\n\n    Args:\n        features: Preprocessed DataFrame with _target_rol_mean, _target_rol_std.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with positive_anomaly/negative_anomaly signal types.\n    \"\"\"\n    target = pl.col(self.target_feature)\n    mean = pl.col(\"_target_rol_mean\")\n    std = pl.col(\"_target_rol_std\")\n\n    # Z-score calculation\n    z_score = (target - mean) / std\n\n    # Classify anomalies\n    is_high = (std &gt; 0) &amp; (z_score &gt; self.threshold)\n    is_low = (std &gt; 0) &amp; (z_score &lt; -self.threshold)\n\n    signal_type_expr = (\n        pl.when(is_high)\n        .then(pl.lit(self.signal_high))\n        .when(is_low)\n        .then(pl.lit(self.signal_low))\n        .otherwise(pl.lit(None, dtype=pl.Utf8))\n        .alias(\"signal_type\")\n    )\n\n    # Probability: how far beyond threshold (clipped to [0, 1])\n    probability_expr = (\n        pl.when(is_high | is_low)\n        .then((z_score.abs() / self.threshold).clip(0.0, 1.0))\n        .otherwise(pl.lit(None, dtype=pl.Float64))\n        .alias(\"probability\")\n    )\n\n    signals_df = (\n        features.with_columns([signal_type_expr, probability_expr])\n        .filter(pl.col(\"signal_type\").is_not_null())\n        .select(\n            [\n                self.pair_col,\n                self.ts_col,\n                \"signal_type\",\n                pl.lit(1).alias(\"signal\"),\n                \"probability\",\n            ]\n        )\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.zscore_anomaly.ZScoreAnomalyDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Run features + compute rolling mean/std for target_feature.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with original columns plus _target_rol_mean, _target_rol_std.</p> Source code in <code>src/signalflow/detector/zscore_anomaly.py</code> <pre><code>def preprocess(\n    self,\n    raw_data_view: RawDataView,\n    context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Run features + compute rolling mean/std for target_feature.\n\n    Args:\n        raw_data_view: View to raw market data.\n        context: Additional context (unused).\n\n    Returns:\n        DataFrame with original columns plus _target_rol_mean, _target_rol_std.\n    \"\"\"\n    # 1. Base preprocessing (OHLCV + features)\n    df = super().preprocess(raw_data_view, context)\n\n    # 2. Compute helper columns for z-score\n    min_samples = max(2, self.rolling_window // 4)\n    df = df.with_columns(\n        [\n            pl.col(self.target_feature)\n            .rolling_mean(window_size=self.rolling_window, min_samples=min_samples)\n            .over(self.pair_col)\n            .alias(\"_target_rol_mean\"),\n            pl.col(self.target_feature)\n            .rolling_std(window_size=self.rolling_window, min_samples=min_samples)\n            .over(self.pair_col)\n            .alias(\"_target_rol_std\"),\n        ]\n    )\n\n    return df\n</code></pre>"},{"location":"api/detector/#percentile-regime-detector","title":"Percentile Regime Detector","text":""},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector","title":"signalflow.detector.percentile_regime.PercentileRegimeDetector  <code>dataclass</code>","text":"<pre><code>PercentileRegimeDetector(signal_category: SignalCategory = SignalCategory.VOLATILITY, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'high_volatility', 'low_volatility'})(), target_feature: str = '_realized_vol', lookback_window: int = 1440, upper_quantile: float = 0.67, lower_quantile: float = 0.33, signal_high: str = 'high_volatility', signal_low: str = 'low_volatility')\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Percentile-based regime detector on any feature.</p> <p>Classifies the current regime by computing the rolling percentile of a target feature within a lookback window.</p> Algorithm <ol> <li>For each bar, compute percentile of target_feature within lookback_window</li> <li>If percentile &gt; upper_quantile -&gt; signal_high</li> <li>If percentile &lt; lower_quantile -&gt; signal_low</li> <li>Otherwise -&gt; no signal</li> </ol> <p>Attributes:</p> Name Type Description <code>target_feature</code> <code>str</code> <p>Column name to analyze for regime.</p> <code>lookback_window</code> <code>int</code> <p>Window size for percentile calculation.</p> <code>upper_quantile</code> <code>float</code> <p>Upper threshold (signal if percentile &gt; this).</p> <code>lower_quantile</code> <code>float</code> <p>Lower threshold (signal if percentile &lt; this).</p> <code>signal_high</code> <code>str</code> <p>Signal type when percentile &gt; upper_quantile.</p> <code>signal_low</code> <code>str</code> <p>Signal type when percentile &lt; lower_quantile.</p> Example <pre><code>from signalflow.detector import PercentileRegimeDetector\nfrom signalflow.feature import FeaturePipeline, RealizedVolExtractor\n\n# Volatility regime detection\ndetector = PercentileRegimeDetector(\n    features_pipe=FeaturePipeline([RealizedVolExtractor()]),\n    target_feature=\"_realized_vol\",\n    upper_quantile=0.67,\n    lower_quantile=0.33,\n    signal_high=\"high_volatility\",\n    signal_low=\"low_volatility\",\n)\nsignals = detector.run(raw_data_view)\n</code></pre> Note <p>Uses numpy for percentile calculation per group since Polars doesn't have native rolling percentile.</p>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'high_volatility', 'low_volatility'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.lookback_window","title":"lookback_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookback_window: int = 1440\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.lower_quantile","title":"lower_quantile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lower_quantile: float = 0.33\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = VOLATILITY\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.signal_high","title":"signal_high  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_high: str = 'high_volatility'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.signal_low","title":"signal_low  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_low: str = 'low_volatility'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.target_feature","title":"target_feature  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_feature: str = '_realized_vol'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.upper_quantile","title":"upper_quantile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>upper_quantile: float = 0.67\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/percentile_regime.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if not 0 &lt; self.lower_quantile &lt; self.upper_quantile &lt; 1:\n        raise ValueError(\n            f\"Quantiles must satisfy 0 &lt; lower &lt; upper &lt; 1, \"\n            f\"got lower={self.lower_quantile}, upper={self.upper_quantile}\"\n        )\n    # Update allowed_signal_types based on configured signal names\n    self.allowed_signal_types = {self.signal_high, self.signal_low}\n</code></pre>"},{"location":"api/detector/#signalflow.detector.percentile_regime.PercentileRegimeDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect regime using rolling percentile method.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Preprocessed DataFrame with target_feature column.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with high_volatility/low_volatility signal types.</p> Source code in <code>src/signalflow/detector/percentile_regime.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect regime using rolling percentile method.\n\n    Args:\n        features: Preprocessed DataFrame with target_feature column.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with high_volatility/low_volatility signal types.\n    \"\"\"\n    results = []\n\n    for pair_name, group in features.group_by(self.pair_col, maintain_order=True):\n        arr = group[self.target_feature].to_numpy().astype(np.float64)\n        n = len(arr)\n\n        signal_types: list[str | None] = [None] * n\n        probabilities: list[float | None] = [None] * n\n\n        for t in range(n):\n            if np.isnan(arr[t]):\n                continue\n\n            # Get lookback window\n            lb_start = max(0, t - self.lookback_window + 1)\n            window = arr[lb_start : t + 1]\n            valid = window[~np.isnan(window)]\n\n            if len(valid) &lt; 2:\n                continue\n\n            # Compute percentile (fraction of values &lt;= current)\n            percentile = float(np.mean(valid &lt;= arr[t]))\n\n            if percentile &gt; self.upper_quantile:\n                signal_types[t] = self.signal_high\n                probabilities[t] = percentile\n            elif percentile &lt; self.lower_quantile:\n                signal_types[t] = self.signal_low\n                probabilities[t] = 1.0 - percentile\n\n        group = group.with_columns(\n            [\n                pl.Series(name=\"signal_type\", values=signal_types, dtype=pl.Utf8),\n                pl.Series(name=\"probability\", values=probabilities, dtype=pl.Float64),\n            ]\n        )\n        results.append(group)\n\n    if not results:\n        return Signals(pl.DataFrame())\n\n    combined = pl.concat(results, how=\"vertical_relaxed\")\n\n    signals_df = combined.filter(pl.col(\"signal_type\").is_not_null()).select(\n        [\n            self.pair_col,\n            self.ts_col,\n            \"signal_type\",\n            pl.lit(1).alias(\"signal\"),\n            \"probability\",\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/detector/#local-extrema-detector","title":"Local Extrema Detector","text":""},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector","title":"signalflow.detector.local_extrema.LocalExtremaDetector  <code>dataclass</code>","text":"<pre><code>LocalExtremaDetector(signal_category: SignalCategory = SignalCategory.PRICE_STRUCTURE, pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType | str = RawDataType.SPOT, features: Feature | list[Feature] | FeaturePipeline | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, allowed_signal_types: set[str] | None = (lambda: {'local_max', 'local_min'})(), price_col: str = 'close', lookback: int = 60, confirmation_bars: int = 10, min_swing_pct: float = 0.02, signal_top: str = 'local_max', signal_bottom: str = 'local_min')\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>Local price extrema detector (tops/bottoms) with confirmation.</p> <p>Detects local price structure using backward-looking zigzag with confirmation delay - a local extremum is only confirmed after confirmation_bars bars have passed showing the reversal.</p> Algorithm <ol> <li>For each bar t, look back lookback bars</li> <li>Find the max and min in the lookback window</li> <li>A local_max is confirmed when:</li> <li>The max occurred at bar (t - confirmation_bars) or earlier</li> <li>Price has dropped &gt;= min_swing_pct from the max</li> <li>Current price &lt; max price</li> <li>A local_min is confirmed when:</li> <li>The min occurred at bar (t - confirmation_bars) or earlier</li> <li>Price has risen &gt;= min_swing_pct from the min</li> <li>Current price &gt; min price</li> <li>Only emit the signal once at the confirmation bar</li> </ol> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column to analyze. Default: \"close\".</p> <code>lookback</code> <code>int</code> <p>Backward window for extrema search. Default: 60.</p> <code>confirmation_bars</code> <code>int</code> <p>Bars of reversal needed for confirmation. Default: 10.</p> <code>min_swing_pct</code> <code>float</code> <p>Minimum swing percentage. Default: 0.02.</p> <code>signal_top</code> <code>str</code> <p>Signal type for local max. Default: \"local_max\".</p> <code>signal_bottom</code> <code>str</code> <p>Signal type for local min. Default: \"local_min\".</p> Example <pre><code>from signalflow.detector import LocalExtremaDetector\n\ndetector = LocalExtremaDetector(\n    lookback=60,\n    confirmation_bars=10,\n    min_swing_pct=0.02,\n)\nsignals = detector.run(raw_data_view)\n</code></pre> Note <p>This detector is backward-looking and safe for live trading.</p>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.allowed_signal_types","title":"allowed_signal_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>allowed_signal_types: set[str] | None = field(default_factory=lambda: {'local_max', 'local_min'})\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.confirmation_bars","title":"confirmation_bars  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>confirmation_bars: int = 10\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.lookback","title":"lookback  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookback: int = 60\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.min_swing_pct","title":"min_swing_pct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_swing_pct: float = 0.02\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.signal_bottom","title":"signal_bottom  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_bottom: str = 'local_min'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = PRICE_STRUCTURE\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.signal_top","title":"signal_top  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_top: str = 'local_max'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/local_extrema.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.confirmation_bars &gt;= self.lookback:\n        raise ValueError(f\"confirmation_bars ({self.confirmation_bars}) must be &lt; lookback ({self.lookback})\")\n    # Update allowed_signal_types based on configured signal names\n    self.allowed_signal_types = {self.signal_top, self.signal_bottom}\n</code></pre>"},{"location":"api/detector/#signalflow.detector.local_extrema.LocalExtremaDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Detect local tops/bottoms with confirmation delay.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>OHLCV data with pair and timestamp columns.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context (unused).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with local_max/local_min signal types.</p> Source code in <code>src/signalflow/detector/local_extrema.py</code> <pre><code>def detect(\n    self,\n    features: pl.DataFrame,\n    context: dict[str, Any] | None = None,\n) -&gt; Signals:\n    \"\"\"Detect local tops/bottoms with confirmation delay.\n\n    Args:\n        features: OHLCV data with pair and timestamp columns.\n        context: Additional context (unused).\n\n    Returns:\n        Signals with local_max/local_min signal types.\n    \"\"\"\n    results = []\n\n    for pair_name, group in features.group_by(self.pair_col, maintain_order=True):\n        prices = group[self.price_col].to_numpy().astype(np.float64)\n        n = len(prices)\n\n        signal_types: list[str | None] = [None] * n\n        probabilities: list[float | None] = [None] * n\n\n        # Track last emitted extremum to avoid duplicates\n        last_emitted_type: str | None = None\n        last_emitted_idx = -self.lookback\n\n        for t in range(self.lookback + self.confirmation_bars, n):\n            p_current = prices[t]\n            if np.isnan(p_current):\n                continue\n\n            # Search window: [t - lookback, t - confirmation_bars]\n            search_start = t - self.lookback\n            search_end = t - self.confirmation_bars + 1\n\n            if search_end &lt;= search_start:\n                continue\n\n            search_window = prices[search_start:search_end]\n            valid_mask = ~np.isnan(search_window)\n            if not np.any(valid_mask):\n                continue\n\n            valid_prices = search_window[valid_mask]\n\n            max_val = np.max(valid_prices)\n            min_val = np.min(valid_prices)\n\n            # Check LOCAL_TOP: max in search window, price dropped since\n            if max_val &gt; 0 and p_current &lt; max_val:\n                swing = (max_val - p_current) / max_val\n                if swing &gt;= self.min_swing_pct:\n                    if last_emitted_type != self.signal_top or (t - last_emitted_idx) &gt; self.lookback:\n                        signal_types[t] = self.signal_top\n                        probabilities[t] = min(1.0, swing / (self.min_swing_pct * 3))\n                        last_emitted_type = self.signal_top\n                        last_emitted_idx = t\n                        continue\n\n            # Check LOCAL_BOTTOM: min in search window, price risen since\n            if min_val &gt; 0 and p_current &gt; min_val:\n                swing = (p_current - min_val) / min_val\n                if swing &gt;= self.min_swing_pct:\n                    if last_emitted_type != self.signal_bottom or (t - last_emitted_idx) &gt; self.lookback:\n                        signal_types[t] = self.signal_bottom\n                        probabilities[t] = min(1.0, swing / (self.min_swing_pct * 3))\n                        last_emitted_type = self.signal_bottom\n                        last_emitted_idx = t\n\n        group = group.with_columns(\n            [\n                pl.Series(name=\"signal_type\", values=signal_types, dtype=pl.Utf8),\n                pl.Series(name=\"probability\", values=probabilities, dtype=pl.Float64),\n            ]\n        )\n        results.append(group)\n\n    if not results:\n        return Signals(pl.DataFrame())\n\n    combined = pl.concat(results, how=\"vertical_relaxed\")\n\n    signals_df = combined.filter(pl.col(\"signal_type\").is_not_null()).select(\n        [\n            self.pair_col,\n            self.ts_col,\n            \"signal_type\",\n            pl.lit(1).alias(\"signal\"),\n            \"probability\",\n        ]\n    )\n\n    return Signals(signals_df)\n</code></pre>"},{"location":"api/feature/","title":"Feature Module","text":"<p>Feature extraction for technical indicators and derived metrics.</p>"},{"location":"api/feature/#base-classes","title":"Base Classes","text":""},{"location":"api/feature/#signalflow.feature.base.Feature","title":"signalflow.feature.base.Feature  <code>dataclass</code>","text":"<pre><code>Feature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None)\n</code></pre> <p>               Bases: <code>KwargsTolerantMixin</code></p> <p>Base class for all features.</p> Two methods to implement <ul> <li>compute(df): all pairs, abstract for GlobalFeature/Pipeline</li> <li>compute_pair(df): one pair, for regular features</li> </ul> <p>Attributes:</p> Name Type Description <code>requires</code> <code>list[str]</code> <p>Input column templates, e.g. [\"{price_col}\"]</p> <code>outputs</code> <code>list[str]</code> <p>Output column templates, e.g. [\"rsi_{period}\"]</p> <code>normalized</code> <code>bool</code> <p>If True, apply rolling z-score normalization to output.</p> <code>norm_period</code> <code>int | None</code> <p>Window for normalization. Defaults to 3x feature period.</p>"},{"location":"api/feature/#signalflow.feature.base.Feature.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = FEATURE\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.group_col","title":"group_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>group_col: str = 'pair'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.norm_period","title":"norm_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>norm_period: int | None = None\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.normalized","title":"normalized  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>normalized: bool = False\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.outputs","title":"outputs  <code>class-attribute</code>","text":"<pre><code>outputs: list[str] = []\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.requires","title":"requires  <code>class-attribute</code>","text":"<pre><code>requires: list[str] = []\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.test_params","title":"test_params  <code>class-attribute</code>","text":"<pre><code>test_params: list[dict] = []\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.warmup","title":"warmup  <code>property</code>","text":"<pre><code>warmup: int\n</code></pre> <p>Minimum bars needed before output is stable.</p> <p>Override in subclasses with feature-specific logic. Default: 0 (no warmup required).</p>"},{"location":"api/feature/#signalflow.feature.base.Feature.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute feature for all pairs</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def compute(self, df: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute feature for all pairs\"\"\"\n    return df.group_by(self.group_col, maintain_order=True).map_groups(self.compute_pair)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.compute_pair","title":"compute_pair","text":"<pre><code>compute_pair(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute feature for single pair. Override for per-pair features.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute feature for single pair. Override for per-pair features.\"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement compute_pair()\")\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.output_cols","title":"output_cols","text":"<pre><code>output_cols(prefix: str = '') -&gt; list[str]\n</code></pre> <p>Actual output column names with parameter substitution.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def output_cols(self, prefix: str = \"\") -&gt; list[str]:\n    \"\"\"Actual output column names with parameter substitution.\"\"\"\n    return [f\"{prefix}{tpl.format(**self.__dict__)}\" for tpl in self.outputs]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.Feature.required_cols","title":"required_cols","text":"<pre><code>required_cols() -&gt; list[str]\n</code></pre> <p>Actual required column names with parameter substitution.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def required_cols(self) -&gt; list[str]:\n    \"\"\"Actual required column names with parameter substitution.\"\"\"\n    return [tpl.format(**self.__dict__) if \"{\" in tpl else tpl for tpl in self.requires]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline","title":"signalflow.feature.feature_pipeline.FeaturePipeline  <code>dataclass</code>","text":"<pre><code>FeaturePipeline(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None, features: list[Feature] = list(), raw_data_type: RawDataType | str = RawDataType.SPOT)\n</code></pre> <p>               Bases: <code>Feature</code></p> <p>Orchestrates multiple features with optimized execution.</p> <p>Groups consecutive per-pair features into batches for single group_by.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Feature]</code> <p>List of features to compute.</p> <code>list()</code> <code>raw_data_type</code> <code>RawDataType | str</code> <p>Type of raw data (defines available columns).</p> <code>SPOT</code> Example <p>pipeline = FeaturePipeline( ...     features=[ ...         RsiFeature(period=14), ...         SmaFeature(period=20), ...         GlobalFeature(base=RsiFeature(period=14), reference_pair=\"BTCUSDT\"), ...     ], ...     raw_data_type=RawDataType.SPOT, ... ) df = pipeline.run(raw_data_view)</p>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.features","title":"features  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>features: list[Feature] = field(default_factory=list)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.outputs","title":"outputs  <code>property</code>","text":"<pre><code>outputs: list[str]\n</code></pre> <p>Aggregated outputs from all features.</p>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.raw_data_type","title":"raw_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_type: RawDataType | str = SPOT\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.requires","title":"requires  <code>class-attribute</code>","text":"<pre><code>requires: list[str] = []\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def __post_init__(self):\n    if not self.features:\n        raise ValueError(\"FeaturePipeline requires at least one feature\")\n    self._validate()\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline._group_into_batches","title":"_group_into_batches","text":"<pre><code>_group_into_batches() -&gt; list[list[Feature]]\n</code></pre> <p>Group features: consecutive per-pair \u2192 batch, global \u2192 separate.</p> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def _group_into_batches(self) -&gt; list[list[Feature]]:\n    \"\"\"Group features: consecutive per-pair \u2192 batch, global \u2192 separate.\"\"\"\n    batches = []\n    current_batch = []\n\n    for f in self.features:\n        is_global = isinstance(f, (GlobalFeature, FeaturePipeline)) or getattr(f, \"_is_global\", False)\n\n        if is_global:\n            if current_batch:\n                batches.append(current_batch)\n                current_batch = []\n            batches.append([f])\n        else:\n            current_batch.append(f)\n\n    if current_batch:\n        batches.append(current_batch)\n\n    return batches\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline._is_per_pair_batch","title":"_is_per_pair_batch","text":"<pre><code>_is_per_pair_batch(batch: list[Feature]) -&gt; bool\n</code></pre> <p>Check if batch contains only per-pair features.</p> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def _is_per_pair_batch(self, batch: list[Feature]) -&gt; bool:\n    \"\"\"Check if batch contains only per-pair features.\"\"\"\n    return not any(\n        isinstance(f, (GlobalFeature, FeaturePipeline)) or getattr(f, \"_is_global\", False) for f in batch\n    )\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline._validate","title":"_validate","text":"<pre><code>_validate()\n</code></pre> <p>Validate all dependencies are satisfied.</p> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def _validate(self):\n    \"\"\"Validate all dependencies are satisfied.\"\"\"\n    available = default_registry.get_raw_data_columns(self.raw_data_type)\n\n    for f in self.features:\n        required = set(f.required_cols())\n        missing = required - available\n\n        if missing:\n            raise ValueError(f\"{f.__class__.__name__} requires {missing}, available: {sorted(available)}\")\n\n        available.update(f.output_cols())\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute all features with optimized batching.</p> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def compute(self, df: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute all features with optimized batching.\"\"\"\n    df = df.sort([self.group_col, self.ts_col])\n\n    batches = self._group_into_batches()\n\n    for batch in batches:\n        if self._is_per_pair_batch(batch):\n\n            def apply_batch(pair_df: pl.DataFrame, features=batch) -&gt; pl.DataFrame:\n                for f in features:\n                    pair_df = f.compute_pair(pair_df)\n                return pair_df\n\n            df = df.group_by(self.group_col, maintain_order=True).map_groups(apply_batch)\n        else:\n            for f in batch:\n                df = f.compute(df, context=context)\n\n    return df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.output_cols","title":"output_cols","text":"<pre><code>output_cols(prefix: str = '') -&gt; list[str]\n</code></pre> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def output_cols(self, prefix: str = \"\") -&gt; list[str]:\n    return [f\"{prefix}{col}\" for col in self.outputs]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.run","title":"run","text":"<pre><code>run(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Entry point: load from RawDataView and compute.</p> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def run(self, raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Entry point: load from RawDataView and compute.\"\"\"\n    key = getattr(self.raw_data_type, \"value\", self.raw_data_type)\n    df = raw_data_view.to_polars(key)\n    return self.compute(df)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_pipeline.FeaturePipeline.to_mermaid","title":"to_mermaid","text":"<pre><code>to_mermaid() -&gt; str\n</code></pre> <p>Generate Mermaid diagram of feature dependencies.</p> Source code in <code>src/signalflow/feature/feature_pipeline.py</code> <pre><code>def to_mermaid(self) -&gt; str:\n    \"\"\"Generate Mermaid diagram of feature dependencies.\"\"\"\n    lines = [\"graph LR\"]\n    lines.append(\"    subgraph Input\")\n    for col in sorted(default_registry.get_raw_data_columns(self.raw_data_type)):\n        lines.append(f\"        {col}[{col}]\")\n    lines.append(\"    end\")\n\n    for f in self.features:\n        name = f.__class__.__name__\n        if hasattr(f, \"period\"):\n            name = f\"{name}_{f.period}\"\n\n        for req in f.required_cols():\n            lines.append(f\"    {req} --&gt; {name}\")\n        for out in f.output_cols():\n            lines.append(f\"    {name} --&gt; {out}[{out}]\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.GlobalFeature","title":"signalflow.feature.base.GlobalFeature  <code>dataclass</code>","text":"<pre><code>GlobalFeature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None)\n</code></pre> <p>               Bases: <code>Feature</code></p> <p>Base class for features computed across all pairs.</p> <p>Override compute() with custom aggregation logic.</p>"},{"location":"api/feature/#signalflow.feature.base.GlobalFeature.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Must override - compute global feature across all pairs.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def compute(self, df: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Must override - compute global feature across all pairs.\"\"\"\n    raise NotImplementedError(f\"{self.__class__.__name__} must implement compute()\")\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature","title":"signalflow.feature.offset_feature.OffsetFeature  <code>dataclass</code>","text":"<pre><code>OffsetFeature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None, feature_name: str = None, feature_params: dict = dict(), window: int = 15, prefix: str | None = None)\n</code></pre> <p>               Bases: <code>Feature</code></p> <p>Multi-timeframe feature via offset resampling.</p> <p>Creates <code>window</code> parallel time series with different offsets. Each offset computes features as if on <code>window</code>-minute bars.</p> <p>Supports both regular Feature (compute_pair) and GlobalFeature (compute).</p> <p>Parameters:</p> Name Type Description Default <code>feature_name</code> <code>str</code> <p>Registered component name (sf_component name).</p> <code>None</code> <code>feature_params</code> <code>dict</code> <p>Parameters for feature class.</p> <code>dict()</code> <code>window</code> <code>int</code> <p>Aggregation window in minutes. Default: 15.</p> <code>15</code> <code>prefix</code> <code>str | None</code> <p>Prefix for output columns. Default: \"{window}m_\".</p> <code>None</code> Example <p>offset = OffsetFeature( ...     feature_name=\"test_rsi\", ...     feature_params={\"period\": 14}, ...     window=15, ... )</p>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature--outputs-15m_rsi_14-offset","title":"Outputs: 15m_rsi_14, offset","text":""},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature--with-globalfeature","title":"With GlobalFeature","text":"<p>offset = OffsetFeature( ...     feature_name=\"global/market_log_return\", ...     feature_params={}, ...     window=15, ... )</p>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.feature_name","title":"feature_name  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_name: str = None\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.feature_params","title":"feature_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_params: dict = field(default_factory=dict)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.outputs","title":"outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>outputs = ['offset']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.prefix","title":"prefix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>prefix: str | None = None\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.requires","title":"requires  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requires = ['open', 'high', 'low', 'close', 'volume', 'timestamp']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.window","title":"window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>window: int = 15\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def __post_init__(self):\n    if self.feature_name is None:\n        raise ValueError(\"OffsetFeature requires 'feature_name'\")\n\n    self._feature_cls = default_registry.get(SfComponentType.FEATURE, self.feature_name)\n    self._base = self._feature_cls(**self.feature_params)\n    self._is_global = isinstance(self._base, GlobalFeature)\n\n    if self.prefix is None:\n        self.prefix = f\"{self.window}m_\"\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature._compute_all_pairs_global","title":"_compute_all_pairs_global","text":"<pre><code>_compute_all_pairs_global(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute features for all pairs with global base feature.</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def _compute_all_pairs_global(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute features for all pairs with global base feature.\"\"\"\n    df = df.sort([self.group_col, self.ts_col])\n    original_len = len(df)\n\n    df = df.with_columns(\n        pl.col(self.ts_col).rank(\"ordinal\").over(self.group_col).cast(pl.UInt32).alias(\"_orig_idx\") - 1\n    )\n\n    df = df.with_columns((pl.col(\"_orig_idx\") % self.window).cast(pl.UInt8).alias(\"offset\"))\n\n    offset_results = []\n    for offset in range(self.window):\n        resampled = (\n            df.drop([\"_orig_idx\", \"offset\"])\n            .with_columns(\n                (pl.col(self.ts_col).rank(\"ordinal\").over(self.group_col).cast(pl.Int64) - 1 - offset)\n                .floordiv(self.window)\n                .alias(\"_grp\")\n            )\n            .group_by([self.group_col, \"_grp\"], maintain_order=True)\n            .agg(\n                [\n                    pl.col(self.ts_col).last(),\n                    pl.col(\"open\").first(),\n                    pl.col(\"high\").max(),\n                    pl.col(\"low\").min(),\n                    pl.col(\"close\").last(),\n                    pl.col(\"volume\").sum(),\n                ]\n            )\n        )\n\n        with_feat = self._compute_base_feature(resampled)\n        with_feat = with_feat.with_columns(pl.lit(offset).cast(pl.UInt8).alias(\"_offset\"))\n\n        for col in self._base.output_cols():\n            if col in with_feat.columns:\n                with_feat = with_feat.rename({col: f\"{self.prefix}{col}\"})\n\n        offset_results.append(with_feat)\n\n    all_offsets = pl.concat(offset_results)\n\n    df = df.with_columns(\n        ((pl.col(\"_orig_idx\").cast(pl.Int64) - pl.col(\"offset\").cast(pl.Int64)) // self.window).alias(\"_grp\")\n    )\n\n    feature_cols = [f\"{self.prefix}{col}\" for col in self._base.output_cols()]\n\n    result = df.join(\n        all_offsets.select([self.group_col, \"_grp\", \"_offset\"] + feature_cols),\n        left_on=[self.group_col, \"_grp\", \"offset\"],\n        right_on=[self.group_col, \"_grp\", \"_offset\"],\n        how=\"left\",\n    )\n\n    result = result.drop([\"_orig_idx\", \"_grp\"])\n    assert len(result) == original_len\n\n    return result\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature._compute_base_feature","title":"_compute_base_feature","text":"<pre><code>_compute_base_feature(resampled: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute base feature - handles both Feature and GlobalFeature.</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def _compute_base_feature(self, resampled: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute base feature - handles both Feature and GlobalFeature.\"\"\"\n    if self._is_global:\n        return self._base.compute(resampled)\n    else:\n        return self._base.compute_pair(resampled)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature._compute_single_pair","title":"_compute_single_pair","text":"<pre><code>_compute_single_pair(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute features for single pair (non-global base).</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def _compute_single_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute features for single pair (non-global base).\"\"\"\n    df = df.sort(self.ts_col)\n    original_len = len(df)\n    df = df.with_row_index(\"_orig_idx\")\n\n    df = df.with_columns((pl.col(\"_orig_idx\") % self.window).cast(pl.UInt8).alias(\"offset\"))\n\n    offset_results = []\n    for offset in range(self.window):\n        resampled = self._resample_ohlcv(df.drop([\"_orig_idx\", \"offset\"]), offset)\n\n        with_feat = self._compute_base_feature(resampled)\n        with_feat = with_feat.with_columns(pl.lit(offset).cast(pl.UInt8).alias(\"_offset\"))\n\n        for col in self._base.output_cols():\n            if col in with_feat.columns:\n                with_feat = with_feat.rename({col: f\"{self.prefix}{col}\"})\n\n        offset_results.append(with_feat)\n\n    all_offsets = pl.concat(offset_results)\n\n    df = df.with_columns(\n        ((pl.col(\"_orig_idx\").cast(pl.Int64) - pl.col(\"offset\").cast(pl.Int64)) // self.window).alias(\"_grp\")\n    )\n\n    feature_cols = [f\"{self.prefix}{col}\" for col in self._base.output_cols()]\n\n    result = df.join(\n        all_offsets.select([\"_grp\", \"_offset\"] + feature_cols),\n        left_on=[\"_grp\", \"offset\"],\n        right_on=[\"_grp\", \"_offset\"],\n        how=\"left\",\n    )\n\n    result = result.drop([\"_orig_idx\", \"_grp\"])\n    assert len(result) == original_len\n\n    return result\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature._resample_ohlcv","title":"_resample_ohlcv","text":"<pre><code>_resample_ohlcv(df: DataFrame, offset: int) -&gt; pl.DataFrame\n</code></pre> <p>Resample 1m OHLCV to window-minute bars with given offset.</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def _resample_ohlcv(self, df: pl.DataFrame, offset: int) -&gt; pl.DataFrame:\n    \"\"\"Resample 1m OHLCV to window-minute bars with given offset.\"\"\"\n    df = df.with_row_index(\"_row_idx\")\n\n    df = df.with_columns(((pl.col(\"_row_idx\").cast(pl.Int64) - offset) // self.window).alias(\"_grp\"))\n\n    agg_exprs = [\n        pl.col(self.ts_col).last(),\n        pl.col(\"open\").first(),\n        pl.col(\"high\").max(),\n        pl.col(\"low\").min(),\n        pl.col(\"close\").last(),\n        pl.col(\"volume\").sum(),\n    ]\n    if self.group_col in df.columns:\n        agg_exprs.append(pl.col(self.group_col).first())\n\n    return df.group_by(\"_grp\", maintain_order=True).agg(agg_exprs)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute for all pairs.</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def compute(self, df: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute for all pairs.\"\"\"\n    if self._is_global:\n        return self._compute_all_pairs_global(df)\n    else:\n        return df.group_by(self.group_col, maintain_order=True).map_groups(self._compute_single_pair)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.compute_pair","title":"compute_pair","text":"<pre><code>compute_pair(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute for single pair (only for non-global base).</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute for single pair (only for non-global base).\"\"\"\n    if self._is_global:\n        raise NotImplementedError(\"GlobalFeature base requires compute(), not compute_pair()\")\n    return self._compute_single_pair(df)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: dict) -&gt; OffsetFeature\n</code></pre> <p>Deserialize from config.</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: dict) -&gt; \"OffsetFeature\":\n    \"\"\"Deserialize from config.\"\"\"\n    return cls(\n        feature_name=data[\"feature_name\"],\n        feature_params=data[\"feature_params\"],\n        window=data[\"window\"],\n        prefix=data.get(\"prefix\"),\n    )\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.output_cols","title":"output_cols","text":"<pre><code>output_cols(prefix: str = '') -&gt; list[str]\n</code></pre> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def output_cols(self, prefix: str = \"\") -&gt; list[str]:\n    base_cols = self._base.output_cols(prefix=f\"{prefix}{self.prefix}\")\n    return base_cols + [f\"{prefix}offset\"]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.required_cols","title":"required_cols","text":"<pre><code>required_cols() -&gt; list[str]\n</code></pre> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def required_cols(self) -&gt; list[str]:\n    return [\"open\", \"high\", \"low\", \"close\", \"volume\", self.ts_col]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.offset_feature.OffsetFeature.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Serialize for Kedro.</p> Source code in <code>src/signalflow/feature/offset_feature.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Serialize for Kedro.\"\"\"\n    return {\n        \"feature_name\": self.feature_name,\n        \"feature_params\": self.feature_params,\n        \"window\": self.window,\n        \"prefix\": self.prefix,\n    }\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature","title":"signalflow.feature.lin_reg_forecast.LinRegForecastFeature  <code>dataclass</code>","text":"<pre><code>LinRegForecastFeature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None, source_col: str = 'rsi_14', n_lags: int = 10, trend_window: int = 5, mean_window: int = 20, refit_period: Literal['hour', 'day', 'week', 'month', None] = 'day', alpha: float = 1.0, forecast_horizon: int = 1, min_samples: int = 50)\n</code></pre> <p>               Bases: <code>Feature</code></p> <p>Enhanced linear regression forecast with trend and mean-reversion features.</p> <p>Instead of predicting raw values, predicts change (diff) and adds: - Trend slope (recent momentum) - Mean reversion signal (deviation from rolling mean) - Volatility scaling</p> <p>Parameters:</p> Name Type Description Default <code>source_col</code> <code>str</code> <p>Column to forecast.</p> <code>'rsi_14'</code> <code>n_lags</code> <code>int</code> <p>Number of lagged diffs. Default: 10.</p> <code>10</code> <code>trend_window</code> <code>int</code> <p>Window for trend calculation. Default: 5.</p> <code>5</code> <code>mean_window</code> <code>int</code> <p>Window for mean reversion. Default: 20.</p> <code>20</code> <code>refit_period</code> <code>Literal['hour', 'day', 'week', 'month', None]</code> <p>When to refit. Default: 'day'.</p> <code>'day'</code> <code>alpha</code> <code>float</code> <p>Ridge regularization. Default: 1.0.</p> <code>1.0</code> <code>forecast_horizon</code> <code>int</code> <p>Steps ahead to forecast. Default: 1.</p> <code>1</code>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.alpha","title":"alpha  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>alpha: float = 1.0\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.forecast_horizon","title":"forecast_horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>forecast_horizon: int = 1\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.mean_window","title":"mean_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mean_window: int = 20\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.min_samples","title":"min_samples  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_samples: int = 50\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.n_lags","title":"n_lags  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_lags: int = 10\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.outputs","title":"outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>outputs = ['{source_col}_forecast', '{source_col}_forecast_change', '{source_col}_forecast_direction']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.refit_period","title":"refit_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>refit_period: Literal['hour', 'day', 'week', 'month', None] = 'day'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.requires","title":"requires  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requires = ['{source_col}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.source_col","title":"source_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>source_col: str = 'rsi_14'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.test_params","title":"test_params  <code>class-attribute</code>","text":"<pre><code>test_params: list[dict] = [{'source_col': 'rsi_14', 'n_lags': 10}, {'source_col': 'rsi_14', 'n_lags': 5, 'mean_window': 10}]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.trend_window","title":"trend_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>trend_window: int = 5\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.warmup","title":"warmup  <code>property</code>","text":"<pre><code>warmup: int\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__()\n</code></pre> Source code in <code>src/signalflow/feature/lin_reg_forecast.py</code> <pre><code>def __post_init__(self):\n    if self.n_lags &lt; 1:\n        raise ValueError(f\"n_lags must be &gt;= 1\")\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature._build_features","title":"_build_features","text":"<pre><code>_build_features(values: ndarray) -&gt; np.ndarray\n</code></pre> <p>Build enhanced feature matrix.</p> Source code in <code>src/signalflow/feature/lin_reg_forecast.py</code> <pre><code>def _build_features(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build enhanced feature matrix.\"\"\"\n    n = len(values)\n\n    diffs = np.diff(values, prepend=values[0])\n\n    n_features = self.n_lags + 3\n    X = np.full((n, n_features), np.nan, dtype=np.float64)\n\n    start_idx = max(self.n_lags, self.mean_window)\n\n    for i in range(start_idx, n):\n        for lag in range(self.n_lags):\n            X[i, lag] = diffs[i - lag - 1]\n\n        window = values[i - self.trend_window : i]\n        if len(window) == self.trend_window:\n            x_trend = np.arange(self.trend_window)\n            X[i, self.n_lags] = np.polyfit(x_trend, window, 1)[0]\n\n        mean_window = values[i - self.mean_window : i]\n        if len(mean_window) == self.mean_window:\n            mean_val = np.mean(mean_window)\n            std_val = np.std(mean_window)\n            if std_val &gt; 1e-8:\n                X[i, self.n_lags + 1] = (values[i] - mean_val) / std_val\n            else:\n                X[i, self.n_lags + 1] = 0\n\n        vol_window = diffs[i - self.trend_window : i]\n        if len(vol_window) == self.trend_window:\n            X[i, self.n_lags + 2] = np.std(vol_window)\n\n    return X\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature._build_targets","title":"_build_targets","text":"<pre><code>_build_targets(values: ndarray) -&gt; np.ndarray\n</code></pre> <p>Build target: forward diff (change).</p> Source code in <code>src/signalflow/feature/lin_reg_forecast.py</code> <pre><code>def _build_targets(self, values: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Build target: forward diff (change).\"\"\"\n    n = len(values)\n    y = np.full(n, np.nan, dtype=np.float64)\n\n    if self.forecast_horizon &lt; n:\n        y[: -self.forecast_horizon] = np.diff(\n            values, n=self.forecast_horizon, append=[np.nan] * self.forecast_horizon\n        )[: -self.forecast_horizon]\n        for i in range(n - self.forecast_horizon):\n            y[i] = values[i + self.forecast_horizon] - values[i]\n\n    return y\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature._get_period_key","title":"_get_period_key","text":"<pre><code>_get_period_key(ts: datetime) -&gt; tuple | None\n</code></pre> Source code in <code>src/signalflow/feature/lin_reg_forecast.py</code> <pre><code>def _get_period_key(self, ts: datetime) -&gt; tuple | None:\n    if self.refit_period == \"hour\":\n        return (ts.year, ts.month, ts.day, ts.hour)\n    elif self.refit_period == \"day\":\n        return (ts.year, ts.month, ts.day)\n    elif self.refit_period == \"week\":\n        return (ts.year, ts.isocalendar()[1])\n    elif self.refit_period == \"month\":\n        return (ts.year, ts.month)\n    return None\n</code></pre>"},{"location":"api/feature/#signalflow.feature.lin_reg_forecast.LinRegForecastFeature.compute_pair","title":"compute_pair","text":"<pre><code>compute_pair(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute forecasts for single pair.</p> Source code in <code>src/signalflow/feature/lin_reg_forecast.py</code> <pre><code>def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute forecasts for single pair.\"\"\"\n    values = df[self.source_col].to_numpy().astype(np.float64)\n    timestamps = df[self.ts_col].to_list()\n    n = len(values)\n\n    X = self._build_features(values)\n    y = self._build_targets(values)\n\n    forecasts = np.full(n, np.nan, dtype=np.float64)\n    forecast_changes = np.full(n, np.nan, dtype=np.float64)\n\n    valid_mask = ~np.any(np.isnan(X), axis=1)\n    target_valid = ~np.isnan(y)\n    train_valid = valid_mask &amp; target_valid\n\n    current_period = None\n    model = Ridge(alpha=self.alpha)\n    fitted = False\n\n    for i in range(self.min_samples, n):\n        if not valid_mask[i]:\n            continue\n\n        period = self._get_period_key(timestamps[i])\n\n        if period != current_period:\n            current_period = period\n            train_idx = np.where(train_valid[:i])[0]\n            if len(train_idx) &gt;= 20:\n                model.fit(X[train_idx], y[train_idx])\n                fitted = True\n\n        if fitted:\n            predicted_change = model.predict(X[i : i + 1])[0]\n            forecast_changes[i] = predicted_change\n            forecasts[i] = values[i] + predicted_change\n\n    forecast_col = f\"{self.source_col}_forecast\"\n    change_col = f\"{self.source_col}_forecast_change\"\n    direction_col = f\"{self.source_col}_forecast_direction\"\n\n    return df.with_columns(\n        [\n            pl.Series(name=forecast_col, values=forecasts),\n            pl.Series(name=change_col, values=forecast_changes),\n            pl.Series(name=direction_col, values=np.sign(forecast_changes)),\n        ]\n    )\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature","title":"signalflow.feature.atr.ATRFeature  <code>dataclass</code>","text":"<pre><code>ATRFeature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None, period: int = 14, smoothing: Literal['sma', 'ema'] = 'ema')\n</code></pre> <p>               Bases: <code>Feature</code></p> <p>Average True Range (ATR) feature.</p> <p>Measures market volatility as the moving average of True Range. True Range = max(high - low, |high - prev_close|, |low - prev_close|)</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>ATR period. Default: 14.</p> <code>14</code> <code>smoothing</code> <code>Literal['sma', 'ema']</code> <p>Smoothing method - \"sma\" or \"ema\" (Wilder's). Default: \"ema\".</p> <code>'ema'</code> Example <p>atr = ATRFeature(period=14) atr.output_cols()  # [\"atr_14\"]</p>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature.outputs","title":"outputs  <code>class-attribute</code>","text":"<pre><code>outputs: list[str] = ['atr_{period}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature.period","title":"period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>period: int = 14\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature.requires","title":"requires  <code>class-attribute</code>","text":"<pre><code>requires: list[str] = ['high', 'low', 'close']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature.smoothing","title":"smoothing  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>smoothing: Literal['sma', 'ema'] = 'ema'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature.test_params","title":"test_params  <code>class-attribute</code>","text":"<pre><code>test_params: list[dict] = [{'period': 14}, {'period': 14, 'smoothing': 'sma'}, {'period': 20}]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature.warmup","title":"warmup  <code>property</code>","text":"<pre><code>warmup: int\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature._get_output_name","title":"_get_output_name","text":"<pre><code>_get_output_name() -&gt; str\n</code></pre> Source code in <code>src/signalflow/feature/atr.py</code> <pre><code>def _get_output_name(self) -&gt; str:\n    suffix = \"_norm\" if self.normalized else \"\"\n    return f\"atr_{self.period}{suffix}\"\n</code></pre>"},{"location":"api/feature/#signalflow.feature.atr.ATRFeature.compute_pair","title":"compute_pair","text":"<pre><code>compute_pair(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute ATR for a single pair.</p> Source code in <code>src/signalflow/feature/atr.py</code> <pre><code>def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute ATR for a single pair.\"\"\"\n    col_name = self._get_output_name()\n\n    high = pl.col(\"high\")\n    low = pl.col(\"low\")\n    prev_close = pl.col(\"close\").shift(1)\n\n    # True Range = max(H-L, |H-prevC|, |L-prevC|)\n    tr = pl.max_horizontal(\n        high - low,\n        (high - prev_close).abs(),\n        (low - prev_close).abs(),\n    )\n\n    # Apply smoothing\n    if self.smoothing == \"sma\":\n        atr = tr.rolling_mean(window_size=self.period)\n    else:\n        # EMA (Wilder's smoothing)\n        atr = tr.ewm_mean(span=self.period, adjust=False)\n\n    df = df.with_columns(atr.alias(col_name))\n\n    # Optional z-score normalization\n    if self.normalized:\n        from signalflow.feature.examples import _get_norm_window, _normalize_zscore\n\n        norm_window = self.norm_period or _get_norm_window(self.period)\n        vals = df[col_name].to_numpy()\n        normed = _normalize_zscore(vals, window=norm_window)\n        df = df.with_columns(pl.Series(name=col_name, values=normed))\n\n    return df\n</code></pre>"},{"location":"api/feature/#examples","title":"Examples","text":""},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature","title":"signalflow.feature.examples.ExampleRsiFeature  <code>dataclass</code>","text":"<pre><code>ExampleRsiFeature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None, period: int = 14, price_col: str = 'close')\n</code></pre> <p>               Bases: <code>Feature</code></p> <p>Relative Strength Index.</p> <p>Bounded oscillator [0, 100]. In normalized mode, rescales to [-1, 1].</p> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>RSI period. Default: 14.</p> <code>14</code> <code>price_col</code> <code>str</code> <p>Price column to use. Default: \"close\".</p> <code>'close'</code> Example <p>rsi = ExampleRsiFeature(period=21) rsi.output_cols()  # [\"rsi_21\"]</p>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.outputs","title":"outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>outputs = ['rsi_{period}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.period","title":"period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>period: int = 14\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.requires","title":"requires  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requires = ['{price_col}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.test_params","title":"test_params  <code>class-attribute</code>","text":"<pre><code>test_params: list[dict] = [{'period': 14}, {'period': 14, 'normalized': True}, {'period': 21}]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.warmup","title":"warmup  <code>property</code>","text":"<pre><code>warmup: int\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature._get_output_name","title":"_get_output_name","text":"<pre><code>_get_output_name() -&gt; str\n</code></pre> Source code in <code>src/signalflow/feature/examples.py</code> <pre><code>def _get_output_name(self) -&gt; str:\n    suffix = \"_norm\" if self.normalized else \"\"\n    return f\"rsi_{self.period}{suffix}\"\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute RSI for all pairs.</p> Source code in <code>src/signalflow/feature/examples.py</code> <pre><code>def compute(self, df: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute RSI for all pairs.\"\"\"\n    return df.group_by(self.group_col, maintain_order=True).map_groups(self.compute_pair)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleRsiFeature.compute_pair","title":"compute_pair","text":"<pre><code>compute_pair(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute RSI for single pair.</p> Source code in <code>src/signalflow/feature/examples.py</code> <pre><code>def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute RSI for single pair.\"\"\"\n    col_name = self._get_output_name()\n\n    delta = pl.col(self.price_col).diff()\n    gain = pl.when(delta &gt; 0).then(delta).otherwise(0)\n    loss = pl.when(delta &lt; 0).then(-delta).otherwise(0)\n\n    avg_gain = gain.rolling_mean(window_size=self.period)\n    avg_loss = loss.rolling_mean(window_size=self.period)\n\n    rs = avg_gain / avg_loss\n    rsi = 100 - (100 / (1 + rs))\n\n    df = df.with_columns(rsi.alias(col_name))\n\n    # Normalization: rescale bounded [0, 100] \u2192 [-1, 1]\n    if self.normalized:\n        df = df.with_columns(((pl.col(col_name) - 50) / 50).alias(col_name))\n\n    return df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature","title":"signalflow.feature.examples.ExampleSmaFeature  <code>dataclass</code>","text":"<pre><code>ExampleSmaFeature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None, period: int = 20, price_col: str = 'close')\n</code></pre> <p>               Bases: <code>Feature</code></p> <p>Simple Moving Average.</p>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature.outputs","title":"outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>outputs = ['sma_{period}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature.period","title":"period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>period: int = 20\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature.requires","title":"requires  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requires = ['{price_col}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature.test_params","title":"test_params  <code>class-attribute</code>","text":"<pre><code>test_params: list[dict] = [{'period': 20}, {'period': 50}]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature.warmup","title":"warmup  <code>property</code>","text":"<pre><code>warmup: int\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature._get_output_name","title":"_get_output_name","text":"<pre><code>_get_output_name() -&gt; str\n</code></pre> Source code in <code>src/signalflow/feature/examples.py</code> <pre><code>def _get_output_name(self) -&gt; str:\n    suffix = \"_norm\" if self.normalized else \"\"\n    return f\"sma_{self.period}{suffix}\"\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleSmaFeature.compute_pair","title":"compute_pair","text":"<pre><code>compute_pair(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/feature/examples.py</code> <pre><code>def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    col_name = self._get_output_name()\n    sma = pl.col(self.price_col).rolling_mean(window_size=self.period)\n    df = df.with_columns(sma.alias(col_name))\n\n    if self.normalized:\n        norm_window = self.norm_period or _get_norm_window(self.period)\n        vals = df[col_name].to_numpy()\n        normed = _normalize_zscore(vals, window=norm_window)\n        df = df.with_columns(pl.Series(name=col_name, values=normed))\n\n    return df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature","title":"signalflow.feature.examples.ExampleGlobalMeanRsiFeature  <code>dataclass</code>","text":"<pre><code>ExampleGlobalMeanRsiFeature(group_col: str = 'pair', ts_col: str = 'timestamp', normalized: bool = False, norm_period: int | None = None, period: int = 14, price_col: str = 'close', add_diff: bool = False)\n</code></pre> <p>               Bases: <code>GlobalFeature</code></p> <p>Mean RSI across all pairs per timestamp.</p> <ol> <li>Compute RSI per pair</li> <li>Mean across all pairs at time t -&gt; global_mean_rsi</li> <li>Optionally: rsi_diff = pair_rsi - global_mean_rsi</li> </ol> <p>Parameters:</p> Name Type Description Default <code>period</code> <code>int</code> <p>RSI period. Default: 14.</p> <code>14</code> <code>add_diff</code> <code>bool</code> <p>Add per-pair difference column. Default: False.</p> <code>False</code>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.add_diff","title":"add_diff  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>add_diff: bool = False\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.outputs","title":"outputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>outputs = ['global_mean_rsi_{period}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.period","title":"period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>period: int = 14\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.requires","title":"requires  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>requires = ['{price_col}']\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.test_params","title":"test_params  <code>class-attribute</code>","text":"<pre><code>test_params: list[dict] = [{'period': 14}, {'period': 14, 'add_diff': True}]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.warmup","title":"warmup  <code>property</code>","text":"<pre><code>warmup: int\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/feature/examples.py</code> <pre><code>def compute(self, df: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    rsi_col = f\"rsi_{self.period}\"\n    out_col = f\"global_mean_rsi_{self.period}\"\n\n    has_rsi = rsi_col in df.columns\n    if not has_rsi:\n        rsi = ExampleRsiFeature(period=self.period, price_col=self.price_col)\n        df = rsi.compute(df)\n\n    mean_df = df.group_by(self.ts_col).agg(pl.col(rsi_col).mean().alias(out_col))\n\n    df = df.join(mean_df, on=self.ts_col, how=\"left\")\n\n    if self.add_diff:\n        df = df.with_columns((pl.col(rsi_col) - pl.col(out_col)).alias(f\"rsi_{self.period}_diff\"))\n\n    if not has_rsi:\n        df = df.drop(rsi_col)\n\n    return df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.examples.ExampleGlobalMeanRsiFeature.output_cols","title":"output_cols","text":"<pre><code>output_cols(prefix: str = '') -&gt; list[str]\n</code></pre> Source code in <code>src/signalflow/feature/examples.py</code> <pre><code>def output_cols(self, prefix: str = \"\") -&gt; list[str]:\n    cols = [f\"{prefix}global_mean_rsi_{self.period}\"]\n    if self.add_diff:\n        cols.append(f\"{prefix}rsi_{self.period}_diff\")\n    return cols\n</code></pre>"},{"location":"api/feature/#feature-informativeness","title":"Feature Informativeness","text":"<p>Measures how informative each feature is relative to multiple targets at multiple prediction horizons. Combines MI magnitude with temporal stability into a composite score.</p>"},{"location":"api/feature/#usage","title":"Usage","text":"<pre><code>from signalflow.feature.informativeness import FeatureInformativenessAnalyzer\nfrom signalflow.detector.market import MarketZScoreDetector\n\nanalyzer = FeatureInformativenessAnalyzer(\n    event_detector=MarketZScoreDetector(z_threshold=3.0),\n)\nreport = analyzer.analyze(df, feature_columns=[\"rsi_14\", \"sma_20\", \"volume_ratio\"])\n\nprint(report.top_features(10))      # best features by composite score\nprint(report.score_matrix)          # NMI heatmap: feature x (horizon, target)\nreport.feature_detail(\"rsi_14\")     # per-target breakdown for one feature\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer","title":"signalflow.feature.informativeness.FeatureInformativenessAnalyzer  <code>dataclass</code>","text":"<pre><code>FeatureInformativenessAnalyzer(target_generator: MultiTargetGenerator = MultiTargetGenerator(), event_detector: SignalDetector | None = _default_event_detector(), rolling_mi: RollingMIConfig = RollingMIConfig(), weights: CompositeWeights = CompositeWeights(), bins: int = 20, pair_col: str = 'pair', ts_col: str = 'timestamp', aggregate_pairs: bool = True)\n</code></pre> <p>Orchestrator for feature informativeness analysis.</p> Pipeline <ol> <li>Generate multi-horizon, multi-target labels</li> <li>Detect and mask global events</li> <li>Compute MI between each feature and each target</li> <li>Compute rolling MI for temporal stability</li> <li>Compute composite weighted scores</li> <li>Generate report</li> </ol> <p>Attributes:</p> Name Type Description <code>target_generator</code> <code>MultiTargetGenerator</code> <p>Multi-target label generator.</p> <code>event_detector</code> <code>SignalDetector | None</code> <p>Global event detector. None to disable.</p> <code>rolling_mi</code> <code>RollingMIConfig</code> <p>Rolling MI stability configuration.</p> <code>weights</code> <code>CompositeWeights</code> <p>Composite scoring weights.</p> <code>bins</code> <code>int</code> <p>Number of histogram bins for MI estimation.</p> <code>pair_col</code> <code>str</code> <p>Pair column name.</p> <code>ts_col</code> <code>str</code> <p>Timestamp column name.</p> <code>aggregate_pairs</code> <code>bool</code> <p>If True, pool all pairs for MI computation.</p>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.aggregate_pairs","title":"aggregate_pairs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>aggregate_pairs: bool = True\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.bins","title":"bins  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>bins: int = 20\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.event_detector","title":"event_detector  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>event_detector: SignalDetector | None = field(default_factory=_default_event_detector)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.rolling_mi","title":"rolling_mi  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>rolling_mi: RollingMIConfig = field(default_factory=RollingMIConfig)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.target_generator","title":"target_generator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_generator: MultiTargetGenerator = field(default_factory=MultiTargetGenerator)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.weights","title":"weights  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>weights: CompositeWeights = field(default_factory=CompositeWeights)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._build_score_matrix","title":"_build_score_matrix","text":"<pre><code>_build_score_matrix(raw_mi: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Build pivoted Feature \u00d7 (Horizon, Target) matrix.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _build_score_matrix(self, raw_mi: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Build pivoted Feature \u00d7 (Horizon, Target) matrix.\"\"\"\n    if raw_mi.height == 0:\n        return pl.DataFrame()\n\n    matrix = raw_mi.with_columns((pl.col(\"horizon\") + \"_\" + pl.col(\"target_type\")).alias(\"_col_key\")).pivot(\n        on=\"_col_key\",\n        index=\"feature\",\n        values=\"nmi\",\n    )\n\n    return matrix\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._compute_all_mi","title":"_compute_all_mi","text":"<pre><code>_compute_all_mi(df: DataFrame, feature_columns: list[str], target_meta: list[dict[str, str]]) -&gt; list[dict]\n</code></pre> <p>Compute MI for all (feature, target) pairs.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _compute_all_mi(\n    self,\n    df: pl.DataFrame,\n    feature_columns: list[str],\n    target_meta: list[dict[str, str]],\n) -&gt; list[dict]:\n    \"\"\"Compute MI for all (feature, target) pairs.\"\"\"\n    rows = []\n\n    for feat_col in feature_columns:\n        for tmeta in target_meta:\n            target_col = tmeta[\"column\"]\n            target_kind = tmeta[\"kind\"]\n\n            feat_arr, target_arr = self._extract_arrays(df, feat_col, target_col)\n            if feat_arr is None:\n                rows.append(self._nan_row(feat_col, tmeta))\n                continue\n\n            mi = self._compute_mi_pair(feat_arr, target_arr, target_kind)\n            h_feat = entropy_continuous(feat_arr, bins=self.bins)\n            h_target = (\n                entropy_discrete(target_arr)\n                if target_kind == \"discrete\"\n                else entropy_continuous(target_arr, bins=self.bins)\n            )\n            nmi = normalized_mutual_information(mi, h_feat, h_target)\n\n            stability = self._compute_stability(feat_arr, target_arr, target_kind)\n\n            rows.append(\n                {\n                    \"feature\": feat_col,\n                    \"horizon\": tmeta[\"horizon\"],\n                    \"target_type\": tmeta[\"target_type\"],\n                    \"mi\": mi,\n                    \"nmi\": nmi,\n                    \"stability_score\": stability,\n                }\n            )\n\n    return rows\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._compute_composite","title":"_compute_composite","text":"<pre><code>_compute_composite(raw_mi: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Compute composite scores from raw MI results.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _compute_composite(self, raw_mi: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Compute composite scores from raw MI results.\"\"\"\n    if raw_mi.height == 0:\n        return pl.DataFrame(schema={\"feature\": pl.Utf8, \"composite_score\": pl.Float64, \"rank\": pl.UInt32})\n\n    w = self.weights\n    alpha = w.stability_weight\n\n    # Build weights per (horizon, target_type)\n    horizons = raw_mi.get_column(\"horizon\").unique().to_list()\n    targets = raw_mi.get_column(\"target_type\").unique().to_list()\n\n    h_weights = w.horizon_weights or {h: 1.0 / len(horizons) for h in horizons}\n    t_weights = w.target_weights or {t: 1.0 / len(targets) for t in targets}\n\n    # Normalize\n    h_total = sum(h_weights.values())\n    t_total = sum(t_weights.values())\n    h_weights = {k: v / h_total for k, v in h_weights.items()}\n    t_weights = {k: v / t_total for k, v in t_weights.items()}\n\n    scored = (\n        raw_mi.with_columns(\n            [\n                pl.col(\"horizon\").replace_strict(h_weights, default=0.0).alias(\"_h_w\"),\n                pl.col(\"target_type\").replace_strict(t_weights, default=0.0).alias(\"_t_w\"),\n            ]\n        )\n        .with_columns((pl.col(\"_h_w\") * pl.col(\"_t_w\")).alias(\"_weight\"))\n        .with_columns(\n            ((1.0 - alpha) * pl.col(\"nmi\").fill_null(0.0) + alpha * pl.col(\"stability_score\").fill_null(0.0)).alias(\n                \"_cell_score\"\n            )\n        )\n        .with_columns((pl.col(\"_cell_score\") * pl.col(\"_weight\")).alias(\"_weighted_score\"))\n    )\n\n    result = (\n        scored.group_by(\"feature\")\n        .agg(pl.col(\"_weighted_score\").sum().alias(\"composite_score\"))\n        .sort(\"composite_score\", descending=True)\n        .with_row_index(\"rank\", offset=1)\n        .select([\"feature\", \"composite_score\", \"rank\"])\n    )\n\n    return result\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._compute_mi_pair","title":"_compute_mi_pair","text":"<pre><code>_compute_mi_pair(feat: ndarray, target: ndarray, target_kind: str) -&gt; float\n</code></pre> <p>Compute MI between one feature and one target.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _compute_mi_pair(\n    self,\n    feat: np.ndarray,\n    target: np.ndarray,\n    target_kind: str,\n) -&gt; float:\n    \"\"\"Compute MI between one feature and one target.\"\"\"\n    if target_kind == \"discrete\":\n        return mutual_information_continuous_discrete(feat, target, bins=self.bins)\n    return mutual_information_continuous(feat, target, bins=self.bins)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._compute_stability","title":"_compute_stability","text":"<pre><code>_compute_stability(feat: ndarray, target: ndarray, target_kind: str) -&gt; float\n</code></pre> <p>Compute temporal stability via rolling MI windows.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _compute_stability(\n    self,\n    feat: np.ndarray,\n    target: np.ndarray,\n    target_kind: str,\n) -&gt; float:\n    \"\"\"Compute temporal stability via rolling MI windows.\"\"\"\n    cfg = self.rolling_mi\n    n = len(feat)\n    step = cfg.window_size\n    min_fill = int(step * cfg.min_window_fill)\n\n    mi_values = []\n    for start in range(0, n - min_fill + 1, step):\n        end = min(start + step, n)\n        f_win = feat[start:end]\n        t_win = target[start:end]\n\n        # Check fill rate\n        if np.issubdtype(f_win.dtype, np.floating):\n            valid = np.isfinite(f_win).sum()\n        else:\n            valid = len(f_win)\n\n        if valid &lt; min_fill:\n            continue\n\n        mi = self._compute_mi_pair(f_win, t_win, target_kind)\n        if not np.isnan(mi):\n            mi_values.append(mi)\n\n    if len(mi_values) &lt; 2:\n        return np.nan\n\n    mean_mi = np.mean(mi_values)\n    std_mi = np.std(mi_values)\n\n    if mean_mi &lt;= 0:\n        return 0.0\n\n    cv = std_mi / mean_mi\n    return 1.0 / (1.0 + cv)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._extract_arrays","title":"_extract_arrays","text":"<pre><code>_extract_arrays(df: DataFrame, feat_col: str, target_col: str) -&gt; tuple[np.ndarray | None, np.ndarray | None]\n</code></pre> <p>Extract aligned numpy arrays, dropping rows with nulls in either.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _extract_arrays(\n    self,\n    df: pl.DataFrame,\n    feat_col: str,\n    target_col: str,\n) -&gt; tuple[np.ndarray | None, np.ndarray | None]:\n    \"\"\"Extract aligned numpy arrays, dropping rows with nulls in either.\"\"\"\n    if feat_col not in df.columns or target_col not in df.columns:\n        return None, None\n\n    subset = df.select([feat_col, target_col]).drop_nulls()\n    if subset.height &lt; 10:\n        return None, None\n\n    feat_arr = subset.get_column(feat_col).to_numpy().astype(np.float64)\n    target_series = subset.get_column(target_col)\n\n    if target_series.dtype == pl.Utf8:\n        target_arr = target_series.to_numpy()\n    else:\n        target_arr = target_series.to_numpy().astype(np.float64)\n\n    return feat_arr, target_arr\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._nan_row","title":"_nan_row","text":"<pre><code>_nan_row(feat_col: str, tmeta: dict) -&gt; dict\n</code></pre> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _nan_row(self, feat_col: str, tmeta: dict) -&gt; dict:\n    return {\n        \"feature\": feat_col,\n        \"horizon\": tmeta[\"horizon\"],\n        \"target_type\": tmeta[\"target_type\"],\n        \"mi\": np.nan,\n        \"nmi\": np.nan,\n        \"stability_score\": np.nan,\n    }\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer._validate","title":"_validate","text":"<pre><code>_validate(df: DataFrame, feature_columns: list[str]) -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def _validate(self, df: pl.DataFrame, feature_columns: list[str]) -&gt; None:\n    if not feature_columns:\n        raise ValueError(\"feature_columns must not be empty\")\n\n    required = {self.pair_col, self.ts_col}\n    missing = required - set(df.columns)\n    if missing:\n        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n\n    missing_features = [c for c in feature_columns if c not in df.columns]\n    if missing_features:\n        raise ValueError(f\"Feature columns not found in DataFrame: {missing_features}\")\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.FeatureInformativenessAnalyzer.analyze","title":"analyze","text":"<pre><code>analyze(df: DataFrame, feature_columns: list[str]) -&gt; InformativenessReport\n</code></pre> <p>Run full informativeness analysis.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>OHLCV DataFrame with pre-computed feature columns.</p> required <code>feature_columns</code> <code>list[str]</code> <p>List of feature column names to evaluate.</p> required <p>Returns:</p> Type Description <code>InformativenessReport</code> <p>InformativenessReport with all results.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns are missing or feature_columns is empty.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def analyze(\n    self,\n    df: pl.DataFrame,\n    feature_columns: list[str],\n) -&gt; InformativenessReport:\n    \"\"\"Run full informativeness analysis.\n\n    Args:\n        df: OHLCV DataFrame with pre-computed feature columns.\n        feature_columns: List of feature column names to evaluate.\n\n    Returns:\n        InformativenessReport with all results.\n\n    Raises:\n        ValueError: If required columns are missing or feature_columns is empty.\n    \"\"\"\n    self._validate(df, feature_columns)\n\n    # 1. Generate targets\n    logger.info(\"Generating multi-horizon targets...\")\n    df = self.target_generator.generate(df)\n    target_meta = self.target_generator.target_columns()\n\n    # 2. Detect and mask global events\n    global_events = None\n    if self.event_detector is not None:\n        logger.info(\"Detecting global events...\")\n        # Convert DataFrame to RawDataView for SignalDetector\n        raw_view = _df_to_raw_data_view(df, self.pair_col, self.ts_col)\n        signals = self.event_detector.run(raw_view)\n        global_events = signals.value\n\n        # Get all target columns\n        target_columns = [meta[\"column\"] for meta in target_meta]\n\n        # Mask targets using the maximum horizon\n        max_horizon = max(h.horizon for h in self.target_generator.horizons)\n\n        df = mask_targets_by_signals(\n            df=df,\n            signals=signals,\n            mask_signal_types=self.event_detector.allowed_signal_types or set(),\n            horizon_bars=max_horizon,\n            cooldown_bars=60,\n            target_columns=target_columns,\n            pair_col=self.pair_col,\n            ts_col=self.ts_col,\n        )\n\n    # 3-4. Compute MI and rolling stability\n    logger.info(f\"Computing MI for {len(feature_columns)} features \u00d7 {len(target_meta)} targets...\")\n    mi_rows = self._compute_all_mi(df, feature_columns, target_meta)\n    raw_mi = pl.DataFrame(mi_rows)\n\n    # 5. Composite scoring\n    logger.info(\"Computing composite scores...\")\n    composite_scores = self._compute_composite(raw_mi)\n    score_matrix = self._build_score_matrix(raw_mi)\n\n    n_events = 0\n    if global_events is not None:\n        n_events = global_events.height\n\n    metadata = {\n        \"n_features\": len(feature_columns),\n        \"n_horizons\": len(self.target_generator.horizons),\n        \"n_target_types\": len(self.target_generator.target_types),\n        \"n_global_events\": n_events,\n        \"bins\": self.bins,\n        \"aggregate_pairs\": self.aggregate_pairs,\n    }\n\n    logger.info(\"Informativeness analysis complete.\")\n    return InformativenessReport(\n        raw_mi=raw_mi,\n        composite_scores=composite_scores,\n        score_matrix=score_matrix,\n        global_events=global_events,\n        metadata=metadata,\n    )\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport","title":"signalflow.feature.informativeness.InformativenessReport  <code>dataclass</code>","text":"<pre><code>InformativenessReport(raw_mi: DataFrame, composite_scores: DataFrame, score_matrix: DataFrame, global_events: DataFrame | None, metadata: dict)\n</code></pre> <p>Container for informativeness analysis results.</p> <p>Attributes:</p> Name Type Description <code>raw_mi</code> <code>DataFrame</code> <p>Full MI results (feature \u00d7 horizon \u00d7 target).</p> <code>composite_scores</code> <code>DataFrame</code> <p>Aggregated scores per feature, ranked.</p> <code>score_matrix</code> <code>DataFrame</code> <p>Pivoted Feature \u00d7 (Horizon, Target) matrix.</p> <code>global_events</code> <code>DataFrame | None</code> <p>Global event detection results (if enabled).</p> <code>metadata</code> <code>dict</code> <p>Analysis configuration and statistics.</p>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.composite_scores","title":"composite_scores  <code>instance-attribute</code>","text":"<pre><code>composite_scores: DataFrame\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.global_events","title":"global_events  <code>instance-attribute</code>","text":"<pre><code>global_events: DataFrame | None\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.metadata","title":"metadata  <code>instance-attribute</code>","text":"<pre><code>metadata: dict\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.raw_mi","title":"raw_mi  <code>instance-attribute</code>","text":"<pre><code>raw_mi: DataFrame\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.score_matrix","title":"score_matrix  <code>instance-attribute</code>","text":"<pre><code>score_matrix: DataFrame\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.bottom_features","title":"bottom_features","text":"<pre><code>bottom_features(n: int = 20) -&gt; pl.DataFrame\n</code></pre> <p>Return bottom N features (least informative).</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def bottom_features(self, n: int = 20) -&gt; pl.DataFrame:\n    \"\"\"Return bottom N features (least informative).\"\"\"\n    return self.composite_scores.tail(n)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.feature_detail","title":"feature_detail","text":"<pre><code>feature_detail(feature_name: str) -&gt; pl.DataFrame\n</code></pre> <p>Return detailed MI breakdown for a single feature.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def feature_detail(self, feature_name: str) -&gt; pl.DataFrame:\n    \"\"\"Return detailed MI breakdown for a single feature.\"\"\"\n    return self.raw_mi.filter(pl.col(\"feature\") == feature_name)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.InformativenessReport.top_features","title":"top_features","text":"<pre><code>top_features(n: int = 20) -&gt; pl.DataFrame\n</code></pre> <p>Return top N features by composite score.</p> Source code in <code>src/signalflow/feature/informativeness.py</code> <pre><code>def top_features(self, n: int = 20) -&gt; pl.DataFrame:\n    \"\"\"Return top N features by composite score.\"\"\"\n    return self.composite_scores.head(n)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.informativeness.RollingMIConfig","title":"signalflow.feature.informativeness.RollingMIConfig  <code>dataclass</code>","text":"<pre><code>RollingMIConfig(window_size: int = 5000, min_window_fill: float = 0.7)\n</code></pre> <p>Configuration for rolling MI stability computation.</p> <p>Attributes:</p> Name Type Description <code>window_size</code> <code>int</code> <p>Number of bars per rolling window.</p> <code>min_window_fill</code> <code>float</code> <p>Minimum fraction of non-null values in a window.</p>"},{"location":"api/feature/#signalflow.feature.informativeness.CompositeWeights","title":"signalflow.feature.informativeness.CompositeWeights  <code>dataclass</code>","text":"<pre><code>CompositeWeights(horizon_weights: dict[str, float] | None = None, target_weights: dict[str, float] | None = None, stability_weight: float = 0.3)\n</code></pre> <p>Weights for composite informativeness scoring.</p> <p>Attributes:</p> Name Type Description <code>horizon_weights</code> <code>dict[str, float] | None</code> <p>Per-horizon weights. None = equal weights.</p> <code>target_weights</code> <code>dict[str, float] | None</code> <p>Per-target weights. None = equal weights.</p> <code>stability_weight</code> <code>float</code> <p>Fraction of score from stability (rest from NMI).</p>"},{"location":"api/feature/#mutual-information-functions","title":"Mutual Information Functions","text":""},{"location":"api/feature/#signalflow.feature.mutual_information","title":"signalflow.feature.mutual_information","text":"<p>Mutual Information estimation for feature-target pairs.</p> <p>Provides histogram-based MI estimation for continuous and discrete variables. Used by FeatureInformativenessAnalyzer to measure feature informativeness against multiple target types.</p> References <ul> <li>Cover &amp; Thomas (2006) - Elements of Information Theory</li> <li>Kraskov et al. (2004) - MI estimation</li> </ul>"},{"location":"api/feature/#signalflow.feature.mutual_information._bin_continuous","title":"_bin_continuous","text":"<pre><code>_bin_continuous(x: ndarray, bins: int) -&gt; np.ndarray\n</code></pre> <p>Bin continuous values into integer bin indices.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def _bin_continuous(x: np.ndarray, bins: int) -&gt; np.ndarray:\n    \"\"\"Bin continuous values into integer bin indices.\"\"\"\n    _, edges = np.histogram(x, bins=bins)\n    return np.clip(np.digitize(x, edges[:-1]) - 1, 0, bins - 1)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information._isnan_any","title":"_isnan_any","text":"<pre><code>_isnan_any(arr: ndarray) -&gt; np.ndarray\n</code></pre> <p>Return boolean mask for NaN-like values in any dtype.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def _isnan_any(arr: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Return boolean mask for NaN-like values in any dtype.\"\"\"\n    if np.issubdtype(arr.dtype, np.floating):\n        return np.isnan(arr)\n    if arr.dtype == object:\n        return np.array([v is None or (isinstance(v, float) and np.isnan(v)) for v in arr])\n    return np.zeros(len(arr), dtype=bool)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information._mi_from_contingency","title":"_mi_from_contingency","text":"<pre><code>_mi_from_contingency(x: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>Compute MI from two discrete arrays via contingency table.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def _mi_from_contingency(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"Compute MI from two discrete arrays via contingency table.\"\"\"\n    x_vals, x_idx = np.unique(x, return_inverse=True)\n    y_vals, y_idx = np.unique(y, return_inverse=True)\n\n    contingency = np.zeros((len(x_vals), len(y_vals)), dtype=np.float64)\n    np.add.at(contingency, (x_idx, y_idx), 1)\n\n    pxy = contingency / contingency.sum()\n    px = pxy.sum(axis=1)\n    py = pxy.sum(axis=0)\n\n    outer = px[:, None] * py[None, :]\n    valid = (pxy &gt; 0) &amp; (outer &gt; 0)\n    mi = np.sum(pxy[valid] * np.log2(pxy[valid] / outer[valid]))\n    return max(mi, 0.0)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information.entropy_continuous","title":"entropy_continuous","text":"<pre><code>entropy_continuous(x: ndarray, bins: int = 20) -&gt; float\n</code></pre> <p>Shannon entropy via histogram of a continuous variable.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>1D array of continuous values.</p> required <code>bins</code> <code>int</code> <p>Number of histogram bins.</p> <code>20</code> <p>Returns:</p> Type Description <code>float</code> <p>Entropy in bits. NaN if fewer than 2 valid values.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def entropy_continuous(x: np.ndarray, bins: int = 20) -&gt; float:\n    \"\"\"Shannon entropy via histogram of a continuous variable.\n\n    Args:\n        x: 1D array of continuous values.\n        bins: Number of histogram bins.\n\n    Returns:\n        Entropy in bits. NaN if fewer than 2 valid values.\n    \"\"\"\n    x = x[np.isfinite(x)]\n    if len(x) &lt; 2:\n        return np.nan\n\n    counts, _ = np.histogram(x, bins=bins)\n    probs = counts / counts.sum()\n    probs = probs[probs &gt; 0]\n    return -np.sum(probs * np.log2(probs))\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information.entropy_discrete","title":"entropy_discrete","text":"<pre><code>entropy_discrete(x: ndarray) -&gt; float\n</code></pre> <p>Shannon entropy of a discrete distribution.</p> <p>H(X) = -sum_x p(x) * log2(p(x))</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>1D array of discrete values.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Entropy in bits. NaN if fewer than 2 values.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def entropy_discrete(x: np.ndarray) -&gt; float:\n    \"\"\"Shannon entropy of a discrete distribution.\n\n    H(X) = -sum_x p(x) * log2(p(x))\n\n    Args:\n        x: 1D array of discrete values.\n\n    Returns:\n        Entropy in bits. NaN if fewer than 2 values.\n    \"\"\"\n    x = x[~_isnan_any(x)]\n    if len(x) &lt; 2:\n        return np.nan\n\n    _, counts = np.unique(x, return_counts=True)\n    probs = counts / counts.sum()\n    probs = probs[probs &gt; 0]\n    return -np.sum(probs * np.log2(probs))\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information.mutual_information_continuous","title":"mutual_information_continuous","text":"<pre><code>mutual_information_continuous(x: ndarray, y: ndarray, bins: int = 20) -&gt; float\n</code></pre> <p>MI between two continuous variables.</p> <p>Bins both variables and computes MI from the 2D histogram.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>1D continuous array.</p> required <code>y</code> <code>ndarray</code> <p>1D continuous array.</p> required <code>bins</code> <code>int</code> <p>Number of bins per dimension.</p> <code>20</code> <p>Returns:</p> Type Description <code>float</code> <p>MI in bits. NaN if insufficient data.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def mutual_information_continuous(\n    x: np.ndarray,\n    y: np.ndarray,\n    bins: int = 20,\n) -&gt; float:\n    \"\"\"MI between two continuous variables.\n\n    Bins both variables and computes MI from the 2D histogram.\n\n    Args:\n        x: 1D continuous array.\n        y: 1D continuous array.\n        bins: Number of bins per dimension.\n\n    Returns:\n        MI in bits. NaN if insufficient data.\n    \"\"\"\n    mask = np.isfinite(x) &amp; np.isfinite(y)\n    x, y = x[mask], y[mask]\n    if len(x) &lt; 2:\n        return np.nan\n\n    hist_2d, _, _ = np.histogram2d(x, y, bins=bins)\n    pxy = hist_2d / hist_2d.sum()\n    px = pxy.sum(axis=1)\n    py = pxy.sum(axis=0)\n\n    outer = px[:, None] * py[None, :]\n    valid = (pxy &gt; 0) &amp; (outer &gt; 0)\n    mi = np.sum(pxy[valid] * np.log2(pxy[valid] / outer[valid]))\n    return max(mi, 0.0)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information.mutual_information_continuous_discrete","title":"mutual_information_continuous_discrete","text":"<pre><code>mutual_information_continuous_discrete(x: ndarray, y: ndarray, bins: int = 20) -&gt; float\n</code></pre> <p>MI between a continuous feature and a discrete target.</p> <p>Bins the continuous variable, then computes MI from the joint contingency table of (binned_x, y).</p> <p>This is the primary use case: continuous feature columns (RSI, SMA, etc.) against discrete labels (RISE/FALL/NONE).</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>1D continuous feature array.</p> required <code>y</code> <code>ndarray</code> <p>1D discrete target array.</p> required <code>bins</code> <code>int</code> <p>Number of bins for the continuous variable.</p> <code>20</code> <p>Returns:</p> Type Description <code>float</code> <p>MI in bits. NaN if insufficient data.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def mutual_information_continuous_discrete(\n    x: np.ndarray,\n    y: np.ndarray,\n    bins: int = 20,\n) -&gt; float:\n    \"\"\"MI between a continuous feature and a discrete target.\n\n    Bins the continuous variable, then computes MI from the\n    joint contingency table of (binned_x, y).\n\n    This is the primary use case: continuous feature columns\n    (RSI, SMA, etc.) against discrete labels (RISE/FALL/NONE).\n\n    Args:\n        x: 1D continuous feature array.\n        y: 1D discrete target array.\n        bins: Number of bins for the continuous variable.\n\n    Returns:\n        MI in bits. NaN if insufficient data.\n    \"\"\"\n    mask = np.isfinite(x) &amp; ~_isnan_any(y)\n    x, y = x[mask], y[mask]\n    if len(x) &lt; 2:\n        return np.nan\n\n    x_binned = _bin_continuous(x, bins)\n    return _mi_from_contingency(x_binned, y)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information.mutual_information_discrete","title":"mutual_information_discrete","text":"<pre><code>mutual_information_discrete(x: ndarray, y: ndarray) -&gt; float\n</code></pre> <p>MI between two discrete (categorical) arrays.</p> <p>MI(X;Y) = sum_{x,y} p(x,y) * log2(p(x,y) / (p(x) * p(y)))</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>1D discrete array.</p> required <code>y</code> <code>ndarray</code> <p>1D discrete array of same length.</p> required <p>Returns:</p> Type Description <code>float</code> <p>MI in bits. NaN if insufficient data.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def mutual_information_discrete(x: np.ndarray, y: np.ndarray) -&gt; float:\n    \"\"\"MI between two discrete (categorical) arrays.\n\n    MI(X;Y) = sum_{x,y} p(x,y) * log2(p(x,y) / (p(x) * p(y)))\n\n    Args:\n        x: 1D discrete array.\n        y: 1D discrete array of same length.\n\n    Returns:\n        MI in bits. NaN if insufficient data.\n    \"\"\"\n    mask = ~(_isnan_any(x) | _isnan_any(y))\n    x, y = x[mask], y[mask]\n    if len(x) &lt; 2:\n        return np.nan\n\n    return _mi_from_contingency(x, y)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.mutual_information.normalized_mutual_information","title":"normalized_mutual_information","text":"<pre><code>normalized_mutual_information(mi: float, h_x: float, h_y: float) -&gt; float\n</code></pre> <p>Normalize MI to [0, 1] using NMI = MI / sqrt(H(X) * H(Y)).</p> <p>Parameters:</p> Name Type Description Default <code>mi</code> <code>float</code> <p>Raw mutual information value.</p> required <code>h_x</code> <code>float</code> <p>Entropy of X.</p> required <code>h_y</code> <code>float</code> <p>Entropy of Y.</p> required <p>Returns:</p> Type Description <code>float</code> <p>NMI in [0, 1]. NaN if either entropy is zero or NaN.</p> Source code in <code>src/signalflow/feature/mutual_information.py</code> <pre><code>def normalized_mutual_information(mi: float, h_x: float, h_y: float) -&gt; float:\n    \"\"\"Normalize MI to [0, 1] using NMI = MI / sqrt(H(X) * H(Y)).\n\n    Args:\n        mi: Raw mutual information value.\n        h_x: Entropy of X.\n        h_y: Entropy of Y.\n\n    Returns:\n        NMI in [0, 1]. NaN if either entropy is zero or NaN.\n    \"\"\"\n    if np.isnan(mi) or np.isnan(h_x) or np.isnan(h_y):\n        return np.nan\n    denom = np.sqrt(h_x * h_y)\n    if denom &lt;= 0:\n        return np.nan\n    return min(mi / denom, 1.0)\n</code></pre>"},{"location":"api/labeler/","title":"Target Module","text":"<p>Signal labeling strategies for machine learning training. These classes generate look-ahead labels (direction, return magnitude, volume regime) at various horizons.</p> <p>Module Name</p> <p>The target functionality is implemented in the <code>signalflow.target</code> module.</p> <p>Event Detection</p> <p>Market-wide event detectors are in the <code>detector.market</code> module. Use <code>mask_targets_by_signals()</code> to exclude labels around detected events.</p>"},{"location":"api/labeler/#base-class","title":"Base Class","text":""},{"location":"api/labeler/#signalflow.target.base.Labeler","title":"signalflow.target.base.Labeler  <code>dataclass</code>","text":"<pre><code>Labeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.PRICE_DIRECTION, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t_hit', 'ret'))\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for Polars-only signal labeling.</p> <p>Assigns forward-looking labels to historical data based on future price movement. Labels are computed per-pair with length-preserving operations.</p> Key concepts <ul> <li>Forward-looking: Labels depend on future data (not available in live trading)</li> <li>Per-pair processing: Each pair labeled independently</li> <li>Length-preserving: Output has same row count as input</li> <li>Signal masking: Optionally label only at signal timestamps</li> </ul> Public API <ul> <li>compute(): Main entry point (handles grouping, filtering, projection)</li> <li>compute_group(): Per-pair labeling logic (must implement)</li> </ul> Common labeling strategies <ul> <li>Fixed horizon: Label based on return over N bars</li> <li>Triple barrier: Label based on first hit of profit/loss/time barrier</li> <li>Quantile-based: Label based on return quantiles</li> </ul> <p>Attributes:</p> Name Type Description <code>component_type</code> <code>ClassVar[SfComponentType]</code> <p>Always LABELER for registry.</p> <code>raw_data_type</code> <code>RawDataType</code> <p>Type of raw data. Default: SPOT.</p> <code>pair_col</code> <code>str</code> <p>Trading pair column. Default: \"pair\".</p> <code>ts_col</code> <code>str</code> <p>Timestamp column. Default: \"timestamp\".</p> <code>keep_input_columns</code> <code>bool</code> <p>Keep all input columns. Default: False.</p> <code>output_columns</code> <code>list[str] | None</code> <p>Specific columns to output. Default: None.</p> <code>filter_signal_type</code> <code>SignalType | None</code> <p>Filter to specific signal type. Default: None.</p> <code>mask_to_signals</code> <code>bool</code> <p>Mask labels to signal timestamps only. Default: True.</p> <code>out_col</code> <code>str</code> <p>Output label column name. Default: \"label\".</p> <code>include_meta</code> <code>bool</code> <p>Include metadata columns. Default: False.</p> <code>meta_columns</code> <code>tuple[str, ...]</code> <p>Metadata column names. Default: (\"t_hit\", \"ret\").</p> Example <pre><code>from signalflow.target import Labeler\nfrom signalflow.core import SignalType\nimport polars as pl\n\nclass FixedHorizonLabeler(Labeler):\n    '''Label based on fixed-horizon return'''\n\n    def __init__(self, horizon: int = 10, threshold: float = 0.01):\n        super().__init__()\n        self.horizon = horizon\n        self.threshold = threshold\n\n    def compute_group(self, group_df, data_context=None):\n        # Compute forward return\n        labels = group_df.with_columns([\n            pl.col(\"close\").shift(-self.horizon).alias(\"future_close\")\n        ]).with_columns([\n            ((pl.col(\"future_close\") / pl.col(\"close\")) - 1).alias(\"return\")\n        ]).with_columns([\n            pl.when(pl.col(\"return\") &gt; self.threshold)\n            .then(pl.lit(SignalType.RISE.value))\n            .when(pl.col(\"return\") &lt; -self.threshold)\n            .then(pl.lit(SignalType.FALL.value))\n            .otherwise(pl.lit(SignalType.NONE.value))\n            .alias(\"label\")\n        ])\n\n        return labels\n\n# Usage\nlabeler = FixedHorizonLabeler(horizon=10, threshold=0.01)\nlabeled = labeler.compute(ohlcv_df, signals=signals)\n</code></pre> Note <p>compute_group() must preserve row count (no filtering). All timestamps must be timezone-naive. Signal masking requires mask_to_signals=True and signal_keys in context.</p> See Also <p>FixedHorizonLabeler: Simple fixed-horizon implementation. TripleBarrierLabeler: Three-barrier labeling strategy.</p>"},{"location":"api/labeler/#signalflow.target.base.Labeler.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = LABELER\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.filter_signal_type","title":"filter_signal_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filter_signal_type: SignalType | None = None\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.include_meta","title":"include_meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_meta: bool = False\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.keep_input_columns","title":"keep_input_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keep_input_columns: bool = False\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.mask_to_signals","title":"mask_to_signals  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_to_signals: bool = True\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('t_hit', 'ret')\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.out_col","title":"out_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>out_col: str = 'label'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.output_columns","title":"output_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_columns: list[str] | None = None\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.raw_data_type","title":"raw_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_type: RawDataType | str = SPOT\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = PRICE_DIRECTION\n</code></pre> <p>Signal category this labeler produces. Default: PRICE_DIRECTION.</p>"},{"location":"api/labeler/#signalflow.target.base.Labeler.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._apply_signal_mask","title":"_apply_signal_mask","text":"<pre><code>_apply_signal_mask(df: DataFrame, data_context: dict[str, Any], group_df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Mask labels to signal timestamps only.</p> <p>Labels are computed for all rows, but only signal timestamps get actual labels; others are set to SignalType.NONE.</p> <p>Used for meta-labeling: only label at detected signal points, not every bar.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with computed labels.</p> required <code>data_context</code> <code>dict[str, Any]</code> <p>Must contain \"signal_keys\" DataFrame.</p> required <code>group_df</code> <code>DataFrame</code> <p>Original group data for extracting pair value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame with masked labels.</p> Example <pre><code># In compute_group with masking\ndef compute_group(self, group_df, data_context=None):\n    # Compute labels for all rows\n    labeled = group_df.with_columns([...])\n\n    # Mask to signal timestamps only\n    if self.mask_to_signals and data_context:\n        labeled = self._apply_signal_mask(\n            labeled, data_context, group_df\n        )\n\n    return labeled\n</code></pre> Note <p>Requires signal_keys in data_context with (pair, timestamp) columns. Non-signal rows get label=SignalType.NONE. Metadata columns also masked if include_meta=True.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _apply_signal_mask(\n    self,\n    df: pl.DataFrame,\n    data_context: dict[str, Any],\n    group_df: pl.DataFrame,\n) -&gt; pl.DataFrame:\n    \"\"\"Mask labels to signal timestamps only.\n\n    Labels are computed for all rows, but only signal timestamps\n    get actual labels; others are set to SignalType.NONE.\n\n    Used for meta-labeling: only label at detected signal points,\n    not every bar.\n\n    Args:\n        df (pl.DataFrame): DataFrame with computed labels.\n        data_context (dict[str, Any]): Must contain \"signal_keys\" DataFrame.\n        group_df (pl.DataFrame): Original group data for extracting pair value.\n\n    Returns:\n        pl.DataFrame: DataFrame with masked labels.\n\n    Example:\n        ```python\n        # In compute_group with masking\n        def compute_group(self, group_df, data_context=None):\n            # Compute labels for all rows\n            labeled = group_df.with_columns([...])\n\n            # Mask to signal timestamps only\n            if self.mask_to_signals and data_context:\n                labeled = self._apply_signal_mask(\n                    labeled, data_context, group_df\n                )\n\n            return labeled\n        ```\n\n    Note:\n        Requires signal_keys in data_context with (pair, timestamp) columns.\n        Non-signal rows get label=SignalType.NONE.\n        Metadata columns also masked if include_meta=True.\n    \"\"\"\n    signal_keys: pl.DataFrame = data_context[\"signal_keys\"]\n    pair_value = group_df.get_column(self.pair_col)[0]\n\n    signal_ts = signal_keys.filter(pl.col(self.pair_col) == pair_value).select(self.ts_col).unique()\n\n    if signal_ts.height == 0:\n        df = df.with_columns(pl.lit(None, dtype=pl.Utf8).alias(self.out_col))\n        if self.include_meta:\n            df = df.with_columns([pl.lit(None).alias(col) for col in self.meta_columns])\n    else:\n        is_signal = pl.col(\"_is_signal\").fill_null(False)\n        mask_exprs = [\n            pl.when(is_signal)\n            .then(pl.col(self.out_col))\n            .otherwise(pl.lit(None, dtype=pl.Utf8))\n            .alias(self.out_col),\n        ]\n        if self.include_meta:\n            mask_exprs += [\n                pl.when(is_signal).then(pl.col(col)).otherwise(pl.lit(None)).alias(col) for col in self.meta_columns\n            ]\n\n        df = (\n            df.join(\n                signal_ts.with_columns(pl.lit(True).alias(\"_is_signal\")),\n                on=self.ts_col,\n                how=\"left\",\n            )\n            .with_columns(mask_exprs)\n            .drop(\"_is_signal\")\n        )\n\n    return df\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._compute_pl","title":"_compute_pl","text":"<pre><code>_compute_pl(df: DataFrame, signals: Signals | None, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> <p>Internal Polars-based computation.</p> <p>Orchestrates validation, filtering, grouping, and projection.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data.</p> required <code>signals</code> <code>Signals | None</code> <p>Optional signals.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Optional context.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Labeled data.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _compute_pl(\n    self,\n    df: pl.DataFrame,\n    signals: Signals | None,\n    data_context: dict[str, Any] | None,\n) -&gt; pl.DataFrame:\n    \"\"\"Internal Polars-based computation.\n\n    Orchestrates validation, filtering, grouping, and projection.\n\n    Args:\n        df (pl.DataFrame): Input data.\n        signals (Signals | None): Optional signals.\n        data_context (dict[str, Any] | None): Optional context.\n\n    Returns:\n        pl.DataFrame: Labeled data.\n    \"\"\"\n    self._validate_input_pl(df)\n    df0 = df.sort([self.pair_col, self.ts_col])\n\n    if signals is not None and self.filter_signal_type is not None:\n        s_pl = self._signals_to_pl(signals)\n        df0 = self._filter_by_signals_pl(df0, s_pl, self.filter_signal_type)\n\n    input_cols = set(df0.columns)\n\n    def _wrapped(g: pl.DataFrame) -&gt; pl.DataFrame:\n        out = self.compute_group(g, data_context=data_context)\n        if not isinstance(out, pl.DataFrame):\n            raise TypeError(f\"{self.__class__.__name__}.compute_group must return pl.DataFrame\")\n        if out.height != g.height:\n            raise ValueError(\n                f\"{self.__class__.__name__}: len(output_group)={out.height} != len(input_group)={g.height}\"\n            )\n        return out\n\n    out = df0.group_by(self.pair_col, maintain_order=True).map_groups(_wrapped).sort([self.pair_col, self.ts_col])\n\n    if self.keep_input_columns:\n        return out\n\n    label_cols = sorted(set(out.columns) - input_cols) if self.output_columns is None else list(self.output_columns)\n\n    keep_cols = [self.pair_col, self.ts_col] + label_cols\n    missing = [c for c in keep_cols if c not in out.columns]\n    if missing:\n        raise ValueError(f\"Projection error, missing columns: {missing}\")\n\n    return out.select(keep_cols)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._filter_by_signals_pl","title":"_filter_by_signals_pl","text":"<pre><code>_filter_by_signals_pl(df: DataFrame, s: DataFrame, signal_type: SignalType) -&gt; pl.DataFrame\n</code></pre> <p>Filter input to rows matching signal timestamps.</p> <p>Inner join with signal timestamps of specific type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data.</p> required <code>s</code> <code>DataFrame</code> <p>Signals DataFrame.</p> required <code>signal_type</code> <code>SignalType</code> <p>Signal type to filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered data (only rows at signal timestamps).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If signals missing required columns.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _filter_by_signals_pl(self, df: pl.DataFrame, s: pl.DataFrame, signal_type: SignalType) -&gt; pl.DataFrame:\n    \"\"\"Filter input to rows matching signal timestamps.\n\n    Inner join with signal timestamps of specific type.\n\n    Args:\n        df (pl.DataFrame): Input data.\n        s (pl.DataFrame): Signals DataFrame.\n        signal_type (SignalType): Signal type to filter.\n\n    Returns:\n        pl.DataFrame: Filtered data (only rows at signal timestamps).\n\n    Raises:\n        ValueError: If signals missing required columns.\n    \"\"\"\n    required = {self.pair_col, self.ts_col, \"signal_type\"}\n    missing = required - set(s.columns)\n    if missing:\n        raise ValueError(f\"Signals missing columns: {sorted(missing)}\")\n\n    s_f = (\n        s.filter(pl.col(\"signal_type\") == signal_type.value)\n        .select([self.pair_col, self.ts_col])\n        .unique(subset=[self.pair_col, self.ts_col])\n    )\n    return df.join(s_f, on=[self.pair_col, self.ts_col], how=\"inner\")\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._signals_to_pl","title":"_signals_to_pl","text":"<pre><code>_signals_to_pl(signals: Signals) -&gt; pl.DataFrame\n</code></pre> <p>Convert Signals to Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Signals container.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Signals as DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If Signals.value is not pl.DataFrame.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _signals_to_pl(self, signals: Signals) -&gt; pl.DataFrame:\n    \"\"\"Convert Signals to Polars DataFrame.\n\n    Args:\n        signals (Signals): Signals container.\n\n    Returns:\n        pl.DataFrame: Signals as DataFrame.\n\n    Raises:\n        TypeError: If Signals.value is not pl.DataFrame.\n    \"\"\"\n    s = signals.value\n    if isinstance(s, pl.DataFrame):\n        return s\n    raise TypeError(f\"Unsupported Signals.value type: {type(s)}\")\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._validate_input_pl","title":"_validate_input_pl","text":"<pre><code>_validate_input_pl(df: DataFrame) -&gt; None\n</code></pre> <p>Validate input DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns missing.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _validate_input_pl(self, df: pl.DataFrame) -&gt; None:\n    \"\"\"Validate input DataFrame schema.\n\n    Args:\n        df (pl.DataFrame): Input to validate.\n\n    Raises:\n        ValueError: If required columns missing.\n    \"\"\"\n    missing = [c for c in (self.pair_col, self.ts_col) if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing required columns: {missing}\")\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, signals: Signals | None = None, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute labels for input DataFrame.</p> <p>Main entry point - handles validation, filtering, grouping, and projection.</p> Processing steps <ol> <li>Validate input schema</li> <li>Sort by (pair, timestamp)</li> <li>(optional) Filter to specific signal type</li> <li>Group by pair and apply compute_group()</li> <li>Validate output (length-preserving)</li> <li>Project to output columns</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data with OHLCV and required columns.</p> required <code>signals</code> <code>Signals | None</code> <p>Signals for filtering/masking.</p> <code>None</code> <code>data_context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Labeled data with columns: - pair, timestamp (always included) - label column(s) (as specified by out_col) - (optional) metadata columns</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df not pl.DataFrame or compute_group returns wrong type.</p> <code>ValueError</code> <p>If compute_group changes row count or columns missing.</p> Example <pre><code># Basic labeling\nlabeled = labeler.compute(ohlcv_df)\n\n# With signal filtering\nlabeled = labeler.compute(\n    ohlcv_df,\n    signals=signals,\n    filter_signal_type=SignalType.RISE\n)\n\n# With masking context\nlabeled = labeler.compute(\n    ohlcv_df,\n    signals=signals,\n    data_context={\"signal_keys\": signal_timestamps_df}\n)\n</code></pre> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def compute(\n    self,\n    df: pl.DataFrame,\n    signals: Signals | None = None,\n    data_context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Compute labels for input DataFrame.\n\n    Main entry point - handles validation, filtering, grouping, and projection.\n\n    Processing steps:\n        1. Validate input schema\n        2. Sort by (pair, timestamp)\n        3. (optional) Filter to specific signal type\n        4. Group by pair and apply compute_group()\n        5. Validate output (length-preserving)\n        6. Project to output columns\n\n    Args:\n        df (pl.DataFrame): Input data with OHLCV and required columns.\n        signals (Signals | None): Signals for filtering/masking.\n        data_context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Labeled data with columns:\n            - pair, timestamp (always included)\n            - label column(s) (as specified by out_col)\n            - (optional) metadata columns\n\n    Raises:\n        TypeError: If df not pl.DataFrame or compute_group returns wrong type.\n        ValueError: If compute_group changes row count or columns missing.\n\n    Example:\n        ```python\n        # Basic labeling\n        labeled = labeler.compute(ohlcv_df)\n\n        # With signal filtering\n        labeled = labeler.compute(\n            ohlcv_df,\n            signals=signals,\n            filter_signal_type=SignalType.RISE\n        )\n\n        # With masking context\n        labeled = labeler.compute(\n            ohlcv_df,\n            signals=signals,\n            data_context={\"signal_keys\": signal_timestamps_df}\n        )\n        ```\n    \"\"\"\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"{self.__class__.__name__}.compute expects pl.DataFrame, got {type(df)}\")\n    return self._compute_pl(df=df, signals=signals, data_context=data_context)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.compute_group","title":"compute_group  <code>abstractmethod</code>","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> <p>Compute labels for single pair group.</p> <p>Core labeling logic - must be implemented by subclasses.</p> <p>CRITICAL: Must preserve row count (len(output) == len(input)). No filtering allowed inside compute_group.</p> <p>Parameters:</p> Name Type Description Default <code>group_df</code> <code>DataFrame</code> <p>Single pair's data, sorted by timestamp.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Same length as input with added label columns.</p> Example <pre><code>def compute_group(self, group_df, data_context=None):\n    # Compute 10-bar forward return\n    return group_df.with_columns([\n        pl.col(\"close\").shift(-10).alias(\"future_close\")\n    ]).with_columns([\n        ((pl.col(\"future_close\") / pl.col(\"close\")) - 1).alias(\"return\"),\n        pl.when((pl.col(\"future_close\") / pl.col(\"close\") - 1) &gt; 0.01)\n        .then(pl.lit(SignalType.RISE.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n        .alias(\"label\")\n    ])\n</code></pre> Note <p>Output must have same height as input (length-preserving). Use shift(-n) for forward-looking operations. Last N bars will have null labels (no future data).</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>@abstractmethod\ndef compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame:\n    \"\"\"Compute labels for single pair group.\n\n    Core labeling logic - must be implemented by subclasses.\n\n    CRITICAL: Must preserve row count (len(output) == len(input)).\n    No filtering allowed inside compute_group.\n\n    Args:\n        group_df (pl.DataFrame): Single pair's data, sorted by timestamp.\n        data_context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Same length as input with added label columns.\n\n    Example:\n        ```python\n        def compute_group(self, group_df, data_context=None):\n            # Compute 10-bar forward return\n            return group_df.with_columns([\n                pl.col(\"close\").shift(-10).alias(\"future_close\")\n            ]).with_columns([\n                ((pl.col(\"future_close\") / pl.col(\"close\")) - 1).alias(\"return\"),\n                pl.when((pl.col(\"future_close\") / pl.col(\"close\") - 1) &gt; 0.01)\n                .then(pl.lit(SignalType.RISE.value))\n                .otherwise(pl.lit(SignalType.NONE.value))\n                .alias(\"label\")\n            ])\n        ```\n\n    Note:\n        Output must have same height as input (length-preserving).\n        Use shift(-n) for forward-looking operations.\n        Last N bars will have null labels (no future data).\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/labeler/#labeling-strategies","title":"Labeling Strategies","text":""},{"location":"api/labeler/#fixed-horizon","title":"Fixed Horizon","text":""},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler","title":"signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler  <code>dataclass</code>","text":"<pre><code>FixedHorizonLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.PRICE_DIRECTION, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t1', 'ret'), price_col: str = 'close', horizon: int = 60)\n</code></pre> <p>               Bases: <code>Labeler</code></p> Fixed-Horizon Labeling <p>label[t0] = sign(close[t0 + horizon] - close[t0])</p> <p>If signals provided, labels are written only on signal rows, while horizon is computed on full series (per pair).</p>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.horizon","title":"horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizon: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('t1', 'ret')\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/fixed_horizon_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.horizon &lt;= 0:\n        raise ValueError(\"horizon must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/fixed_horizon_labeler.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame:\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    if group_df.height == 0:\n        return group_df\n\n    h = int(self.horizon)\n    price = pl.col(self.price_col)\n    future_price = price.shift(-h)\n\n    df = group_df.with_columns(future_price.alias(\"_future_price\"))\n\n    label_expr = (\n        pl.when(\n            pl.col(\"_future_price\").is_null()\n            | pl.col(self.price_col).is_null()\n            | (pl.col(self.price_col) &lt;= 0)\n            | (pl.col(\"_future_price\") &lt;= 0)\n        )\n        .then(pl.lit(SignalType.NONE.value))\n        .when(pl.col(\"_future_price\") &gt; pl.col(self.price_col))\n        .then(pl.lit(SignalType.RISE.value))\n        .when(pl.col(\"_future_price\") &lt; pl.col(self.price_col))\n        .then(pl.lit(SignalType.FALL.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n    )\n\n    df = df.with_columns(label_expr.alias(self.out_col))\n\n    if self.include_meta:\n        df = df.with_columns(\n            [\n                pl.col(self.ts_col).shift(-h).alias(\"t1\"),\n                pl.when(\n                    pl.col(\"_future_price\").is_not_null()\n                    &amp; (pl.col(self.price_col) &gt; 0)\n                    &amp; (pl.col(\"_future_price\") &gt; 0)\n                )\n                .then((pl.col(\"_future_price\") / pl.col(self.price_col)).log())\n                .otherwise(pl.lit(None))\n                .alias(\"ret\"),\n            ]\n        )\n\n    df = df.drop(\"_future_price\")\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#triple-barrier-dynamic","title":"Triple Barrier (Dynamic)","text":""},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler","title":"signalflow.target.triple_barrier_labeler.TripleBarrierLabeler  <code>dataclass</code>","text":"<pre><code>TripleBarrierLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.PRICE_DIRECTION, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t_hit', 'ret'), price_col: str = 'close', vol_window: int = 60, horizon: int = 1440, profit_multiplier: float = 1.0, stop_loss_multiplier: float = 1.0)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Triple-Barrier Labeling (De Prado), Numba-accelerated.</p> Volatility-based barriers <ul> <li>pt = close * exp(vol * profit_multiplier)</li> <li>sl = close * exp(-vol * stop_loss_multiplier)</li> </ul>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler.horizon","title":"horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizon: int = 1440\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler.profit_multiplier","title":"profit_multiplier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>profit_multiplier: float = 1.0\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler.stop_loss_multiplier","title":"stop_loss_multiplier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop_loss_multiplier: float = 1.0\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler.vol_window","title":"vol_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_window: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/triple_barrier_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.vol_window &lt;= 1:\n        raise ValueError(\"vol_window must be &gt; 1\")\n    if self.horizon &lt;= 0:\n        raise ValueError(\"horizon must be &gt; 0\")\n    if self.profit_multiplier &lt;= 0 or self.stop_loss_multiplier &lt;= 0:\n        raise ValueError(\"profit_multiplier/stop_loss_multiplier must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler._apply_labels","title":"_apply_labels","text":"<pre><code>_apply_labels(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Apply RISE/FALL/NONE labels based on barrier hits.</p> Source code in <code>src/signalflow/target/triple_barrier_labeler.py</code> <pre><code>def _apply_labels(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Apply RISE/FALL/NONE labels based on barrier hits.\"\"\"\n    choose_up = pl.col(\"_up_off\").is_not_null() &amp; (\n        pl.col(\"_dn_off\").is_null() | (pl.col(\"_up_off\") &lt;= pl.col(\"_dn_off\"))\n    )\n    choose_dn = pl.col(\"_dn_off\").is_not_null() &amp; (\n        pl.col(\"_up_off\").is_null() | (pl.col(\"_dn_off\") &lt; pl.col(\"_up_off\"))\n    )\n\n    return df.with_columns(\n        pl.when(choose_up)\n        .then(pl.lit(SignalType.RISE.value))\n        .when(choose_dn)\n        .then(pl.lit(SignalType.FALL.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n        .alias(self.out_col)\n    )\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler._compute_meta","title":"_compute_meta","text":"<pre><code>_compute_meta(df: DataFrame, prices: ndarray, up_off_series: Series, dn_off_series: Series, lf: int) -&gt; pl.DataFrame\n</code></pre> <p>Compute t_hit and ret meta columns.</p> Source code in <code>src/signalflow/target/triple_barrier_labeler.py</code> <pre><code>def _compute_meta(\n    self,\n    df: pl.DataFrame,\n    prices: np.ndarray,\n    up_off_series: pl.Series,\n    dn_off_series: pl.Series,\n    lf: int,\n) -&gt; pl.DataFrame:\n    \"\"\"Compute t_hit and ret meta columns.\"\"\"\n    n = df.height\n    ts_arr = df.get_column(self.ts_col).to_numpy()\n\n    idx = np.arange(n)\n    up_np = up_off_series.fill_null(0).to_numpy()\n    dn_np = dn_off_series.fill_null(0).to_numpy()\n\n    hit_off = np.where(\n        (up_np &gt; 0) &amp; ((dn_np == 0) | (up_np &lt;= dn_np)),\n        up_np,\n        np.where(dn_np &gt; 0, dn_np, 0),\n    )\n\n    hit_idx = np.clip(idx + hit_off, 0, n - 1)\n    vert_idx = np.clip(idx + lf, 0, n - 1)\n    final_idx = np.where(hit_off &gt; 0, hit_idx, vert_idx)\n\n    t_hit = ts_arr[final_idx]\n    ret = np.where(prices &gt; 0, np.log(prices[final_idx] / prices), np.nan)\n\n    return df.with_columns(\n        [\n            pl.Series(\"t_hit\", t_hit),\n            pl.Series(\"ret\", ret),\n        ]\n    )\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier_labeler.TripleBarrierLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/triple_barrier_labeler.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame:\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    if group_df.height == 0:\n        return group_df\n\n    lf = int(self.horizon)\n    vw = int(self.vol_window)\n\n    df = group_df.with_columns(\n        (pl.col(self.price_col) / pl.col(self.price_col).shift(1))\n        .log()\n        .rolling_std(window_size=vw, ddof=1)\n        .alias(\"_vol\")\n    ).with_columns(\n        [\n            (pl.col(self.price_col) * (pl.col(\"_vol\") * self.profit_multiplier).exp()).alias(\"_pt\"),\n            (pl.col(self.price_col) * (-pl.col(\"_vol\") * self.stop_loss_multiplier).exp()).alias(\"_sl\"),\n        ]\n    )\n\n    prices = df.get_column(self.price_col).to_numpy().astype(np.float64)\n    pt = df.get_column(\"_pt\").fill_null(np.nan).to_numpy().astype(np.float64)\n    sl = df.get_column(\"_sl\").fill_null(np.nan).to_numpy().astype(np.float64)\n\n    up_off, dn_off = _find_first_hit(prices, pt, sl, lf)\n\n    up_off_series = pl.Series(\"_up_off\", up_off).replace(0, None).cast(pl.Int32)\n    dn_off_series = pl.Series(\"_dn_off\", dn_off).replace(0, None).cast(pl.Int32)\n\n    df = df.with_columns([up_off_series, dn_off_series])\n\n    df = self._apply_labels(df)\n\n    if self.include_meta:\n        df = self._compute_meta(df, prices, up_off_series, dn_off_series, lf)\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    drop_cols = [\"_vol\", \"_pt\", \"_sl\", \"_up_off\", \"_dn_off\"]\n    df = df.drop([c for c in drop_cols if c in df.columns])\n\n    return df\n</code></pre>"},{"location":"api/labeler/#take-profit-symmetric-barrier","title":"Take Profit (Symmetric Barrier)","text":""},{"location":"api/labeler/#signalflow.target.take_profit_labeler.TakeProfitLabeler","title":"signalflow.target.take_profit_labeler.TakeProfitLabeler  <code>dataclass</code>","text":"<pre><code>TakeProfitLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.PRICE_DIRECTION, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t_hit', 'ret'), price_col: str = 'close', horizon: int = 1440, barrier_pct: float = 0.01)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>First-touch labeling with symmetric fixed-percentage barriers.</p> Barriers <ul> <li>TP = close[t0] * (1 + barrier_pct)</li> <li>SL = close[t0] * (1 - barrier_pct)</li> <li>Vertical barrier at t0 + horizon</li> </ul> Label by first touch <ul> <li>RISE if TP touched first (ties -&gt; TP)</li> <li>FALL if SL touched first</li> <li>NONE if neither touched within horizon</li> </ul>"},{"location":"api/labeler/#signalflow.target.take_profit_labeler.TakeProfitLabeler.barrier_pct","title":"barrier_pct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>barrier_pct: float = 0.01\n</code></pre>"},{"location":"api/labeler/#signalflow.target.take_profit_labeler.TakeProfitLabeler.horizon","title":"horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizon: int = 1440\n</code></pre>"},{"location":"api/labeler/#signalflow.target.take_profit_labeler.TakeProfitLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.take_profit_labeler.TakeProfitLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/take_profit_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.horizon &lt;= 0:\n        raise ValueError(\"horizon must be &gt; 0\")\n    if self.barrier_pct &lt;= 0:\n        raise ValueError(\"barrier_pct must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.take_profit_labeler.TakeProfitLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/take_profit_labeler.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame:\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    if group_df.height == 0:\n        return group_df\n\n    lf = int(self.horizon)\n    n = group_df.height\n\n    prices = group_df.get_column(self.price_col).to_numpy().astype(np.float64)\n    pt = prices * (1.0 + self.barrier_pct)\n    sl = prices * (1.0 - self.barrier_pct)\n\n    up_off, dn_off = _find_first_hit_static(prices, pt, sl, lf)\n\n    up_off_series = pl.Series(\"_up_off\", up_off).replace(0, None).cast(pl.Int32)\n    dn_off_series = pl.Series(\"_dn_off\", dn_off).replace(0, None).cast(pl.Int32)\n\n    df = group_df.with_columns([up_off_series, dn_off_series])\n\n    choose_up = pl.col(\"_up_off\").is_not_null() &amp; (\n        pl.col(\"_dn_off\").is_null() | (pl.col(\"_up_off\") &lt;= pl.col(\"_dn_off\"))\n    )\n    choose_dn = pl.col(\"_dn_off\").is_not_null() &amp; (\n        pl.col(\"_up_off\").is_null() | (pl.col(\"_dn_off\") &lt; pl.col(\"_up_off\"))\n    )\n\n    df = df.with_columns(\n        pl.when(choose_up)\n        .then(pl.lit(SignalType.RISE.value))\n        .when(choose_dn)\n        .then(pl.lit(SignalType.FALL.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n        .alias(self.out_col)\n    )\n\n    if self.include_meta:\n        ts_arr = group_df.get_column(self.ts_col).to_numpy()\n\n        up_np = up_off_series.fill_null(0).to_numpy()\n        dn_np = dn_off_series.fill_null(0).to_numpy()\n        idx = np.arange(n)\n\n        hit_off = np.where(\n            (up_np &gt; 0) &amp; ((dn_np == 0) | (up_np &lt;= dn_np)),\n            up_np,\n            np.where(dn_np &gt; 0, dn_np, 0),\n        )\n\n        hit_idx = np.clip(idx + hit_off, 0, n - 1)\n        vert_idx = np.clip(idx + lf, 0, n - 1)\n        final_idx = np.where(hit_off &gt; 0, hit_idx, vert_idx)\n\n        t_hit = ts_arr[final_idx]\n        ret = np.log(prices[final_idx] / prices)\n\n        df = df.with_columns(\n            [\n                pl.Series(\"t_hit\", t_hit),\n                pl.Series(\"ret\", ret),\n            ]\n        )\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    df = df.drop([\"_up_off\", \"_dn_off\"])\n\n    return df\n</code></pre>"},{"location":"api/labeler/#non-price-direction-labelers","title":"Non-Price-Direction Labelers","text":""},{"location":"api/labeler/#anomaly-labeler","title":"Anomaly Labeler","text":""},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler","title":"signalflow.target.anomaly_labeler.AnomalyLabeler  <code>dataclass</code>","text":"<pre><code>AnomalyLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.ANOMALY, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('forward_ret', 'vol'), price_col: str = 'close', horizon: int = 60, vol_window: int = 1440, threshold_return_std: float = 4.0, flash_horizon: int = 10)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Labels black swan and flash crash events in historical data.</p> <p>Forward-looking labeler that identifies anomalous price movements by comparing forward return magnitude against rolling volatility.</p> Algorithm <ol> <li>Compute log returns: log(close[t] / close[t-1])</li> <li>Compute rolling std of returns over <code>vol_window</code> bars</li> <li>Compute forward return magnitude: |log(close[t+horizon] / close[t])|</li> <li>If forward return &gt; threshold_return_std * rolling_std -&gt; \"extreme_positive_anomaly\"</li> <li>If additionally the return is negative AND happened in &lt; flash_horizon    bars -&gt; \"extreme_negative_anomaly\"</li> <li>Otherwise -&gt; null (no label)</li> </ol> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column name. Default: \"close\".</p> <code>horizon</code> <code>int</code> <p>Forward-looking horizon in bars. Default: 60.</p> <code>vol_window</code> <code>int</code> <p>Rolling window for volatility estimation. Default: 1440.</p> <code>threshold_return_std</code> <code>float</code> <p>Number of standard deviations for anomaly threshold. Default: 4.0.</p> <code>flash_horizon</code> <code>int</code> <p>Maximum bars for flash crash classification. Default: 10.</p> Example <pre><code>from signalflow.target.anomaly_labeler import AnomalyLabeler\n\nlabeler = AnomalyLabeler(\n    horizon=60,\n    vol_window=1440,\n    threshold_return_std=4.0,\n    mask_to_signals=False,\n)\nlabeled = labeler.compute(ohlcv_df)\n</code></pre> Note <p>This is a forward-looking labeler -- it uses future data and is NOT suitable for live trading. Use <code>AnomalyDetector</code> for real-time anomaly detection.</p>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.flash_horizon","title":"flash_horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>flash_horizon: int = 10\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.horizon","title":"horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizon: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('forward_ret', 'vol')\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = ANOMALY\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.threshold_return_std","title":"threshold_return_std  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>threshold_return_std: float = 4.0\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.vol_window","title":"vol_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_window: int = 1440\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/anomaly_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.horizon &lt;= 0:\n        raise ValueError(\"horizon must be &gt; 0\")\n    if self.vol_window &lt;= 0:\n        raise ValueError(\"vol_window must be &gt; 0\")\n    if self.threshold_return_std &lt;= 0:\n        raise ValueError(\"threshold_return_std must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.anomaly_labeler.AnomalyLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute anomaly labels for a single pair group.</p> <p>Parameters:</p> Name Type Description Default <code>group_df</code> <code>DataFrame</code> <p>Single pair's data sorted by timestamp.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Same length as input with anomaly label column added. Labels are \"extreme_positive_anomaly\", \"extreme_negative_anomaly\", or null.</p> Source code in <code>src/signalflow/target/anomaly_labeler.py</code> <pre><code>def compute_group(\n    self,\n    group_df: pl.DataFrame,\n    data_context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Compute anomaly labels for a single pair group.\n\n    Args:\n        group_df (pl.DataFrame): Single pair's data sorted by timestamp.\n        data_context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Same length as input with anomaly label column added.\n            Labels are \"extreme_positive_anomaly\", \"extreme_negative_anomaly\", or null.\n    \"\"\"\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    if group_df.height == 0:\n        return group_df\n\n    price = pl.col(self.price_col)\n\n    # Step 1: log returns\n    df = group_df.with_columns(\n        (price / price.shift(1)).log().alias(\"_log_ret\"),\n    )\n\n    # Step 2: rolling std of returns\n    df = df.with_columns(\n        pl.col(\"_log_ret\")\n        .rolling_std(window_size=self.vol_window, min_samples=max(2, self.vol_window // 4))\n        .alias(\"_rolling_vol\"),\n    )\n\n    # Step 3: forward return (signed) and magnitude\n    df = df.with_columns(\n        (price.shift(-self.horizon) / price).log().alias(\"_forward_ret\"),\n    )\n    df = df.with_columns(\n        pl.col(\"_forward_ret\").abs().alias(\"_forward_ret_abs\"),\n    )\n\n    # Step 4-5: compute threshold and classify\n    # Scale per-bar volatility to horizon-length volatility: vol * sqrt(horizon)\n    horizon_threshold = pl.col(\"_rolling_vol\") * self.threshold_return_std * math.sqrt(self.horizon)\n\n    # For flash crash detection, check if a large negative move happens\n    # within flash_horizon bars (shorter window).\n    flash_threshold = pl.col(\"_rolling_vol\") * self.threshold_return_std * math.sqrt(self.flash_horizon)\n    df = df.with_columns(\n        (price.shift(-self.flash_horizon) / price).log().alias(\"_flash_ret\"),\n    )\n\n    is_anomaly = (\n        pl.col(\"_forward_ret_abs\").is_not_null()\n        &amp; pl.col(\"_rolling_vol\").is_not_null()\n        &amp; (pl.col(\"_forward_ret_abs\") &gt; horizon_threshold)\n    )\n\n    is_flash_crash = (\n        is_anomaly\n        &amp; pl.col(\"_flash_ret\").is_not_null()\n        &amp; (pl.col(\"_flash_ret\") &lt; 0)\n        &amp; (pl.col(\"_flash_ret\").abs() &gt; flash_threshold)\n    )\n\n    label_expr = (\n        pl.when(is_flash_crash)\n        .then(pl.lit(\"extreme_negative_anomaly\"))\n        .when(is_anomaly)\n        .then(pl.lit(\"extreme_positive_anomaly\"))\n        .otherwise(pl.lit(None, dtype=pl.Utf8))\n        .alias(self.out_col)\n    )\n\n    df = df.with_columns(label_expr)\n\n    # Step 6: meta columns\n    if self.include_meta:\n        df = df.with_columns(\n            [\n                pl.col(\"_forward_ret\").alias(\"forward_ret\"),\n                pl.col(\"_rolling_vol\").alias(\"vol\"),\n            ]\n        )\n\n    # Clean up temporary columns\n    df = df.drop(\n        [\n            c\n            for c in (\"_log_ret\", \"_rolling_vol\", \"_forward_ret\", \"_forward_ret_abs\", \"_flash_ret\")\n            if c in df.columns\n        ]\n    )\n\n    # Apply signal masking if configured\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#volatility-regime-labeler","title":"Volatility Regime Labeler","text":""},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler","title":"signalflow.target.volatility_labeler.VolatilityRegimeLabeler  <code>dataclass</code>","text":"<pre><code>VolatilityRegimeLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.VOLATILITY, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('realized_vol', 'vol_percentile'), price_col: str = 'close', horizon: int = 60, upper_quantile: float = 0.67, lower_quantile: float = 0.33, lookback_window: int = 1440)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Label bars by forward realized volatility regime.</p> Algorithm <ol> <li>Compute log returns: <code>ln(close[t] / close[t-1])</code></li> <li>Forward realized volatility: <code>std(log_returns[t+1 : t+horizon+1])</code>    computed using reverse-shifted rolling std.</li> <li>Rolling percentile of realized vol over <code>lookback_window</code>.</li> <li>If vol &gt; <code>upper_quantile</code> percentile -&gt; <code>\"high_volatility\"</code></li> <li>If vol &lt; <code>lower_quantile</code> percentile -&gt; <code>\"low_volatility\"</code></li> <li>Otherwise -&gt; <code>null</code> (Polars null)</li> </ol> Implementation <p>Uses pure Polars expressions instead of numpy loops for better performance and memory efficiency.</p> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column name. Default: <code>\"close\"</code>.</p> <code>horizon</code> <code>int</code> <p>Number of forward bars for realized vol. Default: <code>60</code>.</p> <code>upper_quantile</code> <code>float</code> <p>Upper percentile threshold (0-1). Default: <code>0.67</code>.</p> <code>lower_quantile</code> <code>float</code> <p>Lower percentile threshold (0-1). Default: <code>0.33</code>.</p> <code>lookback_window</code> <code>int</code> <p>Rolling window for percentile calc. Default: <code>1440</code>.</p> Example <pre><code>from signalflow.target.volatility_labeler import VolatilityRegimeLabeler\n\nlabeler = VolatilityRegimeLabeler(\n    horizon=60,\n    upper_quantile=0.67,\n    lower_quantile=0.33,\n    mask_to_signals=False,\n)\nresult = labeler.compute(ohlcv_df)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.horizon","title":"horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizon: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.lookback_window","title":"lookback_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookback_window: int = 1440\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.lower_quantile","title":"lower_quantile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lower_quantile: float = 0.33\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('realized_vol', 'vol_percentile')\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = VOLATILITY\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.upper_quantile","title":"upper_quantile  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>upper_quantile: float = 0.67\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/volatility_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.horizon &lt;= 0:\n        raise ValueError(\"horizon must be &gt; 0\")\n    if not (0.0 &lt; self.lower_quantile &lt; self.upper_quantile &lt; 1.0):\n        raise ValueError(\n            \"Require 0 &lt; lower_quantile &lt; upper_quantile &lt; 1, \"\n            f\"got lower_quantile={self.lower_quantile}, upper_quantile={self.upper_quantile}\"\n        )\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler._compute_percentile_series","title":"_compute_percentile_series  <code>staticmethod</code>","text":"<pre><code>_compute_percentile_series(s: Series, window: int) -&gt; pl.Series\n</code></pre> <p>Compute rolling percentile for a series of structs.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>Series</code> <p>Series of structs with 'val' and 'idx' fields.</p> required <code>window</code> <code>int</code> <p>Lookback window size.</p> required <p>Returns:</p> Type Description <code>Series</code> <p>Series of percentile values.</p> Source code in <code>src/signalflow/target/volatility_labeler.py</code> <pre><code>@staticmethod\ndef _compute_percentile_series(s: pl.Series, window: int) -&gt; pl.Series:\n    \"\"\"Compute rolling percentile for a series of structs.\n\n    Args:\n        s: Series of structs with 'val' and 'idx' fields.\n        window: Lookback window size.\n\n    Returns:\n        Series of percentile values.\n    \"\"\"\n    df = s.struct.unnest()\n    vals = df[\"val\"].to_numpy()\n    n = len(vals)\n    result = [None] * n\n\n    import numpy as np\n\n    for i in range(n):\n        if np.isnan(vals[i]) if vals[i] is not None else True:\n            continue\n\n        start = max(0, i - window + 1)\n        window_vals = vals[start : i + 1]\n\n        # Filter out NaN/None values\n        valid = window_vals[~np.isnan(window_vals)]\n        if len(valid) &lt; 2:\n            continue\n\n        # Percentile = fraction of values &lt;= current\n        result[i] = float(np.mean(valid &lt;= vals[i]))\n\n    return pl.Series(result, dtype=pl.Float64)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler._rolling_percentile_expr","title":"_rolling_percentile_expr","text":"<pre><code>_rolling_percentile_expr(col_name: str, window: int) -&gt; pl.Expr\n</code></pre> <p>Compute rolling percentile using Polars expressions.</p> <p>For each row, computes the fraction of values in the lookback window that are less than or equal to the current value.</p> <p>Parameters:</p> Name Type Description Default <code>col_name</code> <code>str</code> <p>Column to compute percentile for.</p> required <code>window</code> <code>int</code> <p>Lookback window size.</p> required <p>Returns:</p> Type Description <code>Expr</code> <p>Polars expression computing rolling percentile.</p> Source code in <code>src/signalflow/target/volatility_labeler.py</code> <pre><code>def _rolling_percentile_expr(self, col_name: str, window: int) -&gt; pl.Expr:\n    \"\"\"Compute rolling percentile using Polars expressions.\n\n    For each row, computes the fraction of values in the lookback window\n    that are less than or equal to the current value.\n\n    Args:\n        col_name: Column to compute percentile for.\n        window: Lookback window size.\n\n    Returns:\n        Polars expression computing rolling percentile.\n    \"\"\"\n    col = pl.col(col_name)\n\n    # Create a struct with current value and row index\n    # Then use rolling_map to compute percentile within each window\n    return pl.struct([col.alias(\"val\"), pl.int_range(pl.len()).alias(\"idx\")]).map_batches(\n        lambda s: self._compute_percentile_series(s, window),\n        return_dtype=pl.Float64,\n    )\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volatility_labeler.VolatilityRegimeLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute volatility regime labels for a single pair.</p> <p>Parameters:</p> Name Type Description Default <code>group_df</code> <code>DataFrame</code> <p>Single pair data sorted by timestamp.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Optional additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with same row count, plus label and optional meta columns.</p> Source code in <code>src/signalflow/target/volatility_labeler.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute volatility regime labels for a single pair.\n\n    Args:\n        group_df: Single pair data sorted by timestamp.\n        data_context: Optional additional context.\n\n    Returns:\n        DataFrame with same row count, plus label and optional meta columns.\n    \"\"\"\n    if group_df.height == 0:\n        return group_df\n\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    # Step 1: Log returns\n    df = group_df.with_columns((pl.col(self.price_col) / pl.col(self.price_col).shift(1)).log().alias(\"_log_ret\"))\n\n    # Step 2: Forward realized volatility\n    # To compute std of log_returns[t+1 : t+horizon+1], we:\n    # - Shift log_ret by -1 to start from next bar\n    # - Apply rolling_std with window=horizon\n    # - The result at position t+horizon-1 contains std of [t, t+horizon)\n    # - Shift back by -(horizon-1) to align with position t\n    df = df.with_columns(\n        pl.col(\"_log_ret\")\n        .shift(-1)\n        .rolling_std(window_size=self.horizon, min_samples=2)\n        .shift(-(self.horizon - 1))\n        .alias(\"_realized_vol\")\n    )\n\n    # Step 3: Rolling percentile using rank-based approach\n    # For each bar, compute what fraction of values in the lookback window\n    # are &lt;= current value. This is equivalent to percentile.\n    #\n    # Using rolling_map with a custom expression to compute percentile:\n    # percentile = count(x &lt;= current) / count(valid)\n    df = df.with_columns(\n        self._rolling_percentile_expr(\"_realized_vol\", self.lookback_window).alias(\"_vol_percentile\")\n    )\n\n    # Step 4-5: Assign labels based on percentile thresholds\n    label_expr = (\n        pl.when(pl.col(\"_vol_percentile\").is_null())\n        .then(pl.lit(None, dtype=pl.Utf8))\n        .when(pl.col(\"_vol_percentile\") &gt; self.upper_quantile)\n        .then(pl.lit(\"high_volatility\"))\n        .when(pl.col(\"_vol_percentile\") &lt; self.lower_quantile)\n        .then(pl.lit(\"low_volatility\"))\n        .otherwise(pl.lit(None, dtype=pl.Utf8))\n        .alias(self.out_col)\n    )\n\n    df = df.with_columns(label_expr)\n\n    if self.include_meta:\n        df = df.with_columns(\n            [\n                pl.col(\"_realized_vol\").alias(\"realized_vol\"),\n                pl.col(\"_vol_percentile\").alias(\"vol_percentile\"),\n            ]\n        )\n\n    # Clean up temporary columns\n    df = df.drop([\"_log_ret\", \"_realized_vol\", \"_vol_percentile\"])\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#trend-scanning-labeler","title":"Trend Scanning Labeler","text":""},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler","title":"signalflow.target.trend_scanning.TrendScanningLabeler  <code>dataclass</code>","text":"<pre><code>TrendScanningLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.TREND_MOMENTUM, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t_stat', 'best_window'), price_col: str = 'close', min_lookforward: int = 5, max_lookforward: int = 60, step: int = 5, critical_value: float = 1.96)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Label bars using De Prado's trend scanning method.</p> <p>For each bar, fits OLS regressions over multiple forward windows and selects the window with the strongest t-statistic. The sign and magnitude of the t-statistic determine the label.</p> Reference <p>De Prado, M. L. (2020). Machine Learning for Asset Managers, Ch. 5.</p> Algorithm <ol> <li> <p>For each bar t, for each window L in    range(min_lookforward, max_lookforward+1, step):</p> </li> <li> <p>Fit OLS: Price[t+i] = alpha + beta * i, for i=0..L-1</p> </li> <li> <p>Compute t-statistic: t = beta / SE(beta)</p> </li> <li> <p>Select L* = argmax_L |t_stat(t, L)|</p> </li> <li> <p>Label:</p> </li> <li> <p><code>\"rise\"</code> if t_stat &gt; critical_value</p> </li> <li><code>\"fall\"</code> if t_stat &lt; -critical_value</li> <li><code>null</code> otherwise</li> </ol> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column. Default: <code>\"close\"</code>.</p> <code>min_lookforward</code> <code>int</code> <p>Minimum forward window. Default: <code>5</code>.</p> <code>max_lookforward</code> <code>int</code> <p>Maximum forward window. Default: <code>60</code>.</p> <code>step</code> <code>int</code> <p>Step between window sizes. Default: <code>5</code>.</p> <code>critical_value</code> <code>float</code> <p>T-stat threshold for significance. Default: <code>1.96</code>.</p> Example <pre><code>from signalflow.target.trend_scanning import TrendScanningLabeler\n\nlabeler = TrendScanningLabeler(\n    min_lookforward=5,\n    max_lookforward=60,\n    critical_value=1.96,\n    mask_to_signals=False,\n)\nresult = labeler.compute(ohlcv_df)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.critical_value","title":"critical_value  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>critical_value: float = 1.96\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.max_lookforward","title":"max_lookforward  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_lookforward: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('t_stat', 'best_window')\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.min_lookforward","title":"min_lookforward  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_lookforward: int = 5\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = TREND_MOMENTUM\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.step","title":"step  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>step: int = 5\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/trend_scanning.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.min_lookforward &lt; 3:\n        raise ValueError(\"min_lookforward must be &gt;= 3 (need at least 3 points for OLS)\")\n    if self.max_lookforward &lt; self.min_lookforward:\n        raise ValueError(\"max_lookforward must be &gt;= min_lookforward\")\n    if self.step &lt; 1:\n        raise ValueError(\"step must be &gt;= 1\")\n    if self.critical_value &lt;= 0:\n        raise ValueError(\"critical_value must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.trend_scanning.TrendScanningLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute trend scanning labels for a single pair.</p> <p>Parameters:</p> Name Type Description Default <code>group_df</code> <code>DataFrame</code> <p>Single pair data sorted by timestamp.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Optional additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with same row count, plus label and optional meta columns.</p> Source code in <code>src/signalflow/target/trend_scanning.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute trend scanning labels for a single pair.\n\n    Args:\n        group_df: Single pair data sorted by timestamp.\n        data_context: Optional additional context.\n\n    Returns:\n        DataFrame with same row count, plus label and optional meta columns.\n    \"\"\"\n    if group_df.height == 0:\n        return group_df\n\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    prices = group_df[self.price_col].to_numpy().astype(np.float64)\n\n    t_stats, best_windows = _trend_scan(\n        prices,\n        self.min_lookforward,\n        self.max_lookforward,\n        self.step,\n    )\n\n    # Assign labels based on t-statistic vs critical value\n    n = len(prices)\n    labels = [None] * n\n    for t in range(n):\n        if np.isnan(t_stats[t]):\n            continue\n        if t_stats[t] &gt; self.critical_value:\n            labels[t] = \"rise\"\n        elif t_stats[t] &lt; -self.critical_value:\n            labels[t] = \"fall\"\n\n    df = group_df.with_columns(pl.Series(name=self.out_col, values=labels, dtype=pl.Utf8))\n\n    if self.include_meta:\n        df = df.with_columns(\n            [\n                pl.Series(name=\"t_stat\", values=t_stats.tolist(), dtype=pl.Float64),\n                pl.Series(\n                    name=\"best_window\",\n                    values=best_windows.tolist(),\n                    dtype=pl.Float64,\n                ),\n            ]\n        )\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#structure-labeler-local-extrema","title":"Structure Labeler (Local Extrema)","text":""},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler","title":"signalflow.target.structure_labeler.StructureLabeler  <code>dataclass</code>","text":"<pre><code>StructureLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.PRICE_STRUCTURE, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('swing_pct',), price_col: str = 'close', lookforward: int = 60, lookback: int = 60, min_swing_pct: float = 0.02, min_swing_zscore: float | None = None, vol_window: int = 500)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Label local tops and bottoms using a symmetric window.</p> <p>Uses future knowledge (look-forward) to identify bars that are local extrema within a combined lookback + lookforward window, filtered by either a fixed percentage or a rolling z-score threshold.</p> Swing Filter Modes <p>Fixed percentage (default): swing must exceed <code>min_swing_pct</code>.</p> <p>Rolling z-score: set <code>min_swing_zscore</code> to enable. Computes rolling mean and std of window swings over <code>vol_window</code> bars, then filters by z-score &gt;= threshold. Adapts to market volatility automatically \u2014 tighter in calm markets, wider in volatile ones.</p> Algorithm <ol> <li>For each bar t, examine <code>close[t-lookback : t+lookforward+1]</code>.</li> <li>Compute swing = <code>(window_max - window_min) / window_min</code>.</li> <li>If <code>close[t]</code> is the maximum in that window -&gt; candidate top.</li> <li>If <code>close[t]</code> is the minimum in that window -&gt; candidate bottom.</li> <li>Apply swing filter to confirm:</li> <li>Fixed: <code>swing &gt;= min_swing_pct</code></li> <li>Z-score: <code>(swing - rolling_mean) / rolling_std &gt;= min_swing_zscore</code></li> <li>Otherwise -&gt; <code>null</code>.</li> </ol> Implementation <p>Uses Polars rolling expressions for computing window max/min and detecting extrema, reducing numpy loop overhead.</p> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column. Default: <code>\"close\"</code>.</p> <code>lookforward</code> <code>int</code> <p>Forward window size. Default: <code>60</code>.</p> <code>lookback</code> <code>int</code> <p>Backward window size. Default: <code>60</code>.</p> <code>min_swing_pct</code> <code>float</code> <p>Fixed minimum swing percentage. Default: <code>0.02</code> (2%). Ignored when <code>min_swing_zscore</code> is set.</p> <code>min_swing_zscore</code> <code>float | None</code> <p>Z-score threshold for adaptive filtering. Default: <code>None</code>. When set, overrides <code>min_swing_pct</code>.</p> <code>vol_window</code> <code>int</code> <p>Rolling window for z-score baseline. Default: <code>500</code>.</p> Example <pre><code># Fixed percentage mode (default)\nlabeler = StructureLabeler(min_swing_pct=0.02, mask_to_signals=False)\n\n# Rolling z-score mode (adaptive)\nlabeler = StructureLabeler(\n    min_swing_zscore=2.0,\n    vol_window=500,\n    mask_to_signals=False,\n)\nresult = labeler.compute(ohlcv_df)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.lookback","title":"lookback  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookback: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.lookforward","title":"lookforward  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookforward: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('swing_pct',)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.min_swing_pct","title":"min_swing_pct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_swing_pct: float = 0.02\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.min_swing_zscore","title":"min_swing_zscore  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_swing_zscore: float | None = None\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = PRICE_STRUCTURE\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.vol_window","title":"vol_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_window: int = 500\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/structure_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lookforward &lt;= 0:\n        raise ValueError(\"lookforward must be &gt; 0\")\n    if self.lookback &lt;= 0:\n        raise ValueError(\"lookback must be &gt; 0\")\n    if self.min_swing_zscore is not None:\n        if self.min_swing_zscore &lt;= 0:\n            raise ValueError(\"min_swing_zscore must be &gt; 0\")\n        if self.vol_window &lt; 20:\n            raise ValueError(\"vol_window must be &gt;= 20 for z-score mode\")\n    elif self.min_swing_pct &lt; 0:\n        raise ValueError(\"min_swing_pct must be &gt;= 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.StructureLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute structure labels for a single pair.</p> Source code in <code>src/signalflow/target/structure_labeler.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute structure labels for a single pair.\"\"\"\n    if group_df.height == 0:\n        return group_df\n\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    price = pl.col(self.price_col)\n\n    # Step 1: Compute centered window max/min using rolling + shift\n    #\n    # For a centered window [t-lookback, t+lookforward]:\n    # - Lookback max/min: rolling_max/min with window=lookback+1 (includes current)\n    # - Lookforward max/min: shift(-lookforward), then rolling_max/min, shift back\n    #\n    # Full window max = max(lookback_max, lookforward_max)\n    # Full window min = min(lookback_min, lookforward_min)\n\n    lookback_window = self.lookback + 1  # Include current bar\n    lookforward_window = self.lookforward + 1  # Include current bar\n\n    # Lookback rolling (includes current bar, looks back)\n    lookback_max = price.rolling_max(window_size=lookback_window, min_samples=1)\n    lookback_min = price.rolling_min(window_size=lookback_window, min_samples=1)\n\n    # Lookforward rolling (shift, apply rolling, shift back)\n    # After shift(-1), position t contains value from t+1\n    # Apply rolling_max to get max of next `lookforward` bars\n    # Then shift back to align\n    lookforward_max = (\n        price.shift(-1).rolling_max(window_size=self.lookforward, min_samples=1).shift(-(self.lookforward - 1))\n    )\n    lookforward_min = (\n        price.shift(-1).rolling_min(window_size=self.lookforward, min_samples=1).shift(-(self.lookforward - 1))\n    )\n\n    df = group_df.with_columns(\n        [\n            lookback_max.alias(\"_lb_max\"),\n            lookback_min.alias(\"_lb_min\"),\n            lookforward_max.alias(\"_lf_max\"),\n            lookforward_min.alias(\"_lf_min\"),\n        ]\n    )\n\n    # Full window max/min (handle null in lookforward part at edges)\n    df = df.with_columns(\n        [\n            pl.max_horizontal(\"_lb_max\", \"_lf_max\").alias(\"_win_max\"),\n            pl.min_horizontal(\"_lb_min\", \"_lf_min\").alias(\"_win_min\"),\n        ]\n    )\n\n    # Step 2: Compute swing = (win_max - win_min) / win_min\n    df = df.with_columns(\n        pl.when((pl.col(\"_win_min\") &gt; 0) &amp; (pl.col(\"_win_max\") != pl.col(\"_win_min\")))\n        .then((pl.col(\"_win_max\") - pl.col(\"_win_min\")) / pl.col(\"_win_min\"))\n        .otherwise(pl.lit(None))\n        .alias(\"_swing\")\n    )\n\n    # Step 3: Detect if current price is the window max or min\n    df = df.with_columns(\n        [\n            (price == pl.col(\"_win_max\")).alias(\"_is_max\"),\n            (price == pl.col(\"_win_min\")).alias(\"_is_min\"),\n        ]\n    )\n\n    # Step 4: Apply threshold filter\n    if self.min_swing_zscore is not None:\n        # Z-score mode: compute rolling mean/std of swings\n        df = df.with_columns(\n            [\n                pl.col(\"_swing\").rolling_mean(window_size=self.vol_window, min_samples=20).alias(\"_swing_mean\"),\n                pl.col(\"_swing\").rolling_std(window_size=self.vol_window, min_samples=20).alias(\"_swing_std\"),\n            ]\n        )\n\n        # Z-score = (swing - mean) / std &gt;= threshold\n        df = df.with_columns(\n            pl.when(pl.col(\"_swing_std\") &gt; 0)\n            .then((pl.col(\"_swing\") - pl.col(\"_swing_mean\")) / pl.col(\"_swing_std\"))\n            .otherwise(pl.lit(None))\n            .alias(\"_zscore\")\n        )\n\n        threshold_mask = pl.col(\"_zscore\") &gt;= self.min_swing_zscore\n    else:\n        # Fixed percentage mode\n        threshold_mask = pl.col(\"_swing\") &gt;= self.min_swing_pct\n\n    # Step 5: Assign labels\n    label_expr = (\n        pl.when(threshold_mask &amp; pl.col(\"_is_max\"))\n        .then(pl.lit(\"local_max\"))\n        .when(threshold_mask &amp; pl.col(\"_is_min\"))\n        .then(pl.lit(\"local_min\"))\n        .otherwise(pl.lit(None, dtype=pl.Utf8))\n        .alias(self.out_col)\n    )\n\n    df = df.with_columns(label_expr)\n\n    # Meta: swing_pct only for labeled rows\n    if self.include_meta:\n        df = df.with_columns(\n            pl.when(pl.col(self.out_col).is_not_null())\n            .then(pl.col(\"_swing\"))\n            .otherwise(pl.lit(None))\n            .alias(\"swing_pct\")\n        )\n\n    # Clean up temporary columns\n    temp_cols = [\n        \"_lb_max\",\n        \"_lb_min\",\n        \"_lf_max\",\n        \"_lf_min\",\n        \"_win_max\",\n        \"_win_min\",\n        \"_swing\",\n        \"_is_max\",\n        \"_is_min\",\n    ]\n    if self.min_swing_zscore is not None:\n        temp_cols.extend([\"_swing_mean\", \"_swing_std\", \"_zscore\"])\n\n    df = df.drop([c for c in temp_cols if c in df.columns])\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#zigzag-structure-labeler-global","title":"Zigzag Structure Labeler (Global)","text":""},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler","title":"signalflow.target.structure_labeler.ZigzagStructureLabeler  <code>dataclass</code>","text":"<pre><code>ZigzagStructureLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.PRICE_STRUCTURE, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('swing_pct',), price_col: str = 'close', min_swing_pct: float = 0.02, min_swing_zscore: float | None = None, vol_window: int = 500)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Label local tops and bottoms using a full-series zigzag algorithm.</p> <p>Unlike <code>StructureLabeler</code> (which uses fixed-size windows around each bar), this labeler scans the entire price series to find alternating swing highs and lows. A new pivot is confirmed only when the price reverses by more than the threshold from the current extreme.</p> The zigzag algorithm ensures <ul> <li>Tops and bottoms strictly alternate (no consecutive tops or bottoms).</li> <li>Each swing exceeds the threshold (either fixed % or adaptive).</li> <li>Pivots are globally consistent across the full series.</li> </ul> Swing Filter Modes <p>Fixed percentage (default): reversal must exceed <code>min_swing_pct</code>.</p> <p>Adaptive (z-score): set <code>min_swing_zscore</code> to enable. Uses rolling volatility (std of log-returns) to compute a per-bar threshold: <code>threshold = zscore \u00d7 vol \u00d7 sqrt(vol_window)</code>.</p> Algorithm <ol> <li>Find first significant swing to determine initial direction.</li> <li>Track the running extreme (highest high or lowest low).</li> <li>When price reverses from the extreme by &gt; threshold:</li> <li>Mark the extreme as <code>\"local_max\"</code> or <code>\"local_min\"</code>.</li> <li>Switch direction and start tracking the new extreme.</li> <li>Result: alternating pivots across the full price series.</li> </ol> Implementation <p>Uses a sequential state-machine algorithm. This is inherently not parallelizable, so numpy/python loops are used. Polars is used for rolling volatility computation in z-score mode.</p> <p>Attributes:</p> Name Type Description <code>price_col</code> <code>str</code> <p>Price column. Default: <code>\"close\"</code>.</p> <code>min_swing_pct</code> <code>float</code> <p>Fixed minimum reversal percentage. Default: <code>0.02</code>. Ignored when <code>min_swing_zscore</code> is set.</p> <code>min_swing_zscore</code> <code>float | None</code> <p>Z-score multiplier for adaptive threshold. Default: <code>None</code>. When set, overrides <code>min_swing_pct</code>.</p> <code>vol_window</code> <code>int</code> <p>Rolling window for volatility computation. Default: <code>500</code>.</p> Example <pre><code># Fixed percentage\nlabeler = ZigzagStructureLabeler(min_swing_pct=0.03, mask_to_signals=False)\n\n# Adaptive threshold (z-score \u00d7 rolling volatility)\nlabeler = ZigzagStructureLabeler(\n    min_swing_zscore=2.0,\n    vol_window=500,\n    mask_to_signals=False,\n)\nresult = labeler.compute(ohlcv_df)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('swing_pct',)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.min_swing_pct","title":"min_swing_pct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_swing_pct: float = 0.02\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.min_swing_zscore","title":"min_swing_zscore  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>min_swing_zscore: float | None = None\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = PRICE_STRUCTURE\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.vol_window","title":"vol_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_window: int = 500\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/structure_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.min_swing_zscore is not None:\n        if self.min_swing_zscore &lt;= 0:\n            raise ValueError(\"min_swing_zscore must be &gt; 0\")\n        if self.vol_window &lt; 20:\n            raise ValueError(\"vol_window must be &gt;= 20 for z-score mode\")\n    elif self.min_swing_pct &lt;= 0:\n        raise ValueError(\"min_swing_pct must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler._adaptive_thresholds","title":"_adaptive_thresholds","text":"<pre><code>_adaptive_thresholds(df: DataFrame, prices: ndarray) -&gt; np.ndarray\n</code></pre> <p>Compute per-bar thresholds: zscore \u00d7 rolling_vol \u00d7 sqrt(vol_window).</p> <p>Uses Polars for rolling std computation.</p> Source code in <code>src/signalflow/target/structure_labeler.py</code> <pre><code>def _adaptive_thresholds(self, df: pl.DataFrame, prices: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Compute per-bar thresholds: zscore \u00d7 rolling_vol \u00d7 sqrt(vol_window).\n\n    Uses Polars for rolling std computation.\n    \"\"\"\n    n = len(prices)\n    if n &lt; 2:\n        return np.full(n, np.inf)\n\n    # Compute log returns using Polars\n    price_col = pl.col(self.price_col)\n    log_ret = (price_col / price_col.shift(1)).log()\n\n    # Rolling std of returns\n    rolling_vol = log_ret.rolling_std(window_size=self.vol_window, min_samples=20)\n\n    # Compute thresholds\n    vol_arr = df.select(rolling_vol.alias(\"vol\"))[\"vol\"].to_numpy()\n\n    # threshold = zscore \u00d7 vol \u00d7 sqrt(vol_window)\n    thresholds = self.min_swing_zscore * vol_arr * np.sqrt(self.vol_window)\n\n    # Before we have enough data, use infinity (don't create pivots)\n    thresholds = np.where(np.isnan(thresholds), np.inf, thresholds)\n\n    return thresholds\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler._zigzag","title":"_zigzag","text":"<pre><code>_zigzag(prices: ndarray, thresholds: ndarray) -&gt; tuple[list[str | None], np.ndarray]\n</code></pre> <p>Run zigzag algorithm with per-bar adaptive thresholds.</p> <p>This is a state-machine algorithm that must process bars sequentially to maintain alternating top/bottom structure.</p> <p>Returns:</p> Type Description <code>tuple[list[str | None], ndarray]</code> <p>(labels, swing_pcts) \u2014 parallel arrays of length n.</p> Source code in <code>src/signalflow/target/structure_labeler.py</code> <pre><code>def _zigzag(self, prices: np.ndarray, thresholds: np.ndarray) -&gt; tuple[list[str | None], np.ndarray]:\n    \"\"\"Run zigzag algorithm with per-bar adaptive thresholds.\n\n    This is a state-machine algorithm that must process bars sequentially\n    to maintain alternating top/bottom structure.\n\n    Returns:\n        (labels, swing_pcts) \u2014 parallel arrays of length n.\n    \"\"\"\n    n = len(prices)\n    labels: list[str | None] = [None] * n\n    swing_pcts = np.full(n, np.nan, dtype=np.float64)\n\n    if n &lt; 2:\n        return labels, swing_pcts\n\n    # Phase 1: Find initial direction \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    high_idx = 0\n    low_idx = 0\n    direction = 0  # 0=unknown, 1=going up (seeking top), -1=going down (seeking bottom)\n    init_end = 0\n\n    for i in range(1, n):\n        if prices[i] &gt; prices[high_idx]:\n            high_idx = i\n        if prices[i] &lt; prices[low_idx]:\n            low_idx = i\n\n        if high_idx != low_idx and prices[low_idx] &gt; 0:\n            swing = (prices[high_idx] - prices[low_idx]) / prices[low_idx]\n            if swing &gt;= thresholds[i]:\n                if high_idx &gt; low_idx:\n                    # Went down first then up \u2192 bottom confirmed\n                    labels[low_idx] = \"local_min\"\n                    swing_pcts[low_idx] = swing\n                    direction = 1  # now going up, seeking top\n                else:\n                    # Went up first then down \u2192 top confirmed\n                    labels[high_idx] = \"local_max\"\n                    swing_pcts[high_idx] = swing\n                    direction = -1  # now going down, seeking bottom\n                init_end = i\n                break\n\n    if direction == 0:\n        return labels, swing_pcts  # No significant swing in entire series\n\n    # Phase 2: Main zigzag loop \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    if direction == 1:\n        candidate_idx = high_idx\n        candidate_price = prices[high_idx]\n    else:\n        candidate_idx = low_idx\n        candidate_price = prices[low_idx]\n\n    for i in range(init_end + 1, n):\n        threshold = thresholds[i]\n\n        if direction == 1:  # Going up \u2192 seeking top\n            if prices[i] &gt;= candidate_price:\n                # New high \u2192 update candidate top\n                candidate_idx = i\n                candidate_price = prices[i]\n            elif candidate_price &gt; 0:\n                reversal = (candidate_price - prices[i]) / candidate_price\n                if reversal &gt;= threshold:\n                    # Confirm top\n                    labels[candidate_idx] = \"local_max\"\n                    swing_pcts[candidate_idx] = reversal\n                    direction = -1\n                    candidate_idx = i\n                    candidate_price = prices[i]\n\n        else:  # direction == -1: Going down \u2192 seeking bottom\n            if prices[i] &lt;= candidate_price:\n                candidate_idx = i\n                candidate_price = prices[i]\n            elif candidate_price &gt; 0:\n                reversal = (prices[i] - candidate_price) / candidate_price\n                if reversal &gt;= threshold:\n                    # Confirm bottom\n                    labels[candidate_idx] = \"local_min\"\n                    swing_pcts[candidate_idx] = reversal\n                    direction = 1\n                    candidate_idx = i\n                    candidate_price = prices[i]\n\n    return labels, swing_pcts\n</code></pre>"},{"location":"api/labeler/#signalflow.target.structure_labeler.ZigzagStructureLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute zigzag structure labels for a single pair.</p> Source code in <code>src/signalflow/target/structure_labeler.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute zigzag structure labels for a single pair.\"\"\"\n    if group_df.height == 0:\n        return group_df\n\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    prices = group_df[self.price_col].to_numpy().astype(np.float64)\n\n    # Compute per-bar thresholds using Polars for rolling vol\n    if self.min_swing_zscore is not None:\n        thresholds = self._adaptive_thresholds(group_df, prices)\n    else:\n        thresholds = np.full(len(prices), self.min_swing_pct)\n\n    # Run sequential zigzag algorithm\n    labels, swing_pcts = self._zigzag(prices, thresholds)\n\n    df = group_df.with_columns(pl.Series(name=self.out_col, values=labels, dtype=pl.Utf8))\n\n    if self.include_meta:\n        df = df.with_columns(\n            pl.Series(\n                name=\"swing_pct\",\n                values=swing_pcts.tolist(),\n                dtype=pl.Float64,\n            )\n        )\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#volume-regime-labeler","title":"Volume Regime Labeler","text":""},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler","title":"signalflow.target.volume_labeler.VolumeRegimeLabeler  <code>dataclass</code>","text":"<pre><code>VolumeRegimeLabeler(raw_data_type: RawDataType | str = RawDataType.SPOT, signal_category: SignalCategory = SignalCategory.VOLUME_LIQUIDITY, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('volume_ratio',), volume_col: str = 'volume', horizon: int = 60, vol_sma_window: int = 1440, spike_threshold: float = 2.0, drought_threshold: float = 0.3)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Label bars by forward volume regime.</p> <p>Detects volume spikes and droughts by comparing forward average volume to a trailing volume SMA.</p> Algorithm <ol> <li>Compute trailing volume SMA: <code>rolling_mean(volume, vol_sma_window)</code>.</li> <li>Compute forward volume ratio:    <code>mean(volume[t+1 : t+horizon+1]) / trailing_sma[t]</code></li> <li>If ratio &gt; <code>spike_threshold</code> -&gt; <code>\"abnormal_volume\"</code></li> <li>If ratio &lt; <code>drought_threshold</code> -&gt; <code>\"illiquidity\"</code></li> <li>Otherwise -&gt; <code>null</code></li> </ol> Implementation <p>Uses pure Polars expressions instead of numpy loops for better performance and memory efficiency.</p> <p>Attributes:</p> Name Type Description <code>volume_col</code> <code>str</code> <p>Volume column. Default: <code>\"volume\"</code>.</p> <code>horizon</code> <code>int</code> <p>Number of forward bars. Default: <code>60</code>.</p> <code>vol_sma_window</code> <code>int</code> <p>Trailing SMA window. Default: <code>1440</code>.</p> <code>spike_threshold</code> <code>float</code> <p>Threshold for volume spike. Default: <code>2.0</code>.</p> <code>drought_threshold</code> <code>float</code> <p>Threshold for volume drought. Default: <code>0.3</code>.</p> Example <pre><code>from signalflow.target.volume_labeler import VolumeRegimeLabeler\n\nlabeler = VolumeRegimeLabeler(\n    horizon=60,\n    spike_threshold=2.0,\n    drought_threshold=0.3,\n    mask_to_signals=False,\n)\nresult = labeler.compute(ohlcv_df)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.drought_threshold","title":"drought_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>drought_threshold: float = 0.3\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.horizon","title":"horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizon: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('volume_ratio',)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.signal_category","title":"signal_category  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal_category: SignalCategory = VOLUME_LIQUIDITY\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.spike_threshold","title":"spike_threshold  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>spike_threshold: float = 2.0\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.vol_sma_window","title":"vol_sma_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_sma_window: int = 1440\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.volume_col","title":"volume_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>volume_col: str = 'volume'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/volume_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.horizon &lt;= 0:\n        raise ValueError(\"horizon must be &gt; 0\")\n    if self.vol_sma_window &lt;= 0:\n        raise ValueError(\"vol_sma_window must be &gt; 0\")\n    if self.drought_threshold &gt;= self.spike_threshold:\n        raise ValueError(\n            f\"drought_threshold ({self.drought_threshold}) must be &lt; spike_threshold ({self.spike_threshold})\"\n        )\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.volume_labeler.VolumeRegimeLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute volume regime labels for a single pair.</p> <p>Parameters:</p> Name Type Description Default <code>group_df</code> <code>DataFrame</code> <p>Single pair data sorted by timestamp.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Optional additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with same row count, plus label and optional meta columns.</p> Source code in <code>src/signalflow/target/volume_labeler.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Compute volume regime labels for a single pair.\n\n    Args:\n        group_df: Single pair data sorted by timestamp.\n        data_context: Optional additional context.\n\n    Returns:\n        DataFrame with same row count, plus label and optional meta columns.\n    \"\"\"\n    if group_df.height == 0:\n        return group_df\n\n    if self.volume_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.volume_col}'\")\n\n    vol = pl.col(self.volume_col)\n\n    # Step 1: Trailing volume SMA using Polars rolling_mean\n    # min_samples=1 allows SMA from the first bar (like original behavior)\n    df = group_df.with_columns(\n        vol.rolling_mean(window_size=self.vol_sma_window, min_samples=1).alias(\"_trailing_sma\")\n    )\n\n    # Step 2: Forward average volume\n    # To compute mean(volume[t+1 : t+horizon+1]), we:\n    # - Shift volume by -1 to start from next bar\n    # - Apply rolling_mean with window=horizon\n    # - The result at position t+horizon-1 contains mean of [t, t+horizon)\n    # - Shift back by -(horizon-1) to align with position t\n    df = df.with_columns(\n        vol.shift(-1)\n        .rolling_mean(window_size=self.horizon, min_samples=1)\n        .shift(-(self.horizon - 1))\n        .alias(\"_forward_avg\")\n    )\n\n    # Step 3: Volume ratio = forward_avg / trailing_sma\n    df = df.with_columns(\n        pl.when(pl.col(\"_trailing_sma\") &gt; 0)\n        .then(pl.col(\"_forward_avg\") / pl.col(\"_trailing_sma\"))\n        .otherwise(pl.lit(None))\n        .alias(\"_volume_ratio\")\n    )\n\n    # Step 4-5: Assign labels based on thresholds\n    label_expr = (\n        pl.when(pl.col(\"_volume_ratio\").is_null())\n        .then(pl.lit(None, dtype=pl.Utf8))\n        .when(pl.col(\"_volume_ratio\") &gt; self.spike_threshold)\n        .then(pl.lit(\"abnormal_volume\"))\n        .when(pl.col(\"_volume_ratio\") &lt; self.drought_threshold)\n        .then(pl.lit(\"illiquidity\"))\n        .otherwise(pl.lit(None, dtype=pl.Utf8))\n        .alias(self.out_col)\n    )\n\n    df = df.with_columns(label_expr)\n\n    if self.include_meta:\n        df = df.with_columns(pl.col(\"_volume_ratio\").alias(\"volume_ratio\"))\n\n    # Clean up temporary columns\n    df = df.drop([\"_trailing_sma\", \"_forward_avg\", \"_volume_ratio\"])\n\n    if self.mask_to_signals and data_context is not None and \"signal_keys\" in data_context:\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#multi-target-generation","title":"Multi-Target Generation","text":""},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator","title":"signalflow.target.multi_target_generator.MultiTargetGenerator  <code>dataclass</code>","text":"<pre><code>MultiTargetGenerator(horizons: list[HorizonConfig] = (lambda: list(DEFAULT_HORIZONS))(), target_types: list[TargetType] = (lambda: list(DEFAULT_TARGET_TYPES))(), volume_window: int = 60, volume_quantiles: tuple[float, float] = (0.33, 0.67), crash_quantiles: tuple[float, float] = (0.1, 0.9), pair_col: str = 'pair', ts_col: str = 'timestamp', price_col: str = 'close')\n</code></pre> <p>Generates multiple targets at multiple horizons from OHLCV data.</p> <p>For each (horizon, target_type) combination, adds a column to the DataFrame. Column naming convention: <code>target_{target_name}_{horizon_name}</code>.</p> <p>Direction targets use the existing Labeler infrastructure. Return magnitude is <code>|log(close[t+h] / close[t])|</code>. Volume regime discretizes <code>volume / sma(volume)</code> into HIGH/MED/LOW. Crash regime classifies forward return into crash/rally/normal.</p> <p>Attributes:</p> Name Type Description <code>horizons</code> <code>list[HorizonConfig]</code> <p>List of HorizonConfig.</p> <code>target_types</code> <code>list[TargetType]</code> <p>List of TargetType to generate.</p> <code>volume_window</code> <code>int</code> <p>Rolling window for volume SMA baseline.</p> <code>volume_quantiles</code> <code>tuple[float, float]</code> <p>(low, high) thresholds for volume regime.</p> <code>crash_quantiles</code> <code>tuple[float, float]</code> <p>(crash, rally) quantile thresholds for crash regime.</p> <code>pair_col</code> <code>str</code> <p>Trading pair column name.</p> <code>ts_col</code> <code>str</code> <p>Timestamp column name.</p> <code>price_col</code> <code>str</code> <p>Price column name.</p>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.crash_quantiles","title":"crash_quantiles  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>crash_quantiles: tuple[float, float] = (0.1, 0.9)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.horizons","title":"horizons  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizons: list[HorizonConfig] = field(default_factory=lambda: list(DEFAULT_HORIZONS))\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.target_types","title":"target_types  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>target_types: list[TargetType] = field(default_factory=lambda: list(DEFAULT_TARGET_TYPES))\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.volume_quantiles","title":"volume_quantiles  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>volume_quantiles: tuple[float, float] = (0.33, 0.67)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.volume_window","title":"volume_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>volume_window: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._crash_regime_expr","title":"_crash_regime_expr","text":"<pre><code>_crash_regime_expr(horizon: int) -&gt; pl.Expr\n</code></pre> <p>Polars expression for crash/rally regime classification.</p> <p>Computes forward log-return over <code>horizon</code> bars, then discretizes into crash/rally/normal based on quantile thresholds.</p> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _crash_regime_expr(self, horizon: int) -&gt; pl.Expr:\n    \"\"\"Polars expression for crash/rally regime classification.\n\n    Computes forward log-return over ``horizon`` bars, then discretizes\n    into crash/rally/normal based on quantile thresholds.\n    \"\"\"\n    price = pl.col(self.price_col)\n    forward_return = (price.shift(-horizon) / price).log()\n\n    crash_q, rally_q = self.crash_quantiles\n    return (\n        pl.when(forward_return.is_null())\n        .then(pl.lit(None, dtype=pl.Utf8))\n        .when(forward_return &lt;= forward_return.quantile(crash_q))\n        .then(pl.lit(\"crash\"))\n        .when(forward_return &gt;= forward_return.quantile(rally_q))\n        .then(pl.lit(\"rally\"))\n        .otherwise(pl.lit(\"normal\"))\n    )\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._create_labeler","title":"_create_labeler","text":"<pre><code>_create_labeler(h: HorizonConfig) -&gt; Labeler\n</code></pre> <p>Instantiate a labeler for the given horizon.</p> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _create_labeler(self, h: HorizonConfig) -&gt; Labeler:\n    \"\"\"Instantiate a labeler for the given horizon.\"\"\"\n    kwargs = {\n        \"price_col\": self.price_col,\n        \"pair_col\": self.pair_col,\n        \"ts_col\": self.ts_col,\n        \"keep_input_columns\": True,\n        \"mask_to_signals\": False,\n        \"include_meta\": False,\n    }\n\n    kwargs[\"horizon\"] = h.horizon\n\n    kwargs.update(h.labeler_kwargs)\n    return h.labeler_cls(**kwargs)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._generate_crash_regime","title":"_generate_crash_regime","text":"<pre><code>_generate_crash_regime(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _generate_crash_regime(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    for h in self.horizons:\n        col_name = f\"target_crash_regime_{h.name}\"\n\n        df = df.with_columns(\n            df.group_by(self.pair_col, maintain_order=True)\n            .agg(self._crash_regime_expr(h.horizon).alias(col_name))\n            .sort(self.pair_col)\n            .get_column(col_name)\n            .explode()\n            .alias(col_name)\n        )\n\n        logger.debug(f\"Generated crash regime target: {col_name}\")\n\n    return df\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._generate_direction","title":"_generate_direction","text":"<pre><code>_generate_direction(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _generate_direction(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    for h in self.horizons:\n        col_name = f\"target_direction_{h.name}\"\n        labeler = self._create_labeler(h)\n        labeled = labeler.compute(df)\n\n        label_series = labeled.get_column(labeler.out_col).alias(col_name)\n        df = df.with_columns(label_series)\n\n        logger.debug(f\"Generated direction target: {col_name}\")\n\n    return df\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._generate_return_magnitude","title":"_generate_return_magnitude","text":"<pre><code>_generate_return_magnitude(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _generate_return_magnitude(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    for h in self.horizons:\n        col_name = f\"target_return_magnitude_{h.name}\"\n\n        df = df.with_columns(\n            df.group_by(self.pair_col, maintain_order=True)\n            .agg((pl.col(self.price_col).shift(-h.horizon) / pl.col(self.price_col)).log().abs().alias(col_name))\n            .sort(self.pair_col)\n            .get_column(col_name)\n            .explode()\n            .alias(col_name)\n        )\n\n        logger.debug(f\"Generated return magnitude target: {col_name}\")\n\n    return df\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._generate_volume_regime","title":"_generate_volume_regime","text":"<pre><code>_generate_volume_regime(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _generate_volume_regime(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    for h in self.horizons:\n        col_name = f\"target_volume_regime_{h.name}\"\n\n        df = df.with_columns(\n            df.group_by(self.pair_col, maintain_order=True)\n            .agg(self._volume_regime_expr(h.horizon).alias(col_name))\n            .sort(self.pair_col)\n            .get_column(col_name)\n            .explode()\n            .alias(col_name)\n        )\n\n        logger.debug(f\"Generated volume regime target: {col_name}\")\n\n    return df\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._validate","title":"_validate","text":"<pre><code>_validate(df: DataFrame) -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _validate(self, df: pl.DataFrame) -&gt; None:\n    required = {self.pair_col, self.ts_col, self.price_col}\n    target_type_names = {t.name for t in self.target_types}\n    if \"volume_regime\" in target_type_names:\n        required.add(\"volume\")\n\n    missing = required - set(df.columns)\n    if missing:\n        raise ValueError(f\"Missing required columns: {sorted(missing)}\")\n\n    if not self.horizons:\n        raise ValueError(\"At least one horizon is required\")\n    if not self.target_types:\n        raise ValueError(\"At least one target type is required\")\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator._volume_regime_expr","title":"_volume_regime_expr","text":"<pre><code>_volume_regime_expr(horizon: int) -&gt; pl.Expr\n</code></pre> <p>Polars expression for volume regime classification.</p> <p>Computes forward-looking average volume ratio vs current rolling SMA, then discretizes into HIGH/MED/LOW based on quantile thresholds.</p> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def _volume_regime_expr(self, horizon: int) -&gt; pl.Expr:\n    \"\"\"Polars expression for volume regime classification.\n\n    Computes forward-looking average volume ratio vs current rolling SMA,\n    then discretizes into HIGH/MED/LOW based on quantile thresholds.\n    \"\"\"\n    vol = pl.col(\"volume\")\n    vol_sma = vol.rolling_mean(window_size=self.volume_window)\n    vol_ratio = vol / vol_sma\n\n    # Use forward-looking mean of volume_ratio over the horizon\n    forward_vol_ratio = vol_ratio.shift(-horizon)\n\n    low_q, high_q = self.volume_quantiles\n    return (\n        pl.when(forward_vol_ratio.is_null())\n        .then(pl.lit(None, dtype=pl.Utf8))\n        .when(forward_vol_ratio &gt;= forward_vol_ratio.quantile(high_q))\n        .then(pl.lit(\"HIGH\"))\n        .when(forward_vol_ratio &lt;= forward_vol_ratio.quantile(low_q))\n        .then(pl.lit(\"LOW\"))\n        .otherwise(pl.lit(\"MED\"))\n    )\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.generate","title":"generate","text":"<pre><code>generate(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Generate all target columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>OHLCV DataFrame with pair, timestamp, open, high, low, close, volume.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Original DataFrame with added <code>target_*</code> columns.</p> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def generate(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Generate all target columns.\n\n    Args:\n        df: OHLCV DataFrame with pair, timestamp, open, high, low, close, volume.\n\n    Returns:\n        Original DataFrame with added ``target_*`` columns.\n    \"\"\"\n    self._validate(df)\n\n    target_type_map = {t.name: t for t in self.target_types}\n\n    if \"direction\" in target_type_map:\n        df = self._generate_direction(df)\n\n    if \"return_magnitude\" in target_type_map:\n        df = self._generate_return_magnitude(df)\n\n    if \"volume_regime\" in target_type_map:\n        df = self._generate_volume_regime(df)\n\n    if \"crash_regime\" in target_type_map:\n        df = self._generate_crash_regime(df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.MultiTargetGenerator.target_columns","title":"target_columns","text":"<pre><code>target_columns() -&gt; list[dict[str, str]]\n</code></pre> <p>Return metadata for all generated target columns.</p> <p>Returns:</p> Type Description <code>list[dict[str, str]]</code> <p>List of dicts with keys: column, horizon, target_type, kind.</p> Source code in <code>src/signalflow/target/multi_target_generator.py</code> <pre><code>def target_columns(self) -&gt; list[dict[str, str]]:\n    \"\"\"Return metadata for all generated target columns.\n\n    Returns:\n        List of dicts with keys: column, horizon, target_type, kind.\n    \"\"\"\n    result = []\n    for h in self.horizons:\n        for t in self.target_types:\n            result.append(\n                {\n                    \"column\": f\"target_{t.name}_{h.name}\",\n                    \"horizon\": h.name,\n                    \"target_type\": t.name,\n                    \"kind\": t.kind,\n                }\n            )\n    return result\n</code></pre>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.HorizonConfig","title":"signalflow.target.multi_target_generator.HorizonConfig  <code>dataclass</code>","text":"<pre><code>HorizonConfig(name: str, horizon: int, labeler_cls: type[Labeler] = TripleBarrierLabeler, labeler_kwargs: dict[str, Any] = dict())\n</code></pre> <p>Configuration for a single prediction horizon.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Human-readable name (e.g., \"short\", \"mid\", \"long\").</p> <code>horizon</code> <code>int</code> <p>Number of bars for the horizon.</p> <code>labeler_cls</code> <code>type[Labeler]</code> <p>Labeler class to use for direction targets.</p> <code>labeler_kwargs</code> <code>dict[str, Any]</code> <p>Extra kwargs passed to the labeler constructor.</p>"},{"location":"api/labeler/#signalflow.target.multi_target_generator.TargetType","title":"signalflow.target.multi_target_generator.TargetType  <code>dataclass</code>","text":"<pre><code>TargetType(name: str, kind: str)\n</code></pre> <p>Defines a target type derived from OHLCV data.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Target name (e.g., \"direction\", \"return_magnitude\", \"volume_regime\").</p> <code>kind</code> <code>str</code> <p>\"discrete\" or \"continuous\" \u2014 determines MI computation method.</p>"},{"location":"api/labeler/#utility-functions","title":"Utility Functions","text":""},{"location":"api/labeler/#mask_targets_by_signals","title":"mask_targets_by_signals","text":""},{"location":"api/labeler/#signalflow.target.utils.mask_targets_by_signals","title":"signalflow.target.utils.mask_targets_by_signals","text":"<pre><code>mask_targets_by_signals(df: DataFrame, signals: Signals, mask_signal_types: set[str], horizon_bars: int, cooldown_bars: int = 60, target_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp') -&gt; pl.DataFrame\n</code></pre> <p>Mask target columns for timestamps overlapping with specified signals.</p> <p>For each signal at time T with type in mask_signal_types: - Masks range [T - horizon_bars, T + cooldown_bars]</p> <p>This is useful for excluding labels that overlap with exogenous events (e.g., flash crashes, global market events) from training data, since no feature could predict such events.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with target columns.</p> required <code>signals</code> <code>Signals</code> <p>Signals object containing detected events.</p> required <code>mask_signal_types</code> <code>set[str]</code> <p>Signal types to mask (e.g. {\"flash_crash\", \"global_event\"}).</p> required <code>horizon_bars</code> <code>int</code> <p>Forward horizon (bars before signal that \"see\" it).</p> required <code>cooldown_bars</code> <code>int</code> <p>Bars after signal to mask (default: 60).</p> <code>60</code> <code>target_columns</code> <code>list[str] | None</code> <p>Columns to mask (default: all columns ending with \"_label\").</p> <code>None</code> <code>pair_col</code> <code>str</code> <p>Pair column name (default: \"pair\").</p> <code>'pair'</code> <code>ts_col</code> <code>str</code> <p>Timestamp column name (default: \"timestamp\").</p> <code>'timestamp'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with affected target columns set to null.</p> Example <pre><code>from signalflow.detector import ZScoreAnomalyDetector\nfrom signalflow.target.utils import mask_targets_by_signals\n\n# Detect anomalies\ndetector = ZScoreAnomalyDetector(threshold=4.0)\nsignals = detector.run(raw_data_view)\n\n# Mask labels overlapping with flash crashes\nlabeled_df = mask_targets_by_signals(\n    df=labeled_df,\n    signals=signals,\n    mask_signal_types={\"anomaly_low\"},  # flash crashes\n    horizon_bars=60,\n    cooldown_bars=60,\n)\n</code></pre> Note <ul> <li>Masking is done per-pair: signal at time T for pair A only masks   labels for pair A at affected timestamps.</li> <li>If signals DataFrame is empty or has no matching signal_types,   the input DataFrame is returned unchanged.</li> </ul> Source code in <code>src/signalflow/target/utils.py</code> <pre><code>def mask_targets_by_signals(\n    df: pl.DataFrame,\n    signals: Signals,\n    mask_signal_types: set[str],\n    horizon_bars: int,\n    cooldown_bars: int = 60,\n    target_columns: list[str] | None = None,\n    pair_col: str = \"pair\",\n    ts_col: str = \"timestamp\",\n) -&gt; pl.DataFrame:\n    \"\"\"Mask target columns for timestamps overlapping with specified signals.\n\n    For each signal at time T with type in mask_signal_types:\n    - Masks range [T - horizon_bars, T + cooldown_bars]\n\n    This is useful for excluding labels that overlap with exogenous events\n    (e.g., flash crashes, global market events) from training data, since\n    no feature could predict such events.\n\n    Args:\n        df: DataFrame with target columns.\n        signals: Signals object containing detected events.\n        mask_signal_types: Signal types to mask (e.g. {\"flash_crash\", \"global_event\"}).\n        horizon_bars: Forward horizon (bars before signal that \"see\" it).\n        cooldown_bars: Bars after signal to mask (default: 60).\n        target_columns: Columns to mask (default: all columns ending with \"_label\").\n        pair_col: Pair column name (default: \"pair\").\n        ts_col: Timestamp column name (default: \"timestamp\").\n\n    Returns:\n        DataFrame with affected target columns set to null.\n\n    Example:\n        ```python\n        from signalflow.detector import ZScoreAnomalyDetector\n        from signalflow.target.utils import mask_targets_by_signals\n\n        # Detect anomalies\n        detector = ZScoreAnomalyDetector(threshold=4.0)\n        signals = detector.run(raw_data_view)\n\n        # Mask labels overlapping with flash crashes\n        labeled_df = mask_targets_by_signals(\n            df=labeled_df,\n            signals=signals,\n            mask_signal_types={\"anomaly_low\"},  # flash crashes\n            horizon_bars=60,\n            cooldown_bars=60,\n        )\n        ```\n\n    Note:\n        - Masking is done per-pair: signal at time T for pair A only masks\n          labels for pair A at affected timestamps.\n        - If signals DataFrame is empty or has no matching signal_types,\n          the input DataFrame is returned unchanged.\n    \"\"\"\n    # Get events matching the specified signal types\n    signals_df = signals.value\n\n    if signals_df.height == 0:\n        return df\n\n    if \"signal_type\" not in signals_df.columns:\n        logger.warning(\"Signals DataFrame has no 'signal_type' column, returning unchanged\")\n        return df\n\n    # Filter to matching signal types\n    events = signals_df.filter(pl.col(\"signal_type\").is_in(list(mask_signal_types)))\n\n    if events.height == 0:\n        logger.debug(f\"No signals matching types {mask_signal_types}, returning unchanged\")\n        return df\n\n    # Determine target columns to mask\n    if target_columns is None:\n        target_columns = [c for c in df.columns if c.endswith(\"_label\")]\n\n    if not target_columns:\n        logger.warning(\"No target columns found to mask\")\n        return df\n\n    existing_cols = [c for c in target_columns if c in df.columns]\n    if not existing_cols:\n        logger.warning(f\"Target columns {target_columns} not found in DataFrame\")\n        return df\n\n    # Get all unique timestamps for efficient index lookup\n    all_timestamps = df.select([pair_col, ts_col]).unique().sort([pair_col, ts_col])\n\n    # Process per pair for correct masking\n    masked_rows: list[pl.DataFrame] = []\n\n    for pair_name in df.get_column(pair_col).unique().to_list():\n        pair_df = df.filter(pl.col(pair_col) == pair_name)\n        pair_events = events.filter(pl.col(pair_col) == pair_name)\n\n        if pair_events.height == 0:\n            masked_rows.append(pair_df)\n            continue\n\n        ts_array = pair_df.get_column(ts_col).to_numpy()\n        event_ts = pair_events.get_column(ts_col).to_numpy()\n\n        mask = np.zeros(len(ts_array), dtype=bool)\n\n        for evt in event_ts:\n            idx = np.searchsorted(ts_array, evt)\n            if idx &gt;= len(ts_array):\n                continue\n\n            start = max(0, idx - horizon_bars)\n            end = min(len(ts_array), idx + cooldown_bars + 1)\n            mask[start:end] = True\n\n        n_masked = int(mask.sum())\n        if n_masked &gt; 0:\n            logger.debug(f\"Masking {n_masked} timestamps for pair {pair_name}\")\n\n            # Create masked DataFrame\n            mask_series = pl.Series(\"_mask\", mask)\n            pair_df = (\n                pair_df.with_columns(mask_series)\n                .with_columns(\n                    [\n                        pl.when(pl.col(\"_mask\")).then(pl.lit(None)).otherwise(pl.col(col)).alias(col)\n                        for col in existing_cols\n                    ]\n                )\n                .drop(\"_mask\")\n            )\n\n        masked_rows.append(pair_df)\n\n    if not masked_rows:\n        return df\n\n    result = pl.concat(masked_rows, how=\"vertical_relaxed\")\n\n    total_events = events.height\n    logger.info(\n        f\"mask_targets_by_signals: masked around {total_events} events \"\n        f\"(types={mask_signal_types}, horizon={horizon_bars}, cooldown={cooldown_bars})\"\n    )\n\n    return result\n</code></pre>"},{"location":"api/labeler/#mask_targets_by_timestamps","title":"mask_targets_by_timestamps","text":""},{"location":"api/labeler/#signalflow.target.utils.mask_targets_by_timestamps","title":"signalflow.target.utils.mask_targets_by_timestamps","text":"<pre><code>mask_targets_by_timestamps(df: DataFrame, event_timestamps: list, horizon_bars: int, cooldown_bars: int = 60, target_columns: list[str] | None = None, ts_col: str = 'timestamp') -&gt; pl.DataFrame\n</code></pre> <p>Mask target columns for timestamps overlapping with event timestamps.</p> <p>Simpler version of mask_targets_by_signals that works with raw timestamps instead of Signals objects. Applies masking globally (not per-pair).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with target columns.</p> required <code>event_timestamps</code> <code>list</code> <p>List of event timestamps to mask around.</p> required <code>horizon_bars</code> <code>int</code> <p>Forward horizon (bars before event that \"see\" it).</p> required <code>cooldown_bars</code> <code>int</code> <p>Bars after event to mask (default: 60).</p> <code>60</code> <code>target_columns</code> <code>list[str] | None</code> <p>Columns to mask (default: all columns ending with \"_label\").</p> <code>None</code> <code>ts_col</code> <code>str</code> <p>Timestamp column name (default: \"timestamp\").</p> <code>'timestamp'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with affected target columns set to null.</p> Example <pre><code>from signalflow.target.utils import mask_targets_by_timestamps\nfrom datetime import datetime\n\n# Mask around known events\nlabeled_df = mask_targets_by_timestamps(\n    df=labeled_df,\n    event_timestamps=[\n        datetime(2024, 3, 1, 10, 30),  # Known flash crash\n        datetime(2024, 5, 15, 14, 0),  # Fed announcement\n    ],\n    horizon_bars=60,\n    cooldown_bars=120,\n)\n</code></pre> Source code in <code>src/signalflow/target/utils.py</code> <pre><code>def mask_targets_by_timestamps(\n    df: pl.DataFrame,\n    event_timestamps: list,\n    horizon_bars: int,\n    cooldown_bars: int = 60,\n    target_columns: list[str] | None = None,\n    ts_col: str = \"timestamp\",\n) -&gt; pl.DataFrame:\n    \"\"\"Mask target columns for timestamps overlapping with event timestamps.\n\n    Simpler version of mask_targets_by_signals that works with raw timestamps\n    instead of Signals objects. Applies masking globally (not per-pair).\n\n    Args:\n        df: DataFrame with target columns.\n        event_timestamps: List of event timestamps to mask around.\n        horizon_bars: Forward horizon (bars before event that \"see\" it).\n        cooldown_bars: Bars after event to mask (default: 60).\n        target_columns: Columns to mask (default: all columns ending with \"_label\").\n        ts_col: Timestamp column name (default: \"timestamp\").\n\n    Returns:\n        DataFrame with affected target columns set to null.\n\n    Example:\n        ```python\n        from signalflow.target.utils import mask_targets_by_timestamps\n        from datetime import datetime\n\n        # Mask around known events\n        labeled_df = mask_targets_by_timestamps(\n            df=labeled_df,\n            event_timestamps=[\n                datetime(2024, 3, 1, 10, 30),  # Known flash crash\n                datetime(2024, 5, 15, 14, 0),  # Fed announcement\n            ],\n            horizon_bars=60,\n            cooldown_bars=120,\n        )\n        ```\n    \"\"\"\n    if not event_timestamps:\n        return df\n\n    # Determine target columns to mask\n    if target_columns is None:\n        target_columns = [c for c in df.columns if c.endswith(\"_label\")]\n\n    if not target_columns:\n        logger.warning(\"No target columns found to mask\")\n        return df\n\n    existing_cols = [c for c in target_columns if c in df.columns]\n    if not existing_cols:\n        logger.warning(f\"Target columns {target_columns} not found in DataFrame\")\n        return df\n\n    ts_array = df.get_column(ts_col).to_numpy()\n    mask = np.zeros(len(ts_array), dtype=bool)\n\n    for evt in event_timestamps:\n        idx = np.searchsorted(ts_array, evt)\n        if idx &gt;= len(ts_array):\n            continue\n\n        start = max(0, idx - horizon_bars)\n        end = min(len(ts_array), idx + cooldown_bars + 1)\n        mask[start:end] = True\n\n    n_masked = int(mask.sum())\n    if n_masked == 0:\n        return df\n\n    logger.info(f\"mask_targets_by_timestamps: masked {n_masked} timestamps around {len(event_timestamps)} events\")\n\n    mask_series = pl.Series(\"_mask\", mask)\n    result = (\n        df.with_columns(mask_series)\n        .with_columns(\n            [pl.when(pl.col(\"_mask\")).then(pl.lit(None)).otherwise(pl.col(col)).alias(col) for col in existing_cols]\n        )\n        .drop(\"_mask\")\n    )\n\n    return result\n</code></pre>"},{"location":"api/strategy/","title":"Strategy Module","text":"<p>The strategy module provides components for backtesting and live trading execution.</p>"},{"location":"api/strategy/#architecture","title":"Architecture","text":"<pre><code>flowchart LR\n    A[Signals] --&gt; B[SignalAggregator]\n    B --&gt; C[EntryFilter]\n    C --&gt; D[PositionSizer]\n    D --&gt; E[Entry Rules]\n    E --&gt; F[Broker]\n    F --&gt; G[Exit Rules]\n\n    style B fill:#16a34a,stroke:#22c55e,color:#fff\n    style C fill:#ea580c,stroke:#f97316,color:#fff\n    style D fill:#2563eb,stroke:#3b82f6,color:#fff</code></pre>"},{"location":"api/strategy/#execution","title":"Execution","text":""},{"location":"api/strategy/#signalflow.strategy.runner.backtest_runner.BacktestRunner","title":"signalflow.strategy.runner.backtest_runner.BacktestRunner  <code>dataclass</code>","text":"<pre><code>BacktestRunner(strategy_id: str = 'backtest', broker: Any = None, entry_rules: list[EntryRule] = list(), exit_rules: list = list(), metrics: list[StrategyMetric] = list(), initial_capital: float = 10000.0, pair_col: str = 'pair', ts_col: str = 'timestamp', price_col: str = 'close', data_key: str = 'spot', show_progress: bool = True)\n</code></pre> <p>               Bases: <code>StrategyRunner</code></p>"},{"location":"api/strategy/#entry-rules","title":"Entry Rules","text":""},{"location":"api/strategy/#signalflow.strategy.component.entry.signal.SignalEntryRule","title":"signalflow.strategy.component.entry.signal.SignalEntryRule  <code>dataclass</code>","text":"<pre><code>SignalEntryRule(signal_type_map: dict[str, str] | None = None, position_sizer: PositionSizer | None = None, entry_filters: list[EntryFilter] | EntryFilter | None = None, base_position_size: float = 100.0, use_probability_sizing: bool = True, min_probability: float = 0.5, max_positions_per_pair: int = 1, max_total_positions: int = 20, allow_shorts: bool = False, max_capital_usage: float = 0.95, min_order_notional: float = 10.0, pair_col: str = 'pair', ts_col: str = 'timestamp', _composite_filter: CompositeEntryFilter | None = None)\n</code></pre> <p>               Bases: <code>EntryRule</code></p> <p>Signal-based entry rule with injectable sizer and filters.</p> <p>Converts signals to entry orders with configurable position sizing and pre-trade filtering.</p> <p>Parameters:</p> Name Type Description Default <code>position_sizer</code> <code>PositionSizer | None</code> <p>Optional PositionSizer for custom sizing logic.</p> <code>None</code> <code>entry_filters</code> <code>list[EntryFilter] | EntryFilter | None</code> <p>Optional list of EntryFilters for pre-trade validation.</p> <code>None</code> <code>base_position_size</code> <code>float</code> <p>Base notional value (used if no sizer provided).</p> <code>100.0</code> <code>use_probability_sizing</code> <code>bool</code> <p>Scale size by probability (legacy mode).</p> <code>True</code> <code>min_probability</code> <code>float</code> <p>Minimum signal probability.</p> <code>0.5</code> <code>max_positions_per_pair</code> <code>int</code> <p>Maximum concurrent positions per pair.</p> <code>1</code> <code>max_total_positions</code> <code>int</code> <p>Maximum total open positions.</p> <code>20</code> <code>allow_shorts</code> <code>bool</code> <p>Allow FALL signals to create short positions.</p> <code>False</code> <code>max_capital_usage</code> <code>float</code> <p>Maximum fraction of equity in positions.</p> <code>0.95</code> <code>min_order_notional</code> <code>float</code> <p>Minimum order size.</p> <code>10.0</code> <code>pair_col</code> <code>str</code> <p>Column name for pair in signals.</p> <code>'pair'</code> <code>ts_col</code> <code>str</code> <p>Column name for timestamp in signals.</p> <code>'timestamp'</code> Example"},{"location":"api/strategy/#signalflow.strategy.component.entry.signal.SignalEntryRule--with-custom-sizer-and-filters","title":"With custom sizer and filters","text":"<p>entry = SignalEntryRule( ...     position_sizer=FixedFractionSizer(fraction=0.02), ...     entry_filters=[ ...         DrawdownFilter(max_drawdown=0.10), ...         PriceDistanceFilter(min_distance_pct=0.02), ...     ], ...     max_positions_per_pair=5,  # For grid strategy ... )</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.signal.SignalEntryRule--legacy-mode-backward-compatible","title":"Legacy mode (backward compatible)","text":"<p>entry = SignalEntryRule( ...     base_position_size=100.0, ...     use_probability_sizing=True, ... )</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.signal.SignalEntryRule.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Normalize filters to composite.</p> Source code in <code>src/signalflow/strategy/component/entry/signal.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Normalize filters to composite.\"\"\"\n    if self.entry_filters is not None:\n        # Import here to avoid circular import\n        from signalflow.strategy.component.entry.filters import (\n            CompositeEntryFilter,\n            EntryFilter,\n        )\n\n        if isinstance(self.entry_filters, EntryFilter):\n            self._composite_filter = CompositeEntryFilter(filters=[self.entry_filters])\n        elif isinstance(self.entry_filters, list):\n            self._composite_filter = CompositeEntryFilter(filters=self.entry_filters)\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.component.entry.signal.SignalEntryRule.check_entries","title":"check_entries","text":"<pre><code>check_entries(signals: Signals, prices: dict[str, float], state: StrategyState) -&gt; list[Order]\n</code></pre> <p>Check signals and generate entry orders.</p> Source code in <code>src/signalflow/strategy/component/entry/signal.py</code> <pre><code>def check_entries(self, signals: Signals, prices: dict[str, float], state: StrategyState) -&gt; list[Order]:\n    \"\"\"Check signals and generate entry orders.\"\"\"\n    orders: list[Order] = []\n\n    if signals is None or signals.value.height == 0:\n        return orders\n\n    # Build positions map\n    positions_by_pair: dict[str, list[Position]] = {}\n    for pos in state.portfolio.open_positions():\n        positions_by_pair.setdefault(pos.pair, []).append(pos)\n\n    total_open = len(state.portfolio.open_positions())\n    if total_open &gt;= self.max_total_positions:\n        return orders\n\n    # Calculate capital limits\n    available_cash = state.portfolio.cash\n    used_capital = sum(pos.entry_price * pos.qty for pos in state.portfolio.open_positions())\n    total_equity = available_cash + used_capital\n    max_allowed_in_positions = total_equity * self.max_capital_usage\n    remaining_allocation = max_allowed_in_positions - used_capital\n\n    # Filter signals\n    df = self._filter_signals(signals.value)\n\n    for row in df.iter_rows(named=True):\n        if total_open &gt;= self.max_total_positions:\n            break\n        if remaining_allocation &lt;= self.min_order_notional:\n            break\n        if available_cash &lt;= self.min_order_notional:\n            break\n\n        pair = row[self.pair_col]\n        signal_type = row[\"signal_type\"]\n        probability = row.get(\"probability\", 1.0) or 1.0\n\n        # Check position limits\n        existing_positions = positions_by_pair.get(pair, [])\n        if len(existing_positions) &gt;= self.max_positions_per_pair:\n            continue\n\n        price = prices.get(pair)\n        if price is None or price &lt;= 0:\n            continue\n\n        # Determine side\n        side = self._determine_side(signal_type)\n        if side is None:\n            continue\n\n        # Build signal context\n        signal_ctx = SignalContext(\n            pair=pair,\n            signal_type=signal_type,\n            probability=probability,\n            price=price,\n            timestamp=row.get(self.ts_col),\n            meta=dict(row),\n        )\n\n        # === Apply filters ===\n        if self._composite_filter is not None:\n            allowed, _reason = self._composite_filter.allow_entry(signal_ctx, state, prices)\n            if not allowed:\n                continue\n\n        # === Compute size ===\n        if self.position_sizer is not None:\n            notional = self.position_sizer.compute_size(signal_ctx, state, prices)\n        else:\n            # Legacy sizing logic\n            notional = self._compute_legacy_size(probability)\n\n        # Apply capital constraints\n        notional = min(notional, available_cash * 0.99)\n        notional = min(notional, remaining_allocation)\n\n        if notional &lt; self.min_order_notional:\n            continue\n\n        qty = notional / price\n\n        order = Order(\n            pair=pair,\n            side=side,\n            order_type=\"MARKET\",\n            qty=qty,\n            signal_strength=probability,\n            meta={\n                \"signal_type\": signal_type,\n                \"signal_probability\": probability,\n                \"signal_ts\": row.get(self.ts_col),\n                \"requested_notional\": notional,\n                \"sizer_used\": (self.position_sizer.__class__.__name__ if self.position_sizer else \"legacy\"),\n            },\n        )\n        orders.append(order)\n\n        # Update tracking\n        total_open += 1\n        available_cash -= notional * 1.002\n        remaining_allocation -= notional\n        positions_by_pair.setdefault(pair, []).append(None)\n\n    return orders\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.component.entry.signal.SignalEntryRule.from_directional_map","title":"from_directional_map  <code>classmethod</code>","text":"<pre><code>from_directional_map(**kwargs) -&gt; SignalEntryRule\n</code></pre> <p>Create entry rule using the global DIRECTIONAL_SIGNAL_MAP.</p> Example <p>entry = SignalEntryRule.from_directional_map(base_position_size=200.0)</p> Source code in <code>src/signalflow/strategy/component/entry/signal.py</code> <pre><code>@classmethod\ndef from_directional_map(cls, **kwargs) -&gt; SignalEntryRule:\n    \"\"\"Create entry rule using the global DIRECTIONAL_SIGNAL_MAP.\n\n    Example:\n        &gt;&gt;&gt; entry = SignalEntryRule.from_directional_map(base_position_size=200.0)\n    \"\"\"\n    return cls(signal_type_map=dict(DIRECTIONAL_SIGNAL_MAP), **kwargs)\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.component.entry.fixed_size.FixedSizeEntryRule","title":"signalflow.strategy.component.entry.fixed_size.FixedSizeEntryRule  <code>dataclass</code>","text":"<pre><code>FixedSizeEntryRule(signal_type_map: dict[str, str] | None = None, position_size: float = 0.01, signal_types: list[str] = (lambda: [SignalType.RISE.value])(), max_positions: int = 10, pair_col: str = 'pair')\n</code></pre> <p>               Bases: <code>EntryRule</code></p> <p>Simple entry rule with fixed position size.</p> <p>Attributes:</p> Name Type Description <code>signal_type_map</code> <code>dict[str, str] | None</code> <p>Mapping signal_type -&gt; \"BUY\"/\"SELL\". When set, overrides <code>signal_types</code> for filtering and side determination. None = legacy behavior using <code>signal_types</code> list.</p> <code>signal_types</code> <code>list[str]</code> <p>Legacy list of actionable signal types (used when signal_type_map is None).</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.fixed_size.FixedSizeEntryRule.from_directional_map","title":"from_directional_map  <code>classmethod</code>","text":"<pre><code>from_directional_map(**kwargs) -&gt; FixedSizeEntryRule\n</code></pre> <p>Create entry rule using the global DIRECTIONAL_SIGNAL_MAP.</p> Source code in <code>src/signalflow/strategy/component/entry/fixed_size.py</code> <pre><code>@classmethod\ndef from_directional_map(cls, **kwargs) -&gt; FixedSizeEntryRule:\n    \"\"\"Create entry rule using the global DIRECTIONAL_SIGNAL_MAP.\"\"\"\n    return cls(signal_type_map=dict(DIRECTIONAL_SIGNAL_MAP), **kwargs)\n</code></pre>"},{"location":"api/strategy/#exit-rules","title":"Exit Rules","text":""},{"location":"api/strategy/#signalflow.strategy.component.exit.tp_sl.TakeProfitStopLossExit","title":"signalflow.strategy.component.exit.tp_sl.TakeProfitStopLossExit  <code>dataclass</code>","text":"<pre><code>TakeProfitStopLossExit(take_profit_pct: float = 0.02, stop_loss_pct: float = 0.01, use_position_levels: bool = False)\n</code></pre> <p>               Bases: <code>ExitRule</code></p> <p>Exit rule based on take-profit and stop-loss levels.</p> <p>Can use fixed percentages or dynamic levels from position meta.</p>"},{"location":"api/strategy/#position-sizing","title":"Position Sizing","text":"<p>Position sizers compute the notional value (in quote currency) for trades based on signal strength, portfolio state, and market conditions.</p>"},{"location":"api/strategy/#base-classes","title":"Base Classes","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.base.SignalContext","title":"signalflow.strategy.component.sizing.base.SignalContext  <code>dataclass</code>","text":"<pre><code>SignalContext(pair: str, signal_type: str, probability: float, price: float, timestamp: Any = None, meta: dict[str, Any] = dict())\n</code></pre> <p>Context for a single signal being sized.</p> <p>Provides all relevant information about a signal for sizing decisions.</p> <p>Attributes:</p> Name Type Description <code>pair</code> <code>str</code> <p>Trading pair (e.g., \"BTCUSDT\").</p> <code>signal_type</code> <code>str</code> <p>Signal direction (\"rise\", \"fall\", \"none\").</p> <code>probability</code> <code>float</code> <p>Signal confidence [0, 1].</p> <code>price</code> <code>float</code> <p>Current market price.</p> <code>timestamp</code> <code>Any</code> <p>Signal timestamp.</p> <code>meta</code> <code>dict[str, Any]</code> <p>Additional signal metadata from detector.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.base.PositionSizer","title":"signalflow.strategy.component.sizing.base.PositionSizer  <code>dataclass</code>","text":"<pre><code>PositionSizer()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for position sizing strategies.</p> <p>Computes the notional value (in quote currency) for a trade based on signal strength, portfolio state, and market conditions.</p> Design principles <ul> <li>Sizers compute NOTIONAL value, not quantity</li> <li>Quantity = notional / price (computed by entry rule)</li> <li>Sizers should be stateless where possible</li> <li>Historical data accessed via state.runtime or state.metrics</li> </ul> Example <p>sizer = FixedFractionSizer(fraction=0.02) notional = sizer.compute_size(signal_ctx, state, prices) qty = notional / prices[signal_ctx.pair]</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.base.PositionSizer.compute_size","title":"compute_size  <code>abstractmethod</code>","text":"<pre><code>compute_size(signal: SignalContext, state: StrategyState, prices: dict[str, float]) -&gt; float\n</code></pre> <p>Compute position size (notional value) for a signal.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>SignalContext</code> <p>Context about the signal being sized.</p> required <code>state</code> <code>StrategyState</code> <p>Current strategy state (portfolio, metrics, runtime).</p> required <code>prices</code> <code>dict[str, float]</code> <p>Current prices for all pairs.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Notional value in quote currency (e.g., USDT).</p> <code>float</code> <p>Return 0.0 to skip this signal.</p> Source code in <code>src/signalflow/strategy/component/sizing/base.py</code> <pre><code>@abstractmethod\ndef compute_size(\n    self,\n    signal: SignalContext,\n    state: StrategyState,\n    prices: dict[str, float],\n) -&gt; float:\n    \"\"\"Compute position size (notional value) for a signal.\n\n    Args:\n        signal: Context about the signal being sized.\n        state: Current strategy state (portfolio, metrics, runtime).\n        prices: Current prices for all pairs.\n\n    Returns:\n        Notional value in quote currency (e.g., USDT).\n        Return 0.0 to skip this signal.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/strategy/#available-sizers","title":"Available Sizers","text":""},{"location":"api/strategy/#fixedfractionsizer","title":"FixedFractionSizer","text":"<p>Allocate a fixed percentage of equity per trade.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.fixed_fraction.FixedFractionSizer","title":"signalflow.strategy.component.sizing.fixed_fraction.FixedFractionSizer  <code>dataclass</code>","text":"<pre><code>FixedFractionSizer(fraction: float = 0.02, min_notional: float = 10.0, max_notional: float = float('inf'))\n</code></pre> <p>               Bases: <code>PositionSizer</code></p> <p>Fixed percentage of equity per trade.</p> <p>Classic position sizing: risk a fixed fraction of current equity. Simple and consistent regardless of signal strength or volatility.</p> <p>Parameters:</p> Name Type Description Default <code>fraction</code> <code>float</code> <p>Fraction of equity to allocate (e.g., 0.02 = 2%).</p> <code>0.02</code> <code>min_notional</code> <code>float</code> <p>Minimum trade size (skip if below).</p> <code>10.0</code> <code>max_notional</code> <code>float</code> <p>Maximum trade size cap.</p> <code>float('inf')</code> Example <p>sizer = FixedFractionSizer(fraction=0.02)  # 2% per trade</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.fixed_fraction.FixedFractionSizer--with-10000-equity-notional-200","title":"With $10,000 equity: notional = $200","text":""},{"location":"api/strategy/#signalstrengthsizer","title":"SignalStrengthSizer","text":"<p>Scale position size by signal probability.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.signal_strength.SignalStrengthSizer","title":"signalflow.strategy.component.sizing.signal_strength.SignalStrengthSizer  <code>dataclass</code>","text":"<pre><code>SignalStrengthSizer(base_size: float = 100.0, min_probability: float = 0.5, scale_factor: float = 1.0, min_notional: float = 10.0, max_notional: float = float('inf'))\n</code></pre> <p>               Bases: <code>PositionSizer</code></p> <p>Size proportional to signal probability/strength.</p> <p>Higher confidence signals get larger positions. Essentially the current SignalEntryRule behavior extracted.</p> <p>Parameters:</p> Name Type Description Default <code>base_size</code> <code>float</code> <p>Base notional value.</p> <code>100.0</code> <code>min_probability</code> <code>float</code> <p>Skip signals below this threshold.</p> <code>0.5</code> <code>scale_factor</code> <code>float</code> <p>Multiplier for probability-based scaling.</p> <code>1.0</code> <code>min_notional</code> <code>float</code> <p>Minimum trade size.</p> <code>10.0</code> <code>max_notional</code> <code>float</code> <p>Maximum trade size.</p> <code>float('inf')</code> Example <p>sizer = SignalStrengthSizer(base_size=100.0)</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.signal_strength.SignalStrengthSizer--signal-with-probability08-notional-80","title":"Signal with probability=0.8 -&gt; notional = 80","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.signal_strength.SignalStrengthSizer--signal-with-probability05-notional-50","title":"Signal with probability=0.5 -&gt; notional = 50","text":""},{"location":"api/strategy/#kellycriterionsizer","title":"KellyCriterionSizer","text":"<p>Optimal sizing using the Kelly Criterion formula.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.kelly.KellyCriterionSizer","title":"signalflow.strategy.component.sizing.kelly.KellyCriterionSizer  <code>dataclass</code>","text":"<pre><code>KellyCriterionSizer(kelly_fraction: float = 0.5, min_trades_for_stats: int = 30, default_win_rate: float = 0.5, default_payoff_ratio: float = 1.0, use_signal_probability: bool = True, min_notional: float = 10.0, max_fraction: float = 0.25)\n</code></pre> <p>               Bases: <code>PositionSizer</code></p> <p>Kelly Criterion position sizing.</p> <p>Formula: f* = (p * b - q) / b Where:     p = win probability (from signal or historical)     q = 1 - p (loss probability)     b = win/loss ratio (payoff ratio)</p> <p>Half-Kelly (kelly_fraction=0.5) is recommended for practical use to reduce volatility while capturing most of the edge.</p> <p>Parameters:</p> Name Type Description Default <code>kelly_fraction</code> <code>float</code> <p>Fraction of Kelly to use (0.5 = half-Kelly recommended).</p> <code>0.5</code> <code>min_trades_for_stats</code> <code>int</code> <p>Minimum closed trades before using historical stats.</p> <code>30</code> <code>default_win_rate</code> <code>float</code> <p>Fallback win rate if insufficient history.</p> <code>0.5</code> <code>default_payoff_ratio</code> <code>float</code> <p>Fallback payoff ratio if insufficient history.</p> <code>1.0</code> <code>use_signal_probability</code> <code>bool</code> <p>Use signal.probability as win rate proxy.</p> <code>True</code> <code>min_notional</code> <code>float</code> <p>Minimum trade size.</p> <code>10.0</code> <code>max_fraction</code> <code>float</code> <p>Maximum fraction of equity (safety cap).</p> <code>0.25</code> Example <p>sizer = KellyCriterionSizer(kelly_fraction=0.5)  # Half-Kelly</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.kelly.KellyCriterionSizer--with-60-win-rate-and-151-payoff-ratio","title":"With 60% win rate and 1.5:1 payoff ratio:","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.kelly.KellyCriterionSizer--full-kelly-f-06-15-04-15-0333","title":"Full Kelly f* = (0.6 * 1.5 - 0.4) / 1.5 = 0.333","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.kelly.KellyCriterionSizer--half-kelly-0167-167-of-equity","title":"Half Kelly = 0.167 = 16.7% of equity","text":""},{"location":"api/strategy/#volatilitytargetsizer","title":"VolatilityTargetSizer","text":"<p>Size positions to achieve target volatility contribution.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.volatility_target.VolatilityTargetSizer","title":"signalflow.strategy.component.sizing.volatility_target.VolatilityTargetSizer  <code>dataclass</code>","text":"<pre><code>VolatilityTargetSizer(target_volatility: float = 0.01, volatility_source: str = 'atr', default_volatility_pct: float = 0.02, min_notional: float = 10.0, max_fraction: float = 0.2)\n</code></pre> <p>               Bases: <code>PositionSizer</code></p> <p>Target specific portfolio volatility per position.</p> <p>Sizes positions to contribute equal volatility to the portfolio. Smaller positions in volatile assets, larger in stable ones.</p> <p>Formula: notional = (target_vol * equity) / asset_vol_pct</p> <p>Parameters:</p> Name Type Description Default <code>target_volatility</code> <code>float</code> <p>Target contribution to portfolio vol (e.g., 0.01 = 1%).</p> <code>0.01</code> <code>volatility_source</code> <code>str</code> <p>Key in state.runtime for ATR/volatility data.</p> <code>'atr'</code> <code>default_volatility_pct</code> <code>float</code> <p>Default volatility if ATR not available.</p> <code>0.02</code> <code>min_notional</code> <code>float</code> <p>Minimum trade size.</p> <code>10.0</code> <code>max_fraction</code> <code>float</code> <p>Maximum fraction of equity per position.</p> <code>0.2</code> Example <p>sizer = VolatilityTargetSizer(target_volatility=0.01)</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.volatility_target.VolatilityTargetSizer--asset-with-2-daily-vol-50-of-target-allocation","title":"Asset with 2% daily vol -&gt; 50% of target allocation","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.volatility_target.VolatilityTargetSizer--asset-with-05-daily-vol-200-of-target-allocation-capped","title":"Asset with 0.5% daily vol -&gt; 200% of target allocation (capped)","text":""},{"location":"api/strategy/#riskparitysizer","title":"RiskParitySizer","text":"<p>Equal risk contribution across positions.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.risk_parity.RiskParitySizer","title":"signalflow.strategy.component.sizing.risk_parity.RiskParitySizer  <code>dataclass</code>","text":"<pre><code>RiskParitySizer(target_positions: int = 10, volatility_source: str = 'atr', default_volatility_pct: float = 0.02, min_notional: float = 10.0)\n</code></pre> <p>               Bases: <code>PositionSizer</code></p> <p>Equal risk contribution across all positions.</p> <p>Allocates capital so each position contributes equally to portfolio risk, accounting for existing positions and their volatilities.</p> <p>Parameters:</p> Name Type Description Default <code>target_positions</code> <code>int</code> <p>Target number of equal-risk positions.</p> <code>10</code> <code>volatility_source</code> <code>str</code> <p>Key in state.runtime for volatility data.</p> <code>'atr'</code> <code>default_volatility_pct</code> <code>float</code> <p>Default volatility if not available.</p> <code>0.02</code> <code>min_notional</code> <code>float</code> <p>Minimum trade size.</p> <code>10.0</code> Example <p>sizer = RiskParitySizer(target_positions=10)</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.risk_parity.RiskParitySizer--each-position-should-contribute-10-of-total-risk-budget","title":"Each position should contribute 10% of total risk budget","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.risk_parity.RiskParitySizer--high-vol-assets-get-smaller-notional-allocation","title":"High-vol assets get smaller notional allocation","text":""},{"location":"api/strategy/#martingalesizer","title":"MartingaleSizer","text":"<p>Grid/DCA strategy with increasing position sizes.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.martingale.MartingaleSizer","title":"signalflow.strategy.component.sizing.martingale.MartingaleSizer  <code>dataclass</code>","text":"<pre><code>MartingaleSizer(base_size: float = 100.0, multiplier: float = 1.5, max_grid_levels: int = 5, max_notional: float = float('inf'), min_notional: float = 10.0)\n</code></pre> <p>               Bases: <code>PositionSizer</code></p> <p>Martingale position sizing for grid strategies.</p> <p>Increases position size with each grid level filled. Useful for DCA (Dollar Cost Averaging) and grid trading strategies.</p> <p>Formula: notional = base_size * (multiplier ^ grid_level)</p> <p>Where grid_level = number of existing open positions in the same pair.</p> <p>Parameters:</p> Name Type Description Default <code>base_size</code> <code>float</code> <p>Initial position size for first grid level.</p> <code>100.0</code> <code>multiplier</code> <code>float</code> <p>Size multiplier per level (e.g., 1.5 = 50% increase).</p> <code>1.5</code> <code>max_grid_levels</code> <code>int</code> <p>Maximum number of grid levels to fill.</p> <code>5</code> <code>max_notional</code> <code>float</code> <p>Maximum position size cap.</p> <code>float('inf')</code> <code>min_notional</code> <code>float</code> <p>Minimum trade size.</p> <code>10.0</code> Example <p>sizer = MartingaleSizer(base_size=100, multiplier=1.5)</p> Warning <p>Martingale can lead to large losses in trending markets. Always use with appropriate risk limits and max_grid_levels.</p>"},{"location":"api/strategy/#signalflow.strategy.component.sizing.martingale.MartingaleSizer--level-0-100","title":"Level 0: $100","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.martingale.MartingaleSizer--level-1-150","title":"Level 1: $150","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.martingale.MartingaleSizer--level-2-225","title":"Level 2: $225","text":""},{"location":"api/strategy/#signalflow.strategy.component.sizing.martingale.MartingaleSizer--level-3-33750","title":"Level 3: $337.50","text":""},{"location":"api/strategy/#usage-example","title":"Usage Example","text":"<pre><code>from signalflow.strategy.component.sizing import (\n    FixedFractionSizer,\n    KellyCriterionSizer,\n    VolatilityTargetSizer,\n    SignalContext,\n)\nfrom signalflow.core import StrategyState\n\n# Create test state\nstate = StrategyState(strategy_id=\"demo\")\nstate.portfolio.cash = 10_000.0\nprices = {\"BTCUSDT\": 50000.0}\n\n# Signal context\nsignal = SignalContext(\n    pair=\"BTCUSDT\",\n    signal_type=\"rise\",\n    probability=0.75,\n    price=50000.0,\n)\n\n# Fixed 2% of equity per trade\nsizer = FixedFractionSizer(fraction=0.02)\nnotional = sizer.compute_size(signal, state, prices)  # $200\n\n# Half-Kelly sizing\nkelly = KellyCriterionSizer(kelly_fraction=0.5, default_win_rate=0.55)\nnotional = kelly.compute_size(signal, state, prices)\n\n# Volatility targeting (requires ATR in state.runtime)\nstate.runtime[\"atr\"] = {\"BTCUSDT\": 1000.0}  # 2% ATR\nvol_sizer = VolatilityTargetSizer(target_volatility=0.01)\nnotional = vol_sizer.compute_size(signal, state, prices)\n</code></pre>"},{"location":"api/strategy/#entry-filters","title":"Entry Filters","text":"<p>Entry filters provide pre-trade validation to improve signal quality. All filters return <code>(allowed: bool, reason: str)</code> tuples.</p>"},{"location":"api/strategy/#base-classes_1","title":"Base Classes","text":""},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.EntryFilter","title":"signalflow.strategy.component.entry.filters.EntryFilter  <code>dataclass</code>","text":"<pre><code>EntryFilter()\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for entry filters.</p> <p>Filters determine whether a signal should be acted upon. All filters must pass (AND logic) for entry to proceed.</p> Design principles <ul> <li>Filters are binary (allow/reject)</li> <li>Should provide rejection reason for debugging</li> <li>Can be composed via CompositeEntryFilter</li> </ul>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.EntryFilter.allow_entry","title":"allow_entry  <code>abstractmethod</code>","text":"<pre><code>allow_entry(signal: SignalContext, state: StrategyState, prices: dict[str, float]) -&gt; tuple[bool, str]\n</code></pre> <p>Check if entry is allowed.</p> <p>Parameters:</p> Name Type Description Default <code>signal</code> <code>SignalContext</code> <p>Signal context.</p> required <code>state</code> <code>StrategyState</code> <p>Strategy state.</p> required <code>prices</code> <code>dict[str, float]</code> <p>Current prices.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>Tuple of (allowed, reason).</p> <code>str</code> <p>reason is empty string if allowed, else rejection reason.</p> Source code in <code>src/signalflow/strategy/component/entry/filters.py</code> <pre><code>@abstractmethod\ndef allow_entry(\n    self,\n    signal: SignalContext,\n    state: StrategyState,\n    prices: dict[str, float],\n) -&gt; tuple[bool, str]:\n    \"\"\"Check if entry is allowed.\n\n    Args:\n        signal: Signal context.\n        state: Strategy state.\n        prices: Current prices.\n\n    Returns:\n        Tuple of (allowed, reason).\n        reason is empty string if allowed, else rejection reason.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.CompositeEntryFilter","title":"signalflow.strategy.component.entry.filters.CompositeEntryFilter  <code>dataclass</code>","text":"<pre><code>CompositeEntryFilter(filters: list[EntryFilter] = list(), require_all: bool = True)\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Combines multiple entry filters.</p> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>list[EntryFilter]</code> <p>List of filters to apply.</p> <code>list()</code> <code>require_all</code> <code>bool</code> <p>If True (default), all must pass. If False, any can pass.</p> <code>True</code> Example <p>composite = CompositeEntryFilter( ...     filters=[DrawdownFilter(max_drawdown=0.10), VolatilityFilter()], ...     require_all=True ... )</p>"},{"location":"api/strategy/#available-filters","title":"Available Filters","text":""},{"location":"api/strategy/#regimefilter","title":"RegimeFilter","text":"<p>Only enter when market regime matches signal direction.</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.RegimeFilter","title":"signalflow.strategy.component.entry.filters.RegimeFilter  <code>dataclass</code>","text":"<pre><code>RegimeFilter(signal_regime_map: dict[str, str] | None = None, regime_key: str = 'regime', allowed_regimes_bullish: list[str] = (lambda: ['trend_up', 'mean_reversion_oversold'])(), allowed_regimes_bearish: list[str] = (lambda: ['trend_down', 'mean_reversion_overbought'])())\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Filter entries based on market regime.</p> <p>Only allow entries when market regime matches signal type: - Bullish signals in trend-up or mean-reversion-oversold regimes - Bearish signals in trend-down or mean-reversion-overbought regimes</p> <p>Regime detected via state.runtime[\"regime\"][pair] or global regime.</p> <p>Parameters:</p> Name Type Description Default <code>signal_regime_map</code> <code>dict[str, str] | None</code> <p>Mapping signal_type -&gt; \"bullish\"/\"bearish\". When set, overrides legacy \"rise\"/\"fall\" hardcoding. None = legacy behavior (only \"rise\" and \"fall\" are regime-checked).</p> <code>None</code> <code>regime_key</code> <code>str</code> <p>Key in state.runtime for regime data.</p> <code>'regime'</code> <code>allowed_regimes_bullish</code> <code>list[str]</code> <p>Regimes allowing bullish entries.</p> <code>(lambda: ['trend_up', 'mean_reversion_oversold'])()</code> <code>allowed_regimes_bearish</code> <code>list[str]</code> <p>Regimes allowing bearish entries.</p> <code>(lambda: ['trend_down', 'mean_reversion_overbought'])()</code>"},{"location":"api/strategy/#volatilityfilter","title":"VolatilityFilter","text":"<p>Skip entries in extreme volatility conditions.</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.VolatilityFilter","title":"signalflow.strategy.component.entry.filters.VolatilityFilter  <code>dataclass</code>","text":"<pre><code>VolatilityFilter(volatility_key: str = 'atr', min_volatility: float = 0.0, max_volatility: float = float('inf'), use_relative: bool = True)\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Skip entries in extreme volatility conditions.</p> <p>Parameters:</p> Name Type Description Default <code>volatility_key</code> <code>str</code> <p>Key in state.runtime for volatility data (default: \"atr\").</p> <code>'atr'</code> <code>min_volatility</code> <code>float</code> <p>Minimum relative volatility to allow entry.</p> <code>0.0</code> <code>max_volatility</code> <code>float</code> <p>Maximum relative volatility to allow entry.</p> <code>float('inf')</code> <code>use_relative</code> <code>bool</code> <p>If True, compare vol/price ratio instead of absolute.</p> <code>True</code>"},{"location":"api/strategy/#drawdownfilter","title":"DrawdownFilter","text":"<p>Pause trading after significant drawdown.</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.DrawdownFilter","title":"signalflow.strategy.component.entry.filters.DrawdownFilter  <code>dataclass</code>","text":"<pre><code>DrawdownFilter(max_drawdown: float = 0.1, recovery_threshold: float = 0.05, drawdown_key: str = 'current_drawdown', _paused: bool = False)\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Pause trading after significant drawdown.</p> <p>Parameters:</p> Name Type Description Default <code>max_drawdown</code> <code>float</code> <p>Maximum drawdown before pausing (e.g., 0.10 = 10%).</p> <code>0.1</code> <code>recovery_threshold</code> <code>float</code> <p>Resume when drawdown reduces to this level.</p> <code>0.05</code> <code>drawdown_key</code> <code>str</code> <p>Key in state.metrics for current drawdown.</p> <code>'current_drawdown'</code>"},{"location":"api/strategy/#correlationfilter","title":"CorrelationFilter","text":"<p>Avoid concentrated positions in correlated assets.</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.CorrelationFilter","title":"signalflow.strategy.component.entry.filters.CorrelationFilter  <code>dataclass</code>","text":"<pre><code>CorrelationFilter(correlation_key: str = 'correlations', max_correlation: float = 0.7, max_correlated_positions: int = 2)\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Avoid concentrated positions in correlated assets.</p> <p>Rejects entry if already holding highly correlated assets.</p> <p>Parameters:</p> Name Type Description Default <code>correlation_key</code> <code>str</code> <p>Key in state.runtime for correlation matrix.</p> <code>'correlations'</code> <code>max_correlation</code> <code>float</code> <p>Maximum allowed correlation with existing positions.</p> <code>0.7</code> <code>max_correlated_positions</code> <code>int</code> <p>Max positions in correlated group.</p> <code>2</code>"},{"location":"api/strategy/#timeofdayfilter","title":"TimeOfDayFilter","text":"<p>Restrict trading to specific hours.</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.TimeOfDayFilter","title":"signalflow.strategy.component.entry.filters.TimeOfDayFilter  <code>dataclass</code>","text":"<pre><code>TimeOfDayFilter(allowed_hours: list[int] | None = None, blocked_hours: list[int] | None = None)\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Restrict trading to specific hours.</p> <p>Parameters:</p> Name Type Description Default <code>allowed_hours</code> <code>list[int] | None</code> <p>List of hours (0-23) when trading is allowed.</p> <code>None</code> <code>blocked_hours</code> <code>list[int] | None</code> <p>List of hours (0-23) when trading is blocked.</p> <code>None</code> <p>Note: If both are None, all hours are allowed.</p>"},{"location":"api/strategy/#pricedistancefilter","title":"PriceDistanceFilter","text":"<p>Filter entries based on price distance from existing positions.</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.PriceDistanceFilter","title":"signalflow.strategy.component.entry.filters.PriceDistanceFilter  <code>dataclass</code>","text":"<pre><code>PriceDistanceFilter(signal_direction_map: dict[str, str] | None = None, min_distance_pct: float = 0.02, direction_aware: bool = True)\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Filter entries based on price distance from existing positions.</p> <p>For grid strategies: prevents buying when price is too close to existing positions in the same pair.</p> <p>Parameters:</p> Name Type Description Default <code>signal_direction_map</code> <code>dict[str, str] | None</code> <p>Mapping signal_type -&gt; \"long\"/\"short\". When set, overrides legacy \"rise\"/\"fall\" hardcoding. None = legacy behavior (only \"rise\" and \"fall\" are direction-aware).</p> <code>None</code> <code>min_distance_pct</code> <code>float</code> <p>Minimum price difference as percentage (e.g., 0.02 = 2%).</p> <code>0.02</code> <code>direction_aware</code> <code>bool</code> <p>If True, check distance based on position direction. - LONG: new entry must be below existing entry by min_distance_pct - SHORT: new entry must be above existing entry by min_distance_pct If False, check absolute distance in either direction.</p> <code>True</code> Example"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.PriceDistanceFilter--grid-strategy-only-buy-when-price-drops-2-from-last-position","title":"Grid strategy: only buy when price drops 2% from last position","text":"<p>filter = PriceDistanceFilter(min_distance_pct=0.02, direction_aware=True)</p>"},{"location":"api/strategy/#signalaccuracyfilter","title":"SignalAccuracyFilter","text":"<p>Pause trading when signal accuracy drops below threshold.</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.SignalAccuracyFilter","title":"signalflow.strategy.component.entry.filters.SignalAccuracyFilter  <code>dataclass</code>","text":"<pre><code>SignalAccuracyFilter(accuracy_key: str = 'signal_accuracy', min_accuracy: float = 0.45, min_samples: int = 20, window_key: str | None = None)\n</code></pre> <p>               Bases: <code>EntryFilter</code></p> <p>Filter based on real-time signal accuracy metrics.</p> <p>Tracks detector/model accuracy and pauses trading when accuracy drops. Useful for detecting model degradation or regime changes.</p> <p>Parameters:</p> Name Type Description Default <code>accuracy_key</code> <code>str</code> <p>Key in state.runtime for accuracy data.</p> <code>'signal_accuracy'</code> <code>min_accuracy</code> <code>float</code> <p>Minimum required accuracy to allow entry.</p> <code>0.45</code> <code>min_samples</code> <code>int</code> <p>Minimum samples before applying filter.</p> <code>20</code> <code>window_key</code> <code>str | None</code> <p>Optional key for accuracy over recent window only.</p> <code>None</code> Example"},{"location":"api/strategy/#signalflow.strategy.component.entry.filters.SignalAccuracyFilter--pause-if-recent-signal-accuracy-drops-below-45","title":"Pause if recent signal accuracy drops below 45%","text":"<p>filter = SignalAccuracyFilter(min_accuracy=0.45, min_samples=20)</p>"},{"location":"api/strategy/#usage-example_1","title":"Usage Example","text":"<pre><code>from signalflow.strategy.component.entry import (\n    CompositeEntryFilter,\n    DrawdownFilter,\n    RegimeFilter,\n    VolatilityFilter,\n    TimeOfDayFilter,\n)\n\n# Combine multiple filters (AND logic)\ncomposite = CompositeEntryFilter(\n    filters=[\n        DrawdownFilter(max_drawdown=0.10, recovery_threshold=0.05),\n        RegimeFilter(),\n        VolatilityFilter(max_volatility=0.03),\n        TimeOfDayFilter(blocked_hours=[0, 1, 2, 3]),  # Skip overnight\n    ],\n    require_all=True,  # All must pass\n)\n\nallowed, reason = composite.allow_entry(signal, state, prices)\nif not allowed:\n    print(f\"Entry rejected: {reason}\")\n</code></pre>"},{"location":"api/strategy/#signal-aggregation","title":"Signal Aggregation","text":"<p>Combine signals from multiple detectors using voting or weighting logic.</p>"},{"location":"api/strategy/#votingmode","title":"VotingMode","text":"Mode Description <code>MAJORITY</code> Most common signal type wins (requires min_agreement) <code>WEIGHTED</code> Weighted average of probabilities <code>UNANIMOUS</code> All detectors must agree <code>ANY</code> Any non-NONE signal passes (highest probability wins) <code>META_LABELING</code> Detector direction \u00d7 validator probability"},{"location":"api/strategy/#signalflow.strategy.component.entry.aggregation.VotingMode","title":"signalflow.strategy.component.entry.aggregation.VotingMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Signal aggregation voting modes.</p>"},{"location":"api/strategy/#signalaggregator","title":"SignalAggregator","text":""},{"location":"api/strategy/#signalflow.strategy.component.entry.aggregation.SignalAggregator","title":"signalflow.strategy.component.entry.aggregation.SignalAggregator  <code>dataclass</code>","text":"<pre><code>SignalAggregator(voting_mode: VotingMode = VotingMode.MAJORITY, min_agreement: float = 0.5, weights: list[float] | None = None, probability_threshold: float = 0.5, pair_col: str = 'pair', ts_col: str = 'timestamp')\n</code></pre> <p>Combine signals from multiple detectors.</p> <p>Aggregates multiple Signals DataFrames into one based on voting/weighting logic.</p> <p>Parameters:</p> Name Type Description Default <code>voting_mode</code> <code>VotingMode</code> <p>How to combine signals (see VotingMode).</p> <code>MAJORITY</code> <code>min_agreement</code> <code>float</code> <p>Minimum fraction of detectors agreeing (for MAJORITY).</p> <code>0.5</code> <code>weights</code> <code>list[float] | None</code> <p>Optional weights per detector (for WEIGHTED mode).</p> <code>None</code> <code>probability_threshold</code> <code>float</code> <p>Minimum combined probability to emit signal.</p> <code>0.5</code> <code>pair_col</code> <code>str</code> <p>Column name for pair.</p> <code>'pair'</code> <code>ts_col</code> <code>str</code> <p>Column name for timestamp.</p> <code>'timestamp'</code> Example"},{"location":"api/strategy/#signalflow.strategy.component.entry.aggregation.SignalAggregator--majority-voting","title":"Majority voting","text":"<p>aggregator = SignalAggregator(voting_mode=VotingMode.MAJORITY) combined = aggregator.aggregate([signals1, signals2, signals3])</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.aggregation.SignalAggregator--meta-labeling-detector-direction-validator-confidence","title":"Meta-labeling: detector direction * validator confidence","text":"<p>aggregator = SignalAggregator(voting_mode=VotingMode.META_LABELING) combined = aggregator.aggregate([detector_signals, validator_signals])</p>"},{"location":"api/strategy/#signalflow.strategy.component.entry.aggregation.SignalAggregator.__call__","title":"__call__","text":"<pre><code>__call__(signals_list: list[Signals], detector_names: list[str] | None = None) -&gt; Signals\n</code></pre> <p>Alias for aggregate().</p> Source code in <code>src/signalflow/strategy/component/entry/aggregation.py</code> <pre><code>def __call__(\n    self,\n    signals_list: list[Signals],\n    detector_names: list[str] | None = None,\n) -&gt; Signals:\n    \"\"\"Alias for aggregate().\"\"\"\n    return self.aggregate(signals_list, detector_names)\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.component.entry.aggregation.SignalAggregator.aggregate","title":"aggregate","text":"<pre><code>aggregate(signals_list: list[Signals], detector_names: list[str] | None = None) -&gt; Signals\n</code></pre> <p>Aggregate multiple signal sources into one.</p> <p>Parameters:</p> Name Type Description Default <code>signals_list</code> <code>list[Signals]</code> <p>List of Signals from different detectors.</p> required <code>detector_names</code> <code>list[str] | None</code> <p>Optional names for tracing (len must match signals_list).</p> <code>None</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Aggregated Signals DataFrame.</p> Source code in <code>src/signalflow/strategy/component/entry/aggregation.py</code> <pre><code>def aggregate(\n    self,\n    signals_list: list[Signals],\n    detector_names: list[str] | None = None,\n) -&gt; Signals:\n    \"\"\"Aggregate multiple signal sources into one.\n\n    Args:\n        signals_list: List of Signals from different detectors.\n        detector_names: Optional names for tracing (len must match signals_list).\n\n    Returns:\n        Aggregated Signals DataFrame.\n    \"\"\"\n    if not signals_list:\n        return Signals(pl.DataFrame())\n\n    if len(signals_list) == 1:\n        return signals_list[0]\n\n    if self.voting_mode == VotingMode.MAJORITY:\n        return self._aggregate_majority(signals_list)\n    elif self.voting_mode == VotingMode.WEIGHTED:\n        return self._aggregate_weighted(signals_list)\n    elif self.voting_mode == VotingMode.UNANIMOUS:\n        return self._aggregate_unanimous(signals_list)\n    elif self.voting_mode == VotingMode.ANY:\n        return self._aggregate_any(signals_list)\n    elif self.voting_mode == VotingMode.META_LABELING:\n        return self._aggregate_meta_labeling(signals_list)\n    else:\n        raise ValueError(f\"Unknown voting mode: {self.voting_mode}\")\n</code></pre>"},{"location":"api/strategy/#usage-examples","title":"Usage Examples","text":"<pre><code>from signalflow.strategy.component.entry import SignalAggregator, VotingMode\n\n# Majority voting: signal needs &gt;50% agreement\naggregator = SignalAggregator(\n    voting_mode=VotingMode.MAJORITY,\n    min_agreement=0.5,\n)\ncombined = aggregator.aggregate([signals_1, signals_2, signals_3])\n\n# Weighted voting with custom weights\naggregator = SignalAggregator(\n    voting_mode=VotingMode.WEIGHTED,\n    weights=[2.0, 1.0, 1.0],  # First detector weighted 2x\n    probability_threshold=0.6,\n)\ncombined = aggregator.aggregate([primary_signals, secondary_1, secondary_2])\n\n# Meta-labeling: detector direction \u00d7 validator confidence\naggregator = SignalAggregator(\n    voting_mode=VotingMode.META_LABELING,\n    probability_threshold=0.5,\n)\ncombined = aggregator.aggregate([detector_signals, validator_signals])\n# Combined probability = detector_prob * validator_prob\n\n# Unanimous: all must agree for high-conviction trades\naggregator = SignalAggregator(\n    voting_mode=VotingMode.UNANIMOUS,\n    probability_threshold=0.7,\n)\ncombined = aggregator.aggregate([detector_1, detector_2, detector_3])\n</code></pre>"},{"location":"api/strategy/#integration-with-signalentryrule","title":"Integration with SignalEntryRule","text":"<p>Position sizers and entry filters can be injected into <code>SignalEntryRule</code>:</p> <pre><code>from signalflow.strategy.runner import BacktestRunner\nfrom signalflow.strategy.component.entry import (\n    SignalEntryRule,\n    CompositeEntryFilter,\n    DrawdownFilter,\n    RegimeFilter,\n)\nfrom signalflow.strategy.component.sizing import KellyCriterionSizer\nfrom signalflow.strategy.component.exit import TakeProfitStopLossExit\nfrom signalflow.strategy.broker import BacktestBroker\nfrom signalflow.strategy.broker.executor import VirtualSpotExecutor\n\n# Create advanced entry rule\nentry_rule = SignalEntryRule(\n    position_sizer=KellyCriterionSizer(kelly_fraction=0.5),\n    entry_filters=CompositeEntryFilter(\n        filters=[\n            DrawdownFilter(max_drawdown=0.10),\n            RegimeFilter(),\n        ],\n    ),\n)\n\n# Run backtest\nrunner = BacktestRunner(\n    strategy_id=\"advanced_strategy\",\n    broker=BacktestBroker(executor=VirtualSpotExecutor(fee_rate=0.001)),\n    entry_rules=[entry_rule],\n    exit_rules=[TakeProfitStopLossExit(take_profit_pct=0.02, stop_loss_pct=0.01)],\n    initial_capital=10_000.0,\n)\n\nstate = runner.run(raw_data=raw_data, signals=signals)\n</code></pre>"},{"location":"api/strategy/#grid-trading-example","title":"Grid Trading Example","text":"<p>Combine <code>MartingaleSizer</code> with <code>PriceDistanceFilter</code> for grid strategies:</p> <pre><code>from signalflow.strategy.component.entry import (\n    SignalEntryRule,\n    PriceDistanceFilter,\n)\nfrom signalflow.strategy.component.sizing import MartingaleSizer\n\n# Grid strategy: buy more as price drops\nentry_rule = SignalEntryRule(\n    position_sizer=MartingaleSizer(\n        base_size=100.0,      # Start with $100\n        multiplier=1.5,       # Increase 50% per level\n        max_grid_levels=5,    # Max 5 levels\n    ),\n    entry_filters=PriceDistanceFilter(\n        min_distance_pct=0.02,  # 2% price drop between levels\n        direction_aware=True,\n    ),\n    max_positions_per_pair=5,\n)\n</code></pre>"},{"location":"api/strategy/#data-sources","title":"Data Sources","text":"<p>Components access data through <code>StrategyState</code>:</p> Component Data Source Key <code>VolatilityTargetSizer</code> ATR values <code>state.runtime[\"atr\"]</code> <code>RiskParitySizer</code> ATR values <code>state.runtime[\"atr\"]</code> <code>DrawdownFilter</code> Current drawdown <code>state.metrics[\"current_drawdown\"]</code> <code>VolatilityFilter</code> ATR values <code>state.runtime[\"atr\"]</code> <code>RegimeFilter</code> Market regime <code>state.runtime[\"regime\"]</code> <code>CorrelationFilter</code> Correlation matrix <code>state.runtime[\"correlations\"]</code> <code>SignalAccuracyFilter</code> Accuracy metrics <code>state.runtime[\"signal_accuracy\"]</code> <p>Populate these during backtest:</p> <pre><code># Example: populate runtime data before entry rule\ndef on_bar_hook(state, timestamp, prices):\n    state.runtime[\"atr\"] = calculate_atr(prices)\n    state.runtime[\"regime\"] = detect_regime(prices)\n</code></pre>"},{"location":"api/strategy/#full-backtest-example","title":"Full Backtest Example","text":"<p>Complete example using signal aggregation, entry filters, and position sizing:</p> <pre><code>from datetime import datetime\nfrom pathlib import Path\n\nimport polars as pl\nfrom signalflow.data.raw_store import DuckDbRawStore\nfrom signalflow.data.source import VirtualDataProvider\nfrom signalflow.data import RawDataFactory\nfrom signalflow.detector import ExampleSmaCrossDetector\nfrom signalflow.strategy.broker import BacktestBroker\nfrom signalflow.strategy.broker.executor import VirtualSpotExecutor\nfrom signalflow.strategy.runner import BacktestRunner\nfrom signalflow.strategy.component.entry import (\n    SignalEntryRule,\n    SignalAggregator,\n    VotingMode,\n    CompositeEntryFilter,\n    DrawdownFilter,\n    TimeOfDayFilter,\n)\nfrom signalflow.strategy.component.sizing import VolatilityTargetSizer\nfrom signalflow.strategy.component.exit import TakeProfitStopLossExit\n\n# 1. Generate synthetic data\nPAIRS = [\"BTCUSDT\", \"ETHUSDT\"]\nSTART = datetime(2025, 1, 1)\n\nspot_store = DuckDbRawStore(db_path=Path(\"backtest.duckdb\"), timeframe=\"1m\")\nprovider = VirtualDataProvider(store=spot_store, seed=42)\nprovider.download(pairs=PAIRS, n_bars=5000, start=START)\n\n# 2. Load data\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"backtest.duckdb\"),\n    pairs=PAIRS,\n    start=START,\n    end=datetime(2025, 1, 4),\n)\n\n# 3. Create multiple detectors\ndetector_fast = ExampleSmaCrossDetector(fast_period=10, slow_period=30)\ndetector_slow = ExampleSmaCrossDetector(fast_period=20, slow_period=50)\n\nsignals_fast = detector_fast.run(raw_data.view())\nsignals_slow = detector_slow.run(raw_data.view())\n\n# 4. Aggregate signals (unanimous agreement)\naggregator = SignalAggregator(\n    voting_mode=VotingMode.UNANIMOUS,\n    probability_threshold=0.0,\n)\nsignals = aggregator.aggregate([signals_fast, signals_slow])\n\n# 5. Configure entry rule with sizer and filters\nentry_rule = SignalEntryRule(\n    position_sizer=VolatilityTargetSizer(\n        target_volatility=0.015,\n        default_volatility_pct=0.02,\n        max_fraction=0.15,\n    ),\n    entry_filters=CompositeEntryFilter(\n        filters=[\n            DrawdownFilter(max_drawdown=0.10),\n            TimeOfDayFilter(allowed_hours=list(range(6, 22))),\n        ],\n    ),\n    max_positions_per_pair=1,\n    max_total_positions=5,\n)\n\n# 6. Run backtest\nrunner = BacktestRunner(\n    strategy_id=\"advanced_strategy\",\n    broker=BacktestBroker(executor=VirtualSpotExecutor(fee_rate=0.001)),\n    entry_rules=[entry_rule],\n    exit_rules=[TakeProfitStopLossExit(take_profit_pct=0.02, stop_loss_pct=0.015)],\n    initial_capital=10_000.0,\n)\n\nstate = runner.run(raw_data, signals)\nresults = runner.get_results()\n\nprint(f\"Total Return: {results.get('final_return', 0) * 100:.2f}%\")\nprint(f\"Max Drawdown: {results.get('max_drawdown', 0) * 100:.2f}%\")\nprint(f\"Win Rate: {results.get('win_rate', 0) * 100:.1f}%\")\n\n# Cleanup\nspot_store.close()\n</code></pre>"},{"location":"api/strategy/#external-model-integration","title":"External Model Integration","text":"<p>SignalFlow supports integration with external ML/RL models via a Protocol-based interface. Models make trading decisions (entry, exit, hold) based on signals and metrics.</p>"},{"location":"api/strategy/#architecture_1","title":"Architecture","text":"<pre><code>flowchart TB\n    A[Signals + Metrics] --&gt; B[ModelContext]\n    B --&gt; C[StrategyModel.decide]\n    C --&gt; D[list of StrategyDecision]\n    D --&gt; E{Action Type}\n    E --&gt;|ENTER| F[ModelEntryRule]\n    E --&gt;|CLOSE/CLOSE_ALL| G[ModelExitRule]\n    F --&gt; H[Entry Orders]\n    G --&gt; I[Exit Orders]\n\n    style C fill:#7c3aed,stroke:#8b5cf6,color:#fff\n    style D fill:#059669,stroke:#10b981,color:#fff</code></pre> <p>Design Principle: Strategy models see signals and metrics only, NOT raw OHLCV prices.</p>"},{"location":"api/strategy/#strategyaction","title":"StrategyAction","text":"Action Description <code>ENTER</code> Open new position (uses <code>size_multiplier</code>) <code>SKIP</code> Skip this signal <code>CLOSE</code> Close specific position (requires <code>position_id</code>) <code>CLOSE_ALL</code> Close all positions for a pair <code>HOLD</code> Do nothing"},{"location":"api/strategy/#signalflow.strategy.model.decision.StrategyAction","title":"signalflow.strategy.model.decision.StrategyAction","text":"<p>               Bases: <code>StrEnum</code></p> <p>Actions a strategy model can take.</p> Values <p>ENTER: Open new position for pair (uses size_multiplier). SKIP: Skip this signal (do not enter). CLOSE: Close specific position (requires position_id). CLOSE_ALL: Close all positions for a pair. HOLD: Do nothing (no action).</p>"},{"location":"api/strategy/#strategydecision","title":"StrategyDecision","text":""},{"location":"api/strategy/#signalflow.strategy.model.decision.StrategyDecision","title":"signalflow.strategy.model.decision.StrategyDecision  <code>dataclass</code>","text":"<pre><code>StrategyDecision(action: StrategyAction, pair: str, position_id: str | None = None, size_multiplier: float = 1.0, confidence: float = 1.0, meta: dict[str, Any] = dict())\n</code></pre> <p>Model output for a single trading decision.</p> <p>Represents one decision from the model about whether to enter, exit, or hold positions. Multiple decisions can be returned per bar.</p> <p>Attributes:</p> Name Type Description <code>action</code> <code>StrategyAction</code> <p>The action to take (ENTER, SKIP, CLOSE, CLOSE_ALL, HOLD).</p> <code>pair</code> <code>str</code> <p>Trading pair this decision applies to.</p> <code>position_id</code> <code>str | None</code> <p>For CLOSE action - specific position to close.</p> <code>size_multiplier</code> <code>float</code> <p>For ENTER action - multiplier on base position size (default 1.0).</p> <code>confidence</code> <code>float</code> <p>Model confidence in this decision (0-1).</p> <code>meta</code> <code>dict[str, Any]</code> <p>Additional metadata (e.g., reason, model_name).</p> Example <p>Raises:</p> Type Description <code>ValueError</code> <p>If CLOSE action is missing position_id.</p> <code>ValueError</code> <p>If ENTER action has non-positive size_multiplier.</p>"},{"location":"api/strategy/#signalflow.strategy.model.decision.StrategyDecision--enter-decision","title":"Enter decision","text":"<p>decision = StrategyDecision( ...     action=StrategyAction.ENTER, ...     pair=\"BTCUSDT\", ...     size_multiplier=1.5, ...     confidence=0.85, ...     meta={\"signal_type\": \"rise\", \"model\": \"rf_v2\"} ... )</p>"},{"location":"api/strategy/#signalflow.strategy.model.decision.StrategyDecision--close-specific-position","title":"Close specific position","text":"<p>decision = StrategyDecision( ...     action=StrategyAction.CLOSE, ...     pair=\"BTCUSDT\", ...     position_id=\"pos_abc123\", ...     confidence=0.92, ...     meta={\"reason\": \"model_exit\"} ... )</p>"},{"location":"api/strategy/#signalflow.strategy.model.decision.StrategyDecision.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate decision parameters.</p> Source code in <code>src/signalflow/strategy/model/decision.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate decision parameters.\"\"\"\n    if self.action == StrategyAction.CLOSE and self.position_id is None:\n        raise ValueError(\"CLOSE action requires position_id\")\n    if self.action == StrategyAction.ENTER and self.size_multiplier &lt;= 0:\n        raise ValueError(\"size_multiplier must be positive for ENTER action\")\n</code></pre>"},{"location":"api/strategy/#modelcontext","title":"ModelContext","text":""},{"location":"api/strategy/#signalflow.strategy.model.context.ModelContext","title":"signalflow.strategy.model.context.ModelContext  <code>dataclass</code>","text":"<pre><code>ModelContext(timestamp: datetime, signals: Signals, prices: dict[str, float] = dict(), positions: list[Position] = list(), metrics: dict[str, float] = dict(), runtime: dict[str, Any] = dict())\n</code></pre> <p>Aggregated context passed to strategy models.</p> <p>Provides all information a model needs to make decisions: - Current bar signals - Strategy metrics (equity, drawdown, etc.) - Current positions and their states - Runtime state (for custom indicators, cooldowns, etc.)</p> <p>Attributes:</p> Name Type Description <code>timestamp</code> <code>datetime</code> <p>Current bar timestamp.</p> <code>signals</code> <code>Signals</code> <p>Current bar signals (Signals container).</p> <code>prices</code> <code>dict[str, float]</code> <p>Current prices per pair.</p> <code>positions</code> <code>list[Position]</code> <p>List of open positions.</p> <code>metrics</code> <code>dict[str, float]</code> <p>Current strategy metrics snapshot.</p> <code>runtime</code> <code>dict[str, Any]</code> <p>Runtime state dict (cooldowns, custom state).</p> Example"},{"location":"api/strategy/#signalflow.strategy.model.context.ModelContext--model-receives-context-each-bar","title":"Model receives context each bar","text":"<p>def decide(self, context: ModelContext) -&gt; list[StrategyDecision]: ...     # Access signals ...     for row in context.signals.value.iter_rows(named=True): ...         pair = row[\"pair\"] ...         signal_type = row[\"signal_type\"] ...         probability = row.get(\"probability\", 0.5) ... ...         # Use metrics for risk management ...         if context.metrics.get(\"max_drawdown\", 0) &gt; 0.15: ...             continue  # Skip during high drawdown ... ...         # Check existing positions ...         pair_positions = [p for p in context.positions if p.pair == pair] ...         ...</p>"},{"location":"api/strategy/#strategymodel-protocol","title":"StrategyModel Protocol","text":""},{"location":"api/strategy/#signalflow.strategy.model.protocol.StrategyModel","title":"signalflow.strategy.model.protocol.StrategyModel","text":"<p>               Bases: <code>Protocol</code></p> <p>Protocol for external strategy models.</p> <p>External models must implement this protocol to integrate with SignalFlow. The model receives context (signals, metrics, positions) and returns a list of trading decisions.</p> Implementation Notes <ul> <li>Models are called ONCE per bar (not per signal).</li> <li>Return empty list for \"no action\".</li> <li>Multiple decisions per bar are allowed.</li> <li>Model should be stateless (use context.runtime for state).</li> </ul> Example Implementation <p>class MyRLModel: ...     '''Reinforcement learning model for trading.''' ... ...     def init(self, model_path: str): ...         self.model = load_model(model_path) ... ...     def decide(self, context: ModelContext) -&gt; list[StrategyDecision]: ...         decisions = [] ... ...         # Skip if drawdown too high ...         if context.metrics.get(\"max_drawdown\", 0) &gt; 0.2: ...             return decisions ... ...         # Process each signal ...         for row in context.signals.value.iter_rows(named=True): ...             pair = row[\"pair\"] ...             prob = row.get(\"probability\", 0.5) ... ...             features = self._build_features(row, context.metrics) ...             action, confidence = self.model.predict(features) ... ...             if action == \"enter\" and confidence &gt; 0.6: ...                 decisions.append(StrategyDecision( ...                     action=StrategyAction.ENTER, ...                     pair=pair, ...                     size_multiplier=min(confidence, 1.5), ...                     confidence=confidence, ...                 )) ... ...         # Check if should close any positions ...         for pos in context.positions: ...             if self._should_close(pos, context): ...                 decisions.append(StrategyDecision( ...                     action=StrategyAction.CLOSE, ...                     pair=pos.pair, ...                     position_id=pos.id, ...                 )) ... ...         return decisions</p>"},{"location":"api/strategy/#signalflow.strategy.model.protocol.StrategyModel.decide","title":"decide","text":"<pre><code>decide(context: ModelContext) -&gt; list[StrategyDecision]\n</code></pre> <p>Make trading decisions based on current context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ModelContext</code> <p>Current bar context with signals, metrics, positions.</p> required <p>Returns:</p> Type Description <code>list[StrategyDecision]</code> <p>List of trading decisions (can be empty).</p> Source code in <code>src/signalflow/strategy/model/protocol.py</code> <pre><code>def decide(self, context: ModelContext) -&gt; list[StrategyDecision]:\n    \"\"\"Make trading decisions based on current context.\n\n    Args:\n        context: Current bar context with signals, metrics, positions.\n\n    Returns:\n        List of trading decisions (can be empty).\n    \"\"\"\n    ...\n</code></pre>"},{"location":"api/strategy/#modelentryrule","title":"ModelEntryRule","text":""},{"location":"api/strategy/#signalflow.strategy.model.rules.ModelEntryRule","title":"signalflow.strategy.model.rules.ModelEntryRule  <code>dataclass</code>","text":"<pre><code>ModelEntryRule(signal_type_map: dict[str, str] | None = None, model: StrategyModel = None, base_position_size: float = 0.01, max_positions: int = 10, min_confidence: float = 0.5, allow_shorts: bool = False, pair_col: str = 'pair')\n</code></pre> <p>               Bases: <code>EntryRule</code></p> <p>Entry rule that delegates to an external model.</p> <p>The model is called once per bar. Its decisions are cached in state.runtime so that ModelExitRule can access exit decisions without calling the model twice.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>StrategyModel</code> <p>External model implementing StrategyModel protocol.</p> <code>base_position_size</code> <code>float</code> <p>Base position size (multiplied by decision.size_multiplier).</p> <code>max_positions</code> <code>int</code> <p>Maximum concurrent positions.</p> <code>min_confidence</code> <code>float</code> <p>Minimum confidence to act on ENTER decisions.</p> <code>allow_shorts</code> <code>bool</code> <p>Allow FALL signals to create short positions.</p> <code>pair_col</code> <code>str</code> <p>Column name for pair in signals.</p> Example <p>from signalflow.strategy.model import ModelEntryRule, ModelExitRule</p> <p>model = MyRLModel(\"model.pt\") entry_rule = ModelEntryRule( ...     model=model, ...     base_position_size=0.01, ...     max_positions=5, ...     min_confidence=0.6, ... ) exit_rule = ModelExitRule(model=model)</p> <p>runner = BacktestRunner( ...     entry_rules=[entry_rule], ...     exit_rules=[exit_rule], ...     ... ... )</p>"},{"location":"api/strategy/#signalflow.strategy.model.rules.ModelEntryRule.check_entries","title":"check_entries","text":"<pre><code>check_entries(signals: Signals, prices: dict[str, float], state: StrategyState) -&gt; list[Order]\n</code></pre> <p>Generate entry orders from model decisions.</p> Source code in <code>src/signalflow/strategy/model/rules.py</code> <pre><code>def check_entries(\n    self,\n    signals: Signals,\n    prices: dict[str, float],\n    state: StrategyState,\n) -&gt; list[Order]:\n    \"\"\"Generate entry orders from model decisions.\"\"\"\n    orders: list[Order] = []\n\n    if signals is None or signals.value.height == 0:\n        return orders\n\n    if self.model is None:\n        return orders\n\n    # Get or compute decisions\n    decisions = _get_cached_decisions(state)\n    if decisions is None:\n        context = _build_model_context(signals, prices, state)\n        decisions = self.model.decide(context)\n        _cache_decisions(state, decisions)\n\n    # Filter to ENTER decisions\n    enter_decisions = [\n        d for d in decisions if d.action == StrategyAction.ENTER and d.confidence &gt;= self.min_confidence\n    ]\n\n    open_count = len(state.portfolio.open_positions())\n\n    for decision in enter_decisions:\n        if open_count &gt;= self.max_positions:\n            break\n\n        price = prices.get(decision.pair)\n        if price is None or price &lt;= 0:\n            continue\n\n        # Calculate position size\n        qty = self.base_position_size * decision.size_multiplier\n\n        # Determine side from signal (lookup in signals)\n        side: OrderSide = self._get_side_from_signals(signals, decision.pair) or \"BUY\"\n\n        order = Order(\n            pair=decision.pair,\n            side=side,\n            order_type=\"MARKET\",\n            qty=qty,\n            signal_strength=decision.confidence,\n            meta={\n                \"decision_action\": decision.action.value,\n                \"model_confidence\": decision.confidence,\n                \"size_multiplier\": decision.size_multiplier,\n                **decision.meta,\n            },\n        )\n        orders.append(order)\n        open_count += 1\n\n    return orders\n</code></pre>"},{"location":"api/strategy/#modelexitrule","title":"ModelExitRule","text":""},{"location":"api/strategy/#signalflow.strategy.model.rules.ModelExitRule","title":"signalflow.strategy.model.rules.ModelExitRule  <code>dataclass</code>","text":"<pre><code>ModelExitRule(model: StrategyModel = None, min_confidence: float = 0.5)\n</code></pre> <p>               Bases: <code>ExitRule</code></p> <p>Exit rule that uses cached model decisions.</p> <p>NOTE: If decisions are not cached yet (exit runs before entry), this rule will call the model and cache the results.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>StrategyModel</code> <p>External model implementing StrategyModel protocol.</p> <code>min_confidence</code> <code>float</code> <p>Minimum confidence to act on CLOSE/CLOSE_ALL decisions.</p> Example <p>exit_rule = ModelExitRule( ...     model=model, ...     min_confidence=0.7,  # Higher threshold for exits ... )</p>"},{"location":"api/strategy/#signalflow.strategy.model.rules.ModelExitRule.check_exits","title":"check_exits","text":"<pre><code>check_exits(positions: list[Position], prices: dict[str, float], state: StrategyState) -&gt; list[Order]\n</code></pre> <p>Generate exit orders from model decisions.</p> Source code in <code>src/signalflow/strategy/model/rules.py</code> <pre><code>def check_exits(\n    self,\n    positions: list[Position],\n    prices: dict[str, float],\n    state: StrategyState,\n) -&gt; list[Order]:\n    \"\"\"Generate exit orders from model decisions.\"\"\"\n    orders: list[Order] = []\n\n    if not positions:\n        return orders\n\n    if self.model is None:\n        return orders\n\n    # Get cached decisions or compute them\n    # (ExitRule runs BEFORE EntryRule in BacktestRunner)\n    decisions = _get_cached_decisions(state)\n    if decisions is None:\n        # Get signals from runtime (stored by runner)\n        signals = state.runtime.get(BAR_SIGNALS_KEY, Signals(pl.DataFrame()))\n        context = _build_model_context(signals, prices, state)\n        decisions = self.model.decide(context)\n        _cache_decisions(state, decisions)\n\n    # Build position lookup\n    positions_by_id = {p.id: p for p in positions if not p.is_closed}\n    positions_by_pair: dict[str, list[Position]] = {}\n    for p in positions:\n        if not p.is_closed:\n            positions_by_pair.setdefault(p.pair, []).append(p)\n\n    # Track which positions we've already added exit orders for\n    exited_position_ids: set[str] = set()\n\n    # Process CLOSE and CLOSE_ALL decisions\n    for decision in decisions:\n        if decision.confidence &lt; self.min_confidence:\n            continue\n\n        if decision.action == StrategyAction.CLOSE:\n            # Close specific position\n            if decision.position_id is None:\n                continue\n            if decision.position_id in exited_position_ids:\n                continue\n\n            position = positions_by_id.get(decision.position_id)\n            if position and not position.is_closed:\n                order = self._create_exit_order(position, prices, decision)\n                if order:\n                    orders.append(order)\n                    exited_position_ids.add(position.id)\n\n        elif decision.action == StrategyAction.CLOSE_ALL:\n            # Close all positions for pair\n            pair_positions = positions_by_pair.get(decision.pair, [])\n            for position in pair_positions:\n                if position.id in exited_position_ids:\n                    continue\n                if position.is_closed:\n                    continue\n\n                order = self._create_exit_order(position, prices, decision)\n                if order:\n                    orders.append(order)\n                    exited_position_ids.add(position.id)\n\n    return orders\n</code></pre>"},{"location":"api/strategy/#model-integration-example","title":"Model Integration Example","text":"<pre><code>from signalflow.strategy.model import (\n    StrategyModel,\n    StrategyAction,\n    StrategyDecision,\n    ModelContext,\n    ModelEntryRule,\n    ModelExitRule,\n)\nfrom signalflow.strategy.runner import BacktestRunner\nfrom signalflow.strategy.broker import BacktestBroker\nfrom signalflow.strategy.broker.executor import VirtualSpotExecutor\nfrom signalflow.strategy.component.exit import TakeProfitStopLossExit\n\n\n# 1. Implement the StrategyModel protocol\nclass MyRLModel:\n    \"\"\"Example RL model for trading decisions.\"\"\"\n\n    def __init__(self, model_path: str):\n        # Load your trained model\n        self.model = self._load_model(model_path)\n\n    def decide(self, context: ModelContext) -&gt; list[StrategyDecision]:\n        decisions = []\n\n        # Risk management: skip during high drawdown\n        if context.metrics.get(\"max_drawdown\", 0) &gt; 0.15:\n            return decisions\n\n        # Process each signal\n        for row in context.signals.value.iter_rows(named=True):\n            pair = row[\"pair\"]\n            prob = row.get(\"probability\", 0.5)\n\n            # Get model prediction\n            features = self._build_features(row, context.metrics)\n            action, confidence = self.model.predict(features)\n\n            if action == \"enter\" and confidence &gt; 0.6:\n                decisions.append(StrategyDecision(\n                    action=StrategyAction.ENTER,\n                    pair=pair,\n                    size_multiplier=min(confidence, 1.5),\n                    confidence=confidence,\n                    meta={\"model\": \"rl_v1\"},\n                ))\n\n        # Check if should close any positions\n        for pos in context.positions:\n            if self._should_close(pos, context):\n                decisions.append(StrategyDecision(\n                    action=StrategyAction.CLOSE,\n                    pair=pos.pair,\n                    position_id=pos.id,\n                    confidence=0.9,\n                    meta={\"reason\": \"model_exit\"},\n                ))\n\n        return decisions\n\n\n# 2. Create rules with the model\nmodel = MyRLModel(\"model.pt\")\n\nentry_rule = ModelEntryRule(\n    model=model,\n    base_position_size=0.02,  # 2% base size\n    max_positions=5,\n    min_confidence=0.6,\n)\n\nexit_rule = ModelExitRule(\n    model=model,\n    min_confidence=0.7,  # Higher threshold for exits\n)\n\n# 3. Run backtest\nrunner = BacktestRunner(\n    strategy_id=\"model_strategy\",\n    broker=BacktestBroker(executor=VirtualSpotExecutor(fee_rate=0.001)),\n    entry_rules=[entry_rule],\n    exit_rules=[\n        exit_rule,\n        TakeProfitStopLossExit(take_profit_pct=0.03, stop_loss_pct=0.02),\n    ],\n    initial_capital=10_000.0,\n)\n\nstate = runner.run(raw_data, signals)\n</code></pre>"},{"location":"api/strategy/#data-export","title":"Data Export","text":"<p>Export backtest results for external ML model training.</p>"},{"location":"api/strategy/#backtestexporter","title":"BacktestExporter","text":""},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter","title":"signalflow.strategy.exporter.parquet_exporter.BacktestExporter  <code>dataclass</code>","text":"<pre><code>BacktestExporter(pair_col: str = 'pair', ts_col: str = 'timestamp')\n</code></pre> <p>Export backtest results for external ML training.</p> <p>Exports per-bar data (signals + metrics) and per-trade data to Parquet format. Does NOT include raw OHLCV prices - only signals and derived metrics.</p> Output Files <ul> <li>{output_path}/bars.parquet: Per-bar signals and metrics</li> <li>{output_path}/trades.parquet: Entry/exit trade pairs with outcomes</li> </ul> <p>Attributes:</p> Name Type Description <code>pair_col</code> <code>str</code> <p>Column name for pair in signals.</p> <code>ts_col</code> <code>str</code> <p>Column name for timestamp in signals.</p> Example <p>exporter = BacktestExporter()</p>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter--during-backtest-can-be-integrated-with-runner","title":"During backtest (can be integrated with runner)","text":"<p>for ts in timestamps: ...     # ... process bar ... ...     exporter.export_bar(ts, signals, state.metrics, state) ... ...     for trade in bar_trades: ...         exporter.export_trade(trade_data)</p>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter--after-backtest","title":"After backtest","text":"<p>exporter.finalize(Path(\"./training_data\"))</p>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter--load-for-training","title":"Load for training","text":"<p>bars = pl.read_parquet(\"./training_data/bars.parquet\") trades = pl.read_parquet(\"./training_data/trades.parquet\")</p>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter.bar_count","title":"bar_count  <code>property</code>","text":"<pre><code>bar_count: int\n</code></pre> <p>Number of bar records collected.</p>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter.trade_count","title":"trade_count  <code>property</code>","text":"<pre><code>trade_count: int\n</code></pre> <p>Number of trade records collected.</p>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter.export_bar","title":"export_bar","text":"<pre><code>export_bar(timestamp: datetime, signals: Signals, metrics: dict[str, float], state: StrategyState) -&gt; None\n</code></pre> <p>Record bar data for export.</p> Records <ul> <li>Timestamp</li> <li>All signals for this bar (flattened)</li> <li>All metrics values</li> <li>Position summary (count, total exposure)</li> </ul> <p>Parameters:</p> Name Type Description Default <code>timestamp</code> <code>datetime</code> <p>Current bar timestamp.</p> required <code>signals</code> <code>Signals</code> <p>Current bar signals.</p> required <code>metrics</code> <code>dict[str, float]</code> <p>Current strategy metrics.</p> required <code>state</code> <code>StrategyState</code> <p>Current strategy state.</p> required Source code in <code>src/signalflow/strategy/exporter/parquet_exporter.py</code> <pre><code>def export_bar(\n    self,\n    timestamp: datetime,\n    signals: Signals,\n    metrics: dict[str, float],\n    state: StrategyState,\n) -&gt; None:\n    \"\"\"Record bar data for export.\n\n    Records:\n        - Timestamp\n        - All signals for this bar (flattened)\n        - All metrics values\n        - Position summary (count, total exposure)\n\n    Args:\n        timestamp: Current bar timestamp.\n        signals: Current bar signals.\n        metrics: Current strategy metrics.\n        state: Current strategy state.\n    \"\"\"\n    if signals is None or signals.value.height == 0:\n        # Still record metrics even without signals\n        record = {\n            self.ts_col: timestamp,\n            **{f\"metric_{k}\": v for k, v in metrics.items() if k != \"timestamp\"},\n            \"open_position_count\": len(state.portfolio.open_positions()),\n        }\n        self._bar_records.append(record)\n        return\n\n    # For each signal, create a record with metrics\n    for row in signals.value.iter_rows(named=True):\n        record = {\n            self.ts_col: timestamp,\n            self.pair_col: row.get(self.pair_col, \"\"),\n            \"signal_type\": row.get(\"signal_type\", \"\"),\n            \"signal\": row.get(\"signal\", 0),\n            \"probability\": row.get(\"probability\", 0.0),\n            **{f\"metric_{k}\": v for k, v in metrics.items() if k != \"timestamp\"},\n            \"open_position_count\": len(state.portfolio.open_positions()),\n        }\n        self._bar_records.append(record)\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter.export_position_close","title":"export_position_close","text":"<pre><code>export_position_close(position: Position, exit_time: datetime, exit_price: float, exit_reason: str = 'unknown') -&gt; None\n</code></pre> <p>Convenience method to export when a position closes.</p> <p>Parameters:</p> Name Type Description Default <code>position</code> <code>Position</code> <p>The position being closed.</p> required <code>exit_time</code> <code>datetime</code> <p>Time of exit.</p> required <code>exit_price</code> <code>float</code> <p>Price at exit.</p> required <code>exit_reason</code> <code>str</code> <p>Reason for exit (e.g., \"take_profit\", \"stop_loss\", \"model_exit\").</p> <code>'unknown'</code> Source code in <code>src/signalflow/strategy/exporter/parquet_exporter.py</code> <pre><code>def export_position_close(\n    self,\n    position: Position,\n    exit_time: datetime,\n    exit_price: float,\n    exit_reason: str = \"unknown\",\n) -&gt; None:\n    \"\"\"Convenience method to export when a position closes.\n\n    Args:\n        position: The position being closed.\n        exit_time: Time of exit.\n        exit_price: Price at exit.\n        exit_reason: Reason for exit (e.g., \"take_profit\", \"stop_loss\", \"model_exit\").\n    \"\"\"\n    entry_meta = position.meta or {}\n\n    trade_data = {\n        \"position_id\": position.id,\n        \"pair\": position.pair,\n        \"position_type\": position.position_type.value,\n        \"entry_time\": position.entry_time,\n        \"entry_price\": position.entry_price,\n        \"exit_time\": exit_time,\n        \"exit_price\": exit_price,\n        \"qty\": position.qty,\n        \"realized_pnl\": position.realized_pnl,\n        \"total_pnl\": position.total_pnl,\n        \"fees_paid\": position.fees_paid,\n        \"signal_strength\": position.signal_strength,\n        \"exit_reason\": exit_reason,\n        \"entry_signal_type\": entry_meta.get(\"signal_type\", \"\"),\n        \"model_confidence\": entry_meta.get(\"model_confidence\", 0.0),\n    }\n    self._trade_records.append(trade_data)\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter.export_trade","title":"export_trade","text":"<pre><code>export_trade(trade_data: dict[str, Any]) -&gt; None\n</code></pre> <p>Record completed trade for export.</p> <p>Parameters:</p> Name Type Description Default <code>trade_data</code> <code>dict[str, Any]</code> <p>Dictionary containing trade information. Expected keys: - position_id - pair - entry_time, entry_price - exit_time, exit_price (if closed) - realized_pnl - hold_duration_bars - entry_signal_type, entry_confidence - exit_reason</p> required Source code in <code>src/signalflow/strategy/exporter/parquet_exporter.py</code> <pre><code>def export_trade(\n    self,\n    trade_data: dict[str, Any],\n) -&gt; None:\n    \"\"\"Record completed trade for export.\n\n    Args:\n        trade_data: Dictionary containing trade information.\n            Expected keys:\n            - position_id\n            - pair\n            - entry_time, entry_price\n            - exit_time, exit_price (if closed)\n            - realized_pnl\n            - hold_duration_bars\n            - entry_signal_type, entry_confidence\n            - exit_reason\n    \"\"\"\n    self._trade_records.append(trade_data)\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter.finalize","title":"finalize","text":"<pre><code>finalize(output_path: Path) -&gt; None\n</code></pre> <p>Write all data to Parquet files.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>Path</code> <p>Directory to write output files. Creates directory if it doesn't exist.</p> required Source code in <code>src/signalflow/strategy/exporter/parquet_exporter.py</code> <pre><code>def finalize(self, output_path: Path) -&gt; None:\n    \"\"\"Write all data to Parquet files.\n\n    Args:\n        output_path: Directory to write output files.\n            Creates directory if it doesn't exist.\n    \"\"\"\n    output_path = Path(output_path)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Export bars\n    if self._bar_records:\n        bars_df = pl.DataFrame(self._bar_records)\n        bars_df.write_parquet(output_path / \"bars.parquet\")\n\n    # Export trades\n    if self._trade_records:\n        trades_df = pl.DataFrame(self._trade_records)\n        trades_df.write_parquet(output_path / \"trades.parquet\")\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.exporter.parquet_exporter.BacktestExporter.reset","title":"reset","text":"<pre><code>reset() -&gt; None\n</code></pre> <p>Clear all recorded data.</p> Source code in <code>src/signalflow/strategy/exporter/parquet_exporter.py</code> <pre><code>def reset(self) -&gt; None:\n    \"\"\"Clear all recorded data.\"\"\"\n    self._bar_records.clear()\n    self._trade_records.clear()\n</code></pre>"},{"location":"api/strategy/#export-format","title":"Export Format","text":"<p>bars.parquet - Per-bar signals and metrics:</p> Column Description <code>timestamp</code> Bar timestamp <code>pair</code> Trading pair <code>signal_type</code> Signal type (e.g. <code>rise</code>, <code>fall</code>, <code>local_max</code>) <code>probability</code> Signal probability <code>metric_equity</code> Portfolio equity <code>metric_max_drawdown</code> Max drawdown <code>open_position_count</code> Open positions count <p>trades.parquet - Completed trades:</p> Column Description <code>position_id</code> Position ID <code>pair</code> Trading pair <code>entry_time</code>, <code>exit_time</code> Trade timestamps <code>entry_price</code>, <code>exit_price</code> Prices <code>realized_pnl</code> Realized profit/loss <code>exit_reason</code> Why trade closed <code>model_confidence</code> Model confidence at entry"},{"location":"api/strategy/#export-example","title":"Export Example","text":"<pre><code>from pathlib import Path\nfrom signalflow.strategy.exporter import BacktestExporter\n\n# Create exporter\nexporter = BacktestExporter()\n\n# Option 1: Manual export during custom backtest loop\nfor ts in timestamps:\n    # ... process bar ...\n    exporter.export_bar(ts, signals, state.metrics, state)\n\n# Export when positions close\nfor closed_position in newly_closed:\n    exporter.export_position_close(\n        position=closed_position,\n        exit_time=ts,\n        exit_price=prices[closed_position.pair],\n        exit_reason=\"take_profit\",\n    )\n\n# Write to disk\nexporter.finalize(Path(\"./training_data\"))\n\n# Option 2: Load for training\nimport polars as pl\n\nbars_df = pl.read_parquet(\"./training_data/bars.parquet\")\ntrades_df = pl.read_parquet(\"./training_data/trades.parquet\")\n\n# Prepare features for ML training\nfeatures = bars_df.select([\n    \"timestamp\", \"pair\", \"signal_type\", \"probability\",\n    \"metric_equity\", \"metric_max_drawdown\",\n])\n</code></pre>"},{"location":"api/strategy/#see-also","title":"See Also","text":"<ul> <li>Model Integration Guide: Detailed ML/RL integration tutorial</li> <li>Quick Start: Basic strategy setup</li> <li>Core API: <code>StrategyState</code>, <code>Portfolio</code>, <code>Position</code></li> </ul>"},{"location":"api/ta/","title":"Technical Analysis Module (signalflow-ta)","text":"<p>API reference for <code>signalflow.ta</code> \u2014 199+ technical indicators.</p> <p>Separate Package</p> <p>Install with <code>pip install signalflow-ta</code>. See the ecosystem page for usage guide, examples, and pipeline factories.</p> <p>All classes extend <code>signalflow.feature.base.Feature</code> and are registered via <code>@sf_component</code>. Import directly from <code>signalflow.ta</code>:</p> <pre><code>import signalflow.ta as ta\nrsi = ta.RsiMom(period=14, normalized=True)\n</code></pre>"},{"location":"api/ta/#momentum","title":"Momentum","text":"<p><code>signalflow.ta.momentum</code> \u2014 18 indicators</p>"},{"location":"api/ta/#core-signalflowtamomentumcore","title":"Core (<code>signalflow.ta.momentum.core</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>RsiMom</code> <code>momentum/rsi</code> <code>period=14</code>, <code>source_col=\"close\"</code> <code>rsi_{period}</code> <code>RocMom</code> <code>momentum/roc</code> <code>period=12</code>, <code>source_col=\"close\"</code> <code>roc_{period}</code> <code>MomMom</code> <code>momentum/mom</code> <code>period=10</code>, <code>source_col=\"close\"</code> <code>mom_{period}</code> <code>CmoMom</code> <code>momentum/cmo</code> <code>period=14</code>, <code>source_col=\"close\"</code> <code>cmo_{period}</code>"},{"location":"api/ta/#oscillators-signalflowtamomentumoscillators","title":"Oscillators (<code>signalflow.ta.momentum.oscillators</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>StochMom</code> <code>momentum/stoch</code> <code>k_period=14</code>, <code>d_period=3</code> <code>stoch_k_{k_period}</code>, <code>stoch_d_{k_period}</code> <code>StochRsiMom</code> <code>momentum/stoch_rsi</code> <code>period=14</code>, <code>k_period=3</code>, <code>d_period=3</code> <code>stoch_rsi_k_{period}</code>, <code>stoch_rsi_d_{period}</code> <code>WillrMom</code> <code>momentum/willr</code> <code>period=14</code> <code>willr_{period}</code> <code>CciMom</code> <code>momentum/cci</code> <code>period=20</code> <code>cci_{period}</code> <code>UoMom</code> <code>momentum/uo</code> <code>s=7</code>, <code>m=14</code>, <code>l=28</code> <code>uo</code> <code>AoMom</code> <code>momentum/ao</code> <code>s=5</code>, <code>l=34</code> <code>ao</code>"},{"location":"api/ta/#macd-family-signalflowtamomentummacd","title":"MACD Family (<code>signalflow.ta.momentum.macd</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>MacdMom</code> <code>momentum/macd</code> <code>fast=12</code>, <code>slow=26</code>, <code>signal=9</code> <code>macd</code>, <code>macd_signal</code>, <code>macd_hist</code> <code>PpoMom</code> <code>momentum/ppo</code> <code>fast=12</code>, <code>slow=26</code>, <code>signal=9</code> <code>ppo</code>, <code>ppo_signal</code>, <code>ppo_hist</code> <code>TsiMom</code> <code>momentum/tsi</code> <code>long=25</code>, <code>short=13</code>, <code>signal=13</code> <code>tsi</code>, <code>tsi_signal</code> <code>TrixMom</code> <code>momentum/trix</code> <code>period=15</code>, <code>signal=9</code> <code>trix</code>, <code>trix_signal</code>"},{"location":"api/ta/#kinematics-signalflowtamomentumkinematics","title":"Kinematics (<code>signalflow.ta.momentum.kinematics</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>AccelerationMom</code> <code>momentum/acceleration</code> <code>period=14</code> <code>acceleration_{period}</code> <code>JerkMom</code> <code>momentum/jerk</code> <code>period=14</code> <code>jerk_{period}</code> <code>AngularMomentumMom</code> <code>momentum/angular_momentum</code> <code>period=14</code> <code>angular_momentum_{period}</code> <code>TorqueMom</code> <code>momentum/torque</code> <code>period=14</code> <code>torque_{period}</code>"},{"location":"api/ta/#overlap","title":"Overlap","text":"<p><code>signalflow.ta.overlap</code> \u2014 26 indicators</p>"},{"location":"api/ta/#smoothers-signalflowtaoverlapsmoothers","title":"Smoothers (<code>signalflow.ta.overlap.smoothers</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>SmaSmooth</code> <code>overlap/sma</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>sma_{period}</code> <code>EmaSmooth</code> <code>overlap/ema</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>ema_{period}</code> <code>WmaSmooth</code> <code>overlap/wma</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>wma_{period}</code> <code>RmaSmooth</code> <code>overlap/rma</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>rma_{period}</code> <code>DemaSmooth</code> <code>overlap/dema</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>dema_{period}</code> <code>TemaSmooth</code> <code>overlap/tema</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>tema_{period}</code> <code>HmaSmooth</code> <code>overlap/hma</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>hma_{period}</code> <code>TrimaSmooth</code> <code>overlap/trima</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>trima_{period}</code> <code>SwmaSmooth</code> <code>overlap/swma</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>swma_{period}</code> <code>SsfSmooth</code> <code>overlap/ssf</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>ssf_{period}</code> <code>FftSmooth</code> <code>overlap/fft</code> <code>period=20</code>, <code>source_col=\"close\"</code> <code>fft_{period}</code>"},{"location":"api/ta/#adaptive-signalflowtaoverlapadaptive","title":"Adaptive (<code>signalflow.ta.overlap.adaptive</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>KamaSmooth</code> <code>overlap/kama</code> <code>period=10</code>, <code>fast=2</code>, <code>slow=30</code> <code>kama_{period}</code> <code>AlmaSmooth</code> <code>overlap/alma</code> <code>period=20</code>, <code>sigma=6.0</code>, <code>offset=0.85</code> <code>alma_{period}</code> <code>JmaSmooth</code> <code>overlap/jma</code> <code>period=7</code>, <code>phase=50</code>, <code>power=2</code> <code>jma_{period}</code> <code>VidyaSmooth</code> <code>overlap/vidya</code> <code>period=20</code>, <code>cmo_period=9</code> <code>vidya_{period}</code> <code>T3Smooth</code> <code>overlap/t3</code> <code>period=5</code>, <code>v_factor=0.7</code> <code>t3_{period}</code> <code>ZlmaSmooth</code> <code>overlap/zlma</code> <code>period=20</code> <code>zlma_{period}</code> <code>McGinleySmooth</code> <code>overlap/mcginley</code> <code>period=20</code> <code>mcginley_{period}</code> <code>FramaSmooth</code> <code>overlap/frama</code> <code>period=20</code> <code>frama_{period}</code>"},{"location":"api/ta/#price-transforms-signalflowtaoverlapprice","title":"Price Transforms (<code>signalflow.ta.overlap.price</code>)","text":"Class <code>@sf_component</code> Output <code>Hl2Price</code> <code>overlap/hl2</code> <code>hl2</code> <code>Hlc3Price</code> <code>overlap/hlc3</code> <code>hlc3</code> <code>Ohlc4Price</code> <code>overlap/ohlc4</code> <code>ohlc4</code> <code>WcpPrice</code> <code>overlap/wcp</code> <code>wcp</code> <code>MidpointPrice</code> <code>overlap/midpoint</code> <code>midpoint</code> <code>MidpricePrice</code> <code>overlap/midprice</code> <code>midprice</code> <code>TypicalPrice</code> <code>overlap/typical</code> <code>typical</code>"},{"location":"api/ta/#volatility","title":"Volatility","text":"<p><code>signalflow.ta.volatility</code> \u2014 18+ indicators</p>"},{"location":"api/ta/#range-signalflowtavolatilityrange","title":"Range (<code>signalflow.ta.volatility.range</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>TrueRangeVol</code> <code>volatility/true_range</code> \u2014 <code>true_range</code> <code>AtrVol</code> <code>volatility/atr</code> <code>period=14</code> <code>atr_{period}</code> <code>NatrVol</code> <code>volatility/natr</code> <code>period=14</code> <code>natr_{period}</code>"},{"location":"api/ta/#bands-signalflowtavolatilitybands","title":"Bands (<code>signalflow.ta.volatility.bands</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>BollingerVol</code> <code>volatility/bollinger</code> <code>period=20</code>, <code>num_std=2.0</code> <code>bb_upper</code>, <code>bb_middle</code>, <code>bb_lower</code>, <code>bb_width</code>, <code>bb_pct</code> <code>KeltnerVol</code> <code>volatility/keltner</code> <code>period=20</code>, <code>atr_period=10</code>, <code>multiplier=1.5</code> <code>kc_upper</code>, <code>kc_middle</code>, <code>kc_lower</code> <code>DonchianVol</code> <code>volatility/donchian</code> <code>period=20</code> <code>dc_upper</code>, <code>dc_middle</code>, <code>dc_lower</code> <code>AccBandsVol</code> <code>volatility/acc_bands</code> <code>period=20</code> <code>acc_upper</code>, <code>acc_middle</code>, <code>acc_lower</code>"},{"location":"api/ta/#measures-signalflowtavolatilitymeasures","title":"Measures (<code>signalflow.ta.volatility.measures</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>MassIndexVol</code> <code>volatility/mass_index</code> <code>fast=9</code>, <code>slow=25</code> <code>mass_index</code> <code>UlcerIndexVol</code> <code>volatility/ulcer_index</code> <code>period=14</code> <code>ulcer_index_{period}</code> <code>RviVol</code> <code>volatility/rvi</code> <code>period=14</code> <code>rvi_{period}</code> <code>GapVol</code> <code>volatility/gap</code> <code>period=14</code> <code>gap_vol_{period}</code>"},{"location":"api/ta/#energy-physics-based-signalflowtavolatilityenergy","title":"Energy \u2014 Physics-Based (<code>signalflow.ta.volatility.energy</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>KineticEnergyVol</code> <code>volatility/kinetic_energy</code> <code>period=20</code> <code>kinetic_energy_{period}</code> <code>PotentialEnergyVol</code> <code>volatility/potential_energy</code> <code>period=20</code> <code>potential_energy_{period}</code> <code>TotalEnergyVol</code> <code>volatility/total_energy</code> <code>period=20</code> <code>total_energy_{period}</code> <code>EnergyFlowVol</code> <code>volatility/energy_flow</code> <code>period=20</code> <code>energy_flow_{period}</code> <code>ElasticStrainVol</code> <code>volatility/elastic_strain</code> <code>period=20</code> <code>elastic_strain_{period}</code> <code>TemperatureVol</code> <code>volatility/temperature</code> <code>period=20</code> <code>temperature_{period}</code> <code>HeatCapacityVol</code> <code>volatility/heat_capacity</code> <code>period=20</code> <code>heat_capacity_{period}</code> <code>FreeEnergyVol</code> <code>volatility/free_energy</code> <code>period=20</code> <code>free_energy_{period}</code>"},{"location":"api/ta/#volume","title":"Volume","text":"<p><code>signalflow.ta.volume</code> \u2014 16 indicators</p>"},{"location":"api/ta/#cumulative-signalflowtavolumecumulative","title":"Cumulative (<code>signalflow.ta.volume.cumulative</code>)","text":"Class <code>@sf_component</code> Output <code>ObvVolume</code> <code>volume/obv</code> <code>obv</code> <code>AdVolume</code> <code>volume/ad</code> <code>ad</code> <code>PvtVolume</code> <code>volume/pvt</code> <code>pvt</code> <code>NviVolume</code> <code>volume/nvi</code> <code>nvi</code> <code>PviVolume</code> <code>volume/pvi</code> <code>pvi</code>"},{"location":"api/ta/#oscillators-signalflowtavolumeoscillators","title":"Oscillators (<code>signalflow.ta.volume.oscillators</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>MfiVolume</code> <code>volume/mfi</code> <code>period=14</code> <code>mfi_{period}</code> <code>CmfVolume</code> <code>volume/cmf</code> <code>period=20</code> <code>cmf_{period}</code> <code>EfiVolume</code> <code>volume/efi</code> <code>period=13</code> <code>efi_{period}</code> <code>EomVolume</code> <code>volume/eom</code> <code>period=14</code> <code>eom_{period}</code> <code>KvoVolume</code> <code>volume/kvo</code> <code>fast=34</code>, <code>slow=55</code>, <code>signal=13</code> <code>kvo</code>, <code>kvo_signal</code>"},{"location":"api/ta/#dynamics-physics-based-signalflowtavolumedynamics","title":"Dynamics \u2014 Physics-Based (<code>signalflow.ta.volume.dynamics</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>MarketForceVolume</code> <code>volume/market_force</code> <code>period=14</code> <code>market_force_{period}</code> <code>ImpulseVolume</code> <code>volume/impulse</code> <code>period=14</code> <code>impulse_{period}</code> <code>MarketMomentumVolume</code> <code>volume/market_momentum</code> <code>period=14</code> <code>market_momentum_{period}</code> <code>MarketPowerVolume</code> <code>volume/market_power</code> <code>period=14</code> <code>market_power_{period}</code> <code>MarketCapacitanceVolume</code> <code>volume/market_capacitance</code> <code>period=14</code> <code>market_capacitance_{period}</code> <code>GravitationalPullVolume</code> <code>volume/gravitational_pull</code> <code>period=14</code> <code>gravitational_pull_{period}</code>"},{"location":"api/ta/#trend","title":"Trend","text":"<p><code>signalflow.ta.trend</code> \u2014 22 indicators</p>"},{"location":"api/ta/#strength-signalflowtatrendstrength","title":"Strength (<code>signalflow.ta.trend.strength</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>AdxTrend</code> <code>trend/adx</code> <code>period=14</code> <code>adx_{period}</code>, <code>di_plus</code>, <code>di_minus</code> <code>AroonTrend</code> <code>trend/aroon</code> <code>period=25</code> <code>aroon_up</code>, <code>aroon_down</code>, <code>aroon_osc</code> <code>VortexTrend</code> <code>trend/vortex</code> <code>period=14</code> <code>vi_plus</code>, <code>vi_minus</code> <code>VhfTrend</code> <code>trend/vhf</code> <code>period=28</code> <code>vhf_{period}</code> <code>ChopTrend</code> <code>trend/chop</code> <code>period=14</code> <code>chop_{period}</code> <code>ViscosityTrend</code> <code>trend/viscosity</code> <code>period=20</code> <code>viscosity_{period}</code> <code>ReynoldsTrend</code> <code>trend/reynolds</code> <code>period=20</code> <code>reynolds_{period}</code> <code>RotationalInertiaTrend</code> <code>trend/rotational_inertia</code> <code>period=20</code> <code>rotational_inertia_{period}</code> <code>MarketImpedanceTrend</code> <code>trend/market_impedance</code> <code>period=20</code> <code>market_impedance_{period}</code> <code>RCTimeConstantTrend</code> <code>trend/rc_time_constant</code> <code>period=20</code> <code>rc_time_constant_{period}</code> <code>SNRTrend</code> <code>trend/snr</code> <code>period=20</code> <code>snr_{period}</code> <code>OrderParameterTrend</code> <code>trend/order_parameter</code> <code>period=20</code> <code>order_parameter_{period}</code> <code>SusceptibilityTrend</code> <code>trend/susceptibility</code> <code>period=20</code> <code>susceptibility_{period}</code>"},{"location":"api/ta/#stops-signalflowtatrendstops","title":"Stops (<code>signalflow.ta.trend.stops</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>PsarTrend</code> <code>trend/psar</code> <code>af=0.02</code>, <code>max_af=0.2</code> <code>psar</code> <code>SupertrendTrend</code> <code>trend/supertrend</code> <code>period=10</code>, <code>multiplier=3.0</code> <code>supertrend</code>, <code>supertrend_dir</code> <code>ChandelierTrend</code> <code>trend/chandelier</code> <code>period=22</code>, <code>multiplier=3.0</code> <code>chandelier_long</code>, <code>chandelier_short</code> <code>HiloTrend</code> <code>trend/hilo</code> <code>period=13</code> <code>hilo</code> <code>CkspTrend</code> <code>trend/cksp</code> <code>p=10</code>, <code>q=3</code>, <code>x=1</code> <code>cksp_stop_long</code>, <code>cksp_stop_short</code>"},{"location":"api/ta/#detection-signalflowtatrenddetection","title":"Detection (<code>signalflow.ta.trend.detection</code>)","text":"Class <code>@sf_component</code> Parameters Output <code>IchimokuTrend</code> <code>trend/ichimoku</code> <code>tenkan=9</code>, <code>kijun=26</code>, <code>senkou=52</code> <code>tenkan_sen</code>, <code>kijun_sen</code>, <code>senkou_a</code>, <code>senkou_b</code>, <code>chikou</code> <code>DpoTrend</code> <code>trend/dpo</code> <code>period=20</code> <code>dpo_{period}</code> <code>QstickTrend</code> <code>trend/qstick</code> <code>period=14</code> <code>qstick_{period}</code> <code>TtmTrend</code> <code>trend/ttm</code> <code>period=20</code> <code>ttm_squeeze</code>"},{"location":"api/ta/#statistics","title":"Statistics","text":"<p><code>signalflow.ta.stat</code> \u2014 73 indicators across 11 submodules</p>"},{"location":"api/ta/#dispersion","title":"Dispersion","text":"Class <code>@sf_component</code> Description <code>VarianceStat</code> <code>stat/variance</code> Rolling variance <code>StdStat</code> <code>stat/std</code> Rolling standard deviation <code>MadStat</code> <code>stat/mad</code> Median absolute deviation <code>ZscoreStat</code> <code>stat/zscore</code> Rolling z-score <code>CvStat</code> <code>stat/cv</code> Coefficient of variation <code>RangeStat</code> <code>stat/range</code> Rolling range (max - min) <code>IqrStat</code> <code>stat/iqr</code> Interquartile range <code>AadStat</code> <code>stat/aad</code> Average absolute deviation <code>RobustZscoreStat</code> <code>stat/robust_zscore</code> Median-based z-score"},{"location":"api/ta/#distribution","title":"Distribution","text":"Class <code>@sf_component</code> Description <code>MedianStat</code> <code>stat/median</code> Rolling median <code>QuantileStat</code> <code>stat/quantile</code> Rolling quantile <code>PctRankStat</code> <code>stat/pct_rank</code> Percentile rank <code>MinMaxStat</code> <code>stat/minmax</code> Min-max normalization <code>SkewStat</code> <code>stat/skew</code> Rolling skewness <code>KurtosisStat</code> <code>stat/kurtosis</code> Rolling kurtosis <code>EntropyStat</code> <code>stat/entropy</code> Shannon entropy <code>JarqueBeraStat</code> <code>stat/jarque_bera</code> Normality test statistic <code>ModeDistanceStat</code> <code>stat/mode_distance</code> Distance from mode <code>AboveMeanRatioStat</code> <code>stat/above_mean_ratio</code> Proportion above mean <code>EntropyRateStat</code> <code>stat/entropy_rate</code> Entropy change rate"},{"location":"api/ta/#memory-diffusion","title":"Memory &amp; Diffusion","text":"Class <code>@sf_component</code> Description <code>HurstStat</code> <code>stat/hurst</code> Hurst exponent (mean-reversion vs trend) <code>AutocorrStat</code> <code>stat/autocorr</code> Autocorrelation <code>VarianceRatioStat</code> <code>stat/variance_ratio</code> Lo-MacKinlay variance ratio <code>DiffusionCoeffStat</code> <code>stat/diffusion_coeff</code> Diffusion coefficient <code>AnomalousDiffusionStat</code> <code>stat/anomalous_diffusion</code> Anomalous diffusion exponent <code>MsdRatioStat</code> <code>stat/msd_ratio</code> Mean squared displacement ratio <code>SpringConstantStat</code> <code>stat/spring_constant</code> Harmonic oscillator spring constant <code>DampingRatioStat</code> <code>stat/damping_ratio</code> Oscillation damping <code>NaturalFrequencyStat</code> <code>stat/natural_frequency</code> Natural oscillation frequency <code>PlasticStrainStat</code> <code>stat/plastic_strain</code> Irreversible deformation <code>EscapeVelocityStat</code> <code>stat/escape_velocity</code> Breakout threshold <code>CorrelationLengthStat</code> <code>stat/correlation_length</code> Spatial correlation decay"},{"location":"api/ta/#cycle-analysis","title":"Cycle Analysis","text":"Class <code>@sf_component</code> Description <code>InstAmplitudeStat</code> <code>stat/inst_amplitude</code> Instantaneous amplitude via Hilbert <code>InstPhaseStat</code> <code>stat/inst_phase</code> Instantaneous phase <code>InstFrequencyStat</code> <code>stat/inst_frequency</code> Instantaneous frequency <code>PhaseAccelerationStat</code> <code>stat/phase_acceleration</code> Phase acceleration <code>ConstructiveInterferenceStat</code> <code>stat/constructive_interference</code> Wave interference measure <code>BeatFrequencyStat</code> <code>stat/beat_frequency</code> Beat frequency from phase <code>StandingWaveRatioStat</code> <code>stat/standing_wave_ratio</code> Standing wave ratio <code>SpectralCentroidStat</code> <code>stat/spectral_centroid</code> Frequency center of mass <code>SpectralEntropyStat</code> <code>stat/spectral_entropy</code> Spectral entropy"},{"location":"api/ta/#complexity","title":"Complexity","text":"Class <code>@sf_component</code> Description <code>PermutationEntropyStat</code> <code>stat/permutation_entropy</code> Bandt-Pompe permutation entropy <code>SampleEntropyStat</code> <code>stat/sample_entropy</code> Sample entropy <code>LempelZivStat</code> <code>stat/lempel_ziv</code> Lempel-Ziv complexity <code>FisherInformationStat</code> <code>stat/fisher_information</code> Fisher information <code>DfaExponentStat</code> <code>stat/dfa_exponent</code> Detrended fluctuation analysis"},{"location":"api/ta/#information-theory","title":"Information Theory","text":"Class <code>@sf_component</code> Description <code>KLDivergenceStat</code> <code>stat/kl_divergence</code> Kullback-Leibler divergence <code>JSDivergenceStat</code> <code>stat/js_divergence</code> Jensen-Shannon divergence <code>RenyiEntropyStat</code> <code>stat/renyi_entropy</code> Renyi entropy <code>AutoMutualInfoStat</code> <code>stat/auto_mutual_info</code> Auto mutual information <code>RelativeInfoGainStat</code> <code>stat/relative_info_gain</code> Relative information gain"},{"location":"api/ta/#regression","title":"Regression","text":"Class <code>@sf_component</code> Description <code>CorrelationStat</code> <code>stat/correlation</code> Rolling Pearson correlation <code>BetaStat</code> <code>stat/beta</code> Rolling beta (vs benchmark) <code>RSquaredStat</code> <code>stat/r_squared</code> R-squared <code>LinRegSlopeStat</code> <code>stat/linreg_slope</code> Linear regression slope <code>LinRegInterceptStat</code> <code>stat/linreg_intercept</code> Linear regression intercept <code>LinRegResidualStat</code> <code>stat/linreg_residual</code> Linear regression residual"},{"location":"api/ta/#realized-volatility","title":"Realized Volatility","text":"Class <code>@sf_component</code> Description <code>RealizedVolStat</code> <code>stat/realized_vol</code> Realized (close-to-close) volatility <code>ParkinsonVolStat</code> <code>stat/parkinson_vol</code> Parkinson range-based volatility <code>GarmanKlassVolStat</code> <code>stat/garman_klass_vol</code> Garman-Klass volatility <code>RogersSatchellVolStat</code> <code>stat/rogers_satchell_vol</code> Rogers-Satchell volatility <code>YangZhangVolStat</code> <code>stat/yang_zhang_vol</code> Yang-Zhang volatility"},{"location":"api/ta/#dsp-signal-processing","title":"DSP / Signal Processing","text":"Class <code>@sf_component</code> Description <code>SpectralFluxStat</code> <code>stat/spectral_flux</code> Spectral flux <code>ZeroCrossingRateStat</code> <code>stat/zero_crossing_rate</code> Zero crossing rate <code>SpectralRolloffStat</code> <code>stat/spectral_rolloff</code> Spectral rolloff frequency <code>SpectralFlatnessStat</code> <code>stat/spectral_flatness</code> Wiener entropy <code>PowerCepstrumStat</code> <code>stat/power_cepstrum</code> Power cepstrum"},{"location":"api/ta/#control-theory","title":"Control Theory","text":"Class <code>@sf_component</code> Description <code>KalmanInnovationStat</code> <code>stat/kalman_innovation</code> Kalman filter innovation <code>ARCoefficientStat</code> <code>stat/ar_coefficient</code> Autoregressive coefficient <code>LyapunovExponentStat</code> <code>stat/lyapunov_exponent</code> Largest Lyapunov exponent <code>PIDErrorStat</code> <code>stat/pid_error</code> PID controller error <code>PredictionErrorDecompositionStat</code> <code>stat/prediction_error_decomposition</code> Prediction error decomposition"},{"location":"api/ta/#cross-sectional","title":"Cross-Sectional","text":"Class <code>@sf_component</code> Description <code>CrossSectionalStat</code> <code>stat/cross_sectional</code> Cross-sectional aggregation"},{"location":"api/ta/#performance","title":"Performance","text":"<p><code>signalflow.ta.performance</code> \u2014 2 indicators</p> Class <code>@sf_component</code> Parameters Output <code>LogReturn</code> <code>performance/log_return</code> <code>period=1</code> <code>log_return_{period}</code> <code>PctReturn</code> <code>performance/pct_return</code> <code>period=1</code> <code>pct_return_{period}</code>"},{"location":"api/ta/#divergence","title":"Divergence","text":"<p><code>signalflow.ta.divergence</code> \u2014 2 detectors</p> Class <code>@sf_component</code> Description <code>RsiDivergence</code> <code>divergence/rsi</code> RSI-based divergence detector (regular &amp; hidden) <code>MacdDivergence</code> <code>divergence/macd</code> MACD histogram divergence detector"},{"location":"api/ta/#common-parameters","title":"Common Parameters","text":"<p>All indicators inherit these from <code>Feature</code>:</p> Parameter Type Default Description <code>normalized</code> <code>bool</code> <code>False</code> Enable normalization (z-score for unbounded, linear for bounded) <code>norm_period</code> <code>int \\| None</code> <code>None</code> Custom normalization window (auto-computed if None) <code>group_col</code> <code>str</code> <code>\"pair\"</code> Column for per-asset grouping <code>ts_col</code> <code>str</code> <code>\"timestamp\"</code> Timestamp column <p>Each indicator also has a <code>warmup</code> property indicating the minimum bars needed for stable output.</p>"},{"location":"api/validator/","title":"Validator Module","text":"<p>Signal validators (meta-labelers) predict the quality or risk of trading signals. In De Prado's terminology, this implements the meta-labeling approach - training a secondary model to predict whether a primary signal will be successful.</p>"},{"location":"api/validator/#available-validators","title":"Available Validators","text":"Validator Model Best For <code>LightGBMValidator</code> LGBMClassifier Fast training, good defaults <code>XGBoostValidator</code> XGBClassifier Robust, regularized <code>RandomForestValidator</code> RandomForestClassifier Ensemble, interpretable <code>LogisticRegressionValidator</code> LogisticRegression Fast, linear relationships <code>SVMValidator</code> SVC Small datasets, non-linear <code>AutoSelectValidator</code> Auto Automatic model selection"},{"location":"api/validator/#quick-start","title":"Quick Start","text":"<pre><code>import polars as pl\nfrom signalflow.validator import LightGBMValidator\nfrom signalflow.core import Signals\n\n# Prepare data - filter to active signals (not NONE)\ntrain_df = train_df.filter(pl.col(\"signal_type\") != \"none\")\n\n# Create and train validator\nvalidator = LightGBMValidator(n_estimators=200, learning_rate=0.05)\nvalidator.fit(\n    train_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    train_df.select(\"label\"),\n    X_val=val_df.select([\"pair\", \"timestamp\"] + feature_cols),  # Early stopping\n    y_val=val_df.select(\"label\"),\n)\n\n# Validate new signals\nvalidated = validator.validate_signals(\n    Signals(test_df.select(signal_cols)),\n    test_df.select([\"pair\", \"timestamp\"] + feature_cols),\n)\n\n# Filter to high-confidence predictions\nconfident = validated.value.filter(pl.col(\"probability_rise\") &gt; 0.7)\n</code></pre>"},{"location":"api/validator/#auto-model-selection","title":"Auto Model Selection","text":"<p>Use <code>AutoSelectValidator</code> to automatically select the best model:</p> <pre><code>from signalflow.validator import AutoSelectValidator\n\nvalidator = AutoSelectValidator()\nvalidator.fit(X_train, y_train)\n\n# Check which model was selected\nprint(validator.selected_validator)  # e.g., LightGBMValidator\n</code></pre>"},{"location":"api/validator/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Each validator supports Optuna-based hyperparameter tuning:</p> <pre><code>from signalflow.validator import RandomForestValidator\n\nvalidator = RandomForestValidator()\nvalidator.tune_params = {\"n_trials\": 50, \"cv_folds\": 5, \"timeout\": 600}\nbest_params = validator.tune(X_train, y_train)\n\n# Fit with best params\nvalidator.fit(X_train, y_train)\n</code></pre>"},{"location":"api/validator/#base-class","title":"Base Class","text":""},{"location":"api/validator/#signalflow.validator.base.SignalValidator","title":"signalflow.validator.base.SignalValidator  <code>dataclass</code>","text":"<pre><code>SignalValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp')\n</code></pre> <p>Base class for signal validators (meta-labelers).</p> <p>Validates trading signals by predicting their risk/quality. In De Prado's terminology - this is a meta-labeler.</p> <p>Note: Filtering to active signals (RISE/FALL only) should be done BEFORE passing data to fit. This keeps the validator simple and gives users full control over data preparation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any | None</code> <p>The trained model instance</p> <code>model_type</code> <code>str | None</code> <p>String identifier for model type (e.g., \"lightgbm\", \"xgboost\")</p> <code>model_params</code> <code>dict | None</code> <p>Parameters for model initialization</p> <code>train_params</code> <code>dict | None</code> <p>Parameters for training (e.g., early stopping)</p> <code>tune_enabled</code> <code>bool</code> <p>Whether hyperparameter tuning is enabled</p> <code>tune_params</code> <code>dict | None</code> <p>Parameters for tuning (e.g., n_trials, cv_folds)</p> <code>feature_columns</code> <code>list[str] | None</code> <p>List of feature column names (set after fit)</p>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = VALIDATOR\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.feature_columns","title":"feature_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_columns: list[str] | None = field(default=None, repr=False)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.model","title":"model  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model: Any | None = None\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.model_params","title":"model_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_params: dict | None = None\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.model_type","title":"model_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>model_type: str | None = None\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.train_params","title":"train_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>train_params: dict | None = None\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.tune_enabled","title":"tune_enabled  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_enabled: bool = False\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.tune_params","title":"tune_params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_params: dict | None = None\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.fit","title":"fit","text":"<pre><code>fit(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; SignalValidator\n</code></pre> <p>Train the validator model.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>Training features (Polars DataFrame)</p> required <code>y_train</code> <code>DataFrame | Series</code> <p>Training labels</p> required <code>X_val</code> <code>DataFrame | None</code> <p>Validation features (optional)</p> <code>None</code> <code>y_val</code> <code>DataFrame | Series | None</code> <p>Validation labels (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>SignalValidator</code> <p>Self for method chaining</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def fit(\n    self,\n    X_train: pl.DataFrame,\n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None,\n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; \"SignalValidator\":\n    \"\"\"Train the validator model.\n\n    Args:\n        X_train: Training features (Polars DataFrame)\n        y_train: Training labels\n        X_val: Validation features (optional)\n        y_val: Validation labels (optional)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement fit()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(path: str | Path) -&gt; SignalValidator\n</code></pre> <p>Load model from file.</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>@classmethod\ndef load(cls, path: str | Path) -&gt; \"SignalValidator\":\n    \"\"\"Load model from file.\"\"\"\n    raise NotImplementedError(\"Subclasses must implement load()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.predict","title":"predict","text":"<pre><code>predict(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class labels and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>X</code> <code>DataFrame</code> <p>Features (Polars DataFrame) with (pair, timestamp) + feature columns</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with prediction column added</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def predict(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class labels and return updated Signals.\n\n    Args:\n        signals: Input signals container\n        X: Features (Polars DataFrame) with (pair, timestamp) + feature columns\n\n    Returns:\n        New Signals with prediction column added\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement predict()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class probabilities and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>X</code> <code>DataFrame</code> <p>Features (Polars DataFrame)</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with probability columns added</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def predict_proba(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class probabilities and return updated Signals.\n\n    Args:\n        signals: Input signals container\n        X: Features (Polars DataFrame)\n\n    Returns:\n        New Signals with probability columns added\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement predict_proba()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.save","title":"save","text":"<pre><code>save(path: str | Path) -&gt; None\n</code></pre> <p>Save model to file.</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save model to file.\"\"\"\n    raise NotImplementedError(\"Subclasses must implement save()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.tune","title":"tune","text":"<pre><code>tune(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Tune hyperparameters.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Best parameters found</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def tune(\n    self,\n    X_train: pl.DataFrame,\n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None,\n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Tune hyperparameters.\n\n    Returns:\n        Best parameters found\n    \"\"\"\n    if not self.tune_enabled:\n        raise ValueError(\"Tuning is not enabled for this validator\")\n    raise NotImplementedError(\"Subclasses must implement tune()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.validate_signals","title":"validate_signals","text":"<pre><code>validate_signals(signals: Signals, features: DataFrame, prefix: str = 'probability_') -&gt; Signals\n</code></pre> <p>Add validation predictions to signals.</p> <p>Convenience method - calls predict_proba internally.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>features</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + feature columns</p> required <code>prefix</code> <code>str</code> <p>Prefix for probability columns</p> <code>'probability_'</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with added validation columns</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def validate_signals(\n    self,\n    signals: Signals,\n    features: pl.DataFrame,\n    prefix: str = \"probability_\",\n) -&gt; Signals:\n    \"\"\"Add validation predictions to signals.\n\n    Convenience method - calls predict_proba internally.\n\n    Args:\n        signals: Input signals container\n        features: Features DataFrame with (pair, timestamp) + feature columns\n        prefix: Prefix for probability columns\n\n    Returns:\n        Signals with added validation columns\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement validate_signals()\")\n</code></pre>"},{"location":"api/validator/#sklearn-base","title":"Sklearn Base","text":""},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase","title":"signalflow.validator.sklearn_validator.SklearnValidatorBase  <code>dataclass</code>","text":"<pre><code>SklearnValidatorBase(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', tune_metric: str = 'roc_auc', tune_cv_folds: int = 5, tune_n_trials: int = 50, tune_timeout: int = 600, early_stopping_rounds: int = 50)\n</code></pre> <p>               Bases: <code>SignalValidator</code></p> <p>Base class for sklearn-compatible signal validators.</p> <p>Provides common functionality for feature extraction, fitting, prediction, and serialization. Subclasses define model-specific configuration via class variables.</p> <p>Class Variables (override in subclasses):     _model_class: Import path for the model class (e.g., \"lightgbm.LGBMClassifier\")     _default_params: Default model parameters     _tune_space: Optuna tuning space definition     _supports_early_stopping: Whether the model supports early stopping</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._default_params","title":"_default_params  <code>class-attribute</code>","text":"<pre><code>_default_params: dict[str, Any] = {}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._model_class","title":"_model_class  <code>class-attribute</code>","text":"<pre><code>_model_class: str = ''\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._supports_early_stopping","title":"_supports_early_stopping  <code>class-attribute</code>","text":"<pre><code>_supports_early_stopping: bool = False\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._tune_space","title":"_tune_space  <code>class-attribute</code>","text":"<pre><code>_tune_space: dict[str, tuple] = {}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.early_stopping_rounds","title":"early_stopping_rounds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>early_stopping_rounds: int = 50\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.tune_cv_folds","title":"tune_cv_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_cv_folds: int = 5\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.tune_metric","title":"tune_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_metric: str = 'roc_auc'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.tune_n_trials","title":"tune_n_trials  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_n_trials: int = 50\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.tune_timeout","title":"tune_timeout  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>tune_timeout: int = 600\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.model_params is None:\n        self.model_params = {}\n    if self.train_params is None:\n        self.train_params = {}\n    if self.tune_params is None:\n        self.tune_params = {\n            \"n_trials\": self.tune_n_trials,\n            \"cv_folds\": self.tune_cv_folds,\n            \"timeout\": self.tune_timeout,\n        }\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._create_model","title":"_create_model","text":"<pre><code>_create_model(params: dict | None = None) -&gt; Any\n</code></pre> <p>Create model instance with given parameters.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def _create_model(self, params: dict | None = None) -&gt; Any:\n    \"\"\"Create model instance with given parameters.\"\"\"\n    model_class = import_model_class(self._model_class)\n\n    final_params = {**self._default_params}\n    if self.model_params:\n        final_params.update(self.model_params)\n    if params:\n        final_params.update(params)\n\n    return model_class(**final_params)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._extract_features","title":"_extract_features","text":"<pre><code>_extract_features(X: DataFrame, fit_mode: bool = False) -&gt; np.ndarray\n</code></pre> <p>Extract feature matrix from DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>Input DataFrame</p> required <code>fit_mode</code> <code>bool</code> <p>If True, infer and store feature columns</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Feature matrix as numpy array</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def _extract_features(\n    self,\n    X: pl.DataFrame,\n    fit_mode: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"Extract feature matrix from DataFrame.\n\n    Args:\n        X: Input DataFrame\n        fit_mode: If True, infer and store feature columns\n\n    Returns:\n        Feature matrix as numpy array\n    \"\"\"\n    exclude_cols = {self.pair_col, self.ts_col}\n\n    if fit_mode:\n        self.feature_columns = [c for c in X.columns if c not in exclude_cols]\n\n    if self.feature_columns is None:\n        raise ValueError(\"feature_columns not set. Call fit() first.\")\n\n    missing = set(self.feature_columns) - set(X.columns)\n    if missing:\n        raise ValueError(f\"Missing feature columns: {sorted(missing)}\")\n\n    return X.select(self.feature_columns).to_numpy()\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._extract_labels","title":"_extract_labels","text":"<pre><code>_extract_labels(y: DataFrame | Series) -&gt; np.ndarray\n</code></pre> <p>Extract label array.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def _extract_labels(self, y: pl.DataFrame | pl.Series) -&gt; np.ndarray:\n    \"\"\"Extract label array.\"\"\"\n    if isinstance(y, pl.DataFrame):\n        if y.width == 1:\n            return y.to_numpy().ravel()\n        elif \"label\" in y.columns:\n            return y[\"label\"].to_numpy()\n        else:\n            raise ValueError(\"y DataFrame must have single column or 'label' column\")\n    return y.to_numpy()\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._get_class_labels","title":"_get_class_labels","text":"<pre><code>_get_class_labels() -&gt; list[str]\n</code></pre> <p>Get class labels for probability columns.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def _get_class_labels(self) -&gt; list[str]:\n    \"\"\"Get class labels for probability columns.\"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not fitted.\")\n\n    classes = getattr(self.model, \"classes_\", None)\n    if classes is None:\n        return [\"none\", \"rise\", \"fall\"]\n\n    _legacy_map = {0: \"none\", 1: \"rise\", 2: \"fall\"}\n    return [_legacy_map.get(c, str(c)) for c in classes]\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase._get_early_stopping_kwargs","title":"_get_early_stopping_kwargs","text":"<pre><code>_get_early_stopping_kwargs(X_val: ndarray, y_val: ndarray) -&gt; dict[str, Any]\n</code></pre> <p>Get early stopping fit kwargs. Override in subclasses.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def _get_early_stopping_kwargs(\n    self,\n    X_val: np.ndarray,\n    y_val: np.ndarray,\n) -&gt; dict[str, Any]:\n    \"\"\"Get early stopping fit kwargs. Override in subclasses.\"\"\"\n    return {}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.fit","title":"fit","text":"<pre><code>fit(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; SklearnValidatorBase\n</code></pre> <p>Train the validator.</p> <p>Note: Filter to active signals BEFORE calling this method.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>Training features (already filtered to active signals)</p> required <code>y_train</code> <code>DataFrame | Series</code> <p>Training labels</p> required <code>X_val</code> <code>DataFrame | None</code> <p>Validation features (optional, for early stopping)</p> <code>None</code> <code>y_val</code> <code>DataFrame | Series | None</code> <p>Validation labels (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>SklearnValidatorBase</code> <p>Self for method chaining</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def fit(\n    self,\n    X_train: pl.DataFrame,\n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None,\n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; \"SklearnValidatorBase\":\n    \"\"\"Train the validator.\n\n    Note: Filter to active signals BEFORE calling this method.\n\n    Args:\n        X_train: Training features (already filtered to active signals)\n        y_train: Training labels\n        X_val: Validation features (optional, for early stopping)\n        y_val: Validation labels (optional)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    X_np = self._extract_features(X_train, fit_mode=True)\n    y_np = self._extract_labels(y_train)\n\n    self.model = self._create_model()\n\n    fit_kwargs: dict[str, Any] = {}\n\n    if X_val is not None and y_val is not None and self._supports_early_stopping:\n        X_val_np = self._extract_features(X_val)\n        y_val_np = self._extract_labels(y_val)\n        fit_kwargs = self._get_early_stopping_kwargs(X_val_np, y_val_np)\n\n    self.model.fit(X_np, y_np, **fit_kwargs)\n\n    return self\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(path: str | Path) -&gt; SklearnValidatorBase\n</code></pre> <p>Load validator from file.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>@classmethod\ndef load(cls, path: str | Path) -&gt; \"SklearnValidatorBase\":\n    \"\"\"Load validator from file.\"\"\"\n    path = Path(path)\n\n    with open(path, \"rb\") as f:\n        state = pickle.load(f)\n\n    validator = cls(\n        model=state[\"model\"],\n        model_params=state[\"model_params\"],\n        train_params=state[\"train_params\"],\n        tune_params=state[\"tune_params\"],\n        feature_columns=state[\"feature_columns\"],\n        pair_col=state.get(\"pair_col\", \"pair\"),\n        ts_col=state.get(\"ts_col\", \"timestamp\"),\n    )\n\n    return validator\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.predict","title":"predict","text":"<pre><code>predict(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class labels and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>X</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + feature columns</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with 'validation_pred' column added</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def predict(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class labels and return updated Signals.\n\n    Args:\n        signals: Input signals container\n        X: Features DataFrame with (pair, timestamp) + feature columns\n\n    Returns:\n        New Signals with 'validation_pred' column added\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not fitted. Call fit() first.\")\n\n    signals_df = signals.value\n\n    X_matched = signals_df.select([self.pair_col, self.ts_col]).join(\n        X,\n        on=[self.pair_col, self.ts_col],\n        how=\"left\",\n    )\n\n    X_np = self._extract_features(X_matched)\n    predictions = self.model.predict(X_np)\n\n    result_df = signals_df.with_columns(pl.Series(name=\"validation_pred\", values=predictions))\n\n    return Signals(result_df)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class probabilities and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>X</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + feature columns</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with probability columns added</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def predict_proba(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class probabilities and return updated Signals.\n\n    Args:\n        signals: Input signals container\n        X: Features DataFrame with (pair, timestamp) + feature columns\n\n    Returns:\n        New Signals with probability columns added\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not fitted. Call fit() first.\")\n\n    signals_df = signals.value\n    classes = self._get_class_labels()\n\n    X_matched = signals_df.select([self.pair_col, self.ts_col]).join(\n        X,\n        on=[self.pair_col, self.ts_col],\n        how=\"left\",\n    )\n\n    X_np = self._extract_features(X_matched)\n    probas = self.model.predict_proba(X_np)\n\n    result_df = signals_df\n    for i, class_label in enumerate(classes):\n        col_name = f\"probability_{class_label}\"\n        result_df = result_df.with_columns(pl.Series(name=col_name, values=probas[:, i]))\n\n    return Signals(result_df)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.save","title":"save","text":"<pre><code>save(path: str | Path) -&gt; None\n</code></pre> <p>Save validator to file.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save validator to file.\"\"\"\n    path = Path(path)\n\n    state = {\n        \"validator_class\": self.__class__.__name__,\n        \"model\": self.model,\n        \"model_params\": self.model_params,\n        \"train_params\": self.train_params,\n        \"tune_params\": self.tune_params,\n        \"feature_columns\": self.feature_columns,\n        \"pair_col\": self.pair_col,\n        \"ts_col\": self.ts_col,\n    }\n\n    with open(path, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.tune","title":"tune","text":"<pre><code>tune(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Tune hyperparameters using Optuna.</p> <p>Note: Filter to active signals BEFORE calling this method.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Best parameters found</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def tune(\n    self,\n    X_train: pl.DataFrame,\n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None,\n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Tune hyperparameters using Optuna.\n\n    Note: Filter to active signals BEFORE calling this method.\n\n    Returns:\n        Best parameters found\n    \"\"\"\n    import optuna\n    from sklearn.model_selection import cross_val_score\n\n    X_np = self._extract_features(X_train, fit_mode=True)\n    y_np = self._extract_labels(y_train)\n\n    n_trials = self.tune_params.get(\"n_trials\", self.tune_n_trials)\n    cv_folds = self.tune_params.get(\"cv_folds\", self.tune_cv_folds)\n    timeout = self.tune_params.get(\"timeout\", self.tune_timeout)\n\n    def objective(trial: optuna.Trial) -&gt; float:\n        params = build_optuna_params(trial, self._tune_space)\n        model = self._create_model(params)\n\n        scores = cross_val_score(\n            model,\n            X_np,\n            y_np,\n            cv=cv_folds,\n            scoring=self.tune_metric,\n            n_jobs=-1,\n        )\n        return scores.mean()\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(\n        objective,\n        n_trials=n_trials,\n        timeout=timeout,\n        show_progress_bar=True,\n    )\n\n    best_params = {**self._default_params, **study.best_params}\n    self.model_params = best_params\n\n    return best_params\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnValidatorBase.validate_signals","title":"validate_signals","text":"<pre><code>validate_signals(signals: Signals, features: DataFrame, prefix: str = 'probability_') -&gt; Signals\n</code></pre> <p>Add validation probabilities to signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input Signals container</p> required <code>features</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + features</p> required <code>prefix</code> <code>str</code> <p>Prefix for probability columns (default: \"probability_\")</p> <code>'probability_'</code> <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with probability columns added.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def validate_signals(\n    self,\n    signals: Signals,\n    features: pl.DataFrame,\n    prefix: str = \"probability_\",\n) -&gt; Signals:\n    \"\"\"Add validation probabilities to signals.\n\n    Args:\n        signals: Input Signals container\n        features: Features DataFrame with (pair, timestamp) + features\n        prefix: Prefix for probability columns (default: \"probability_\")\n\n    Returns:\n        New Signals with probability columns added.\n    \"\"\"\n    return self.predict_proba(signals, features)\n</code></pre>"},{"location":"api/validator/#lightgbm-validator","title":"LightGBM Validator","text":""},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator","title":"signalflow.validator.sklearn_validator.LightGBMValidator  <code>dataclass</code>","text":"<pre><code>LightGBMValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', tune_metric: str = 'roc_auc', tune_cv_folds: int = 5, tune_n_trials: int = 50, tune_timeout: int = 600, early_stopping_rounds: int = 50, n_estimators: int = 100, max_depth: int = 6, learning_rate: float = 0.1, num_leaves: int = 31)\n</code></pre> <p>               Bases: <code>SklearnValidatorBase</code></p> <p>LightGBM-based signal validator.</p> <p>Gradient boosting model optimized for speed and performance. Supports early stopping with validation data.</p> Example <p>validator = LightGBMValidator(n_estimators=200) validator.fit(X_train, y_train, X_val, y_val) validated = validator.validate_signals(signals, features)</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator._default_params","title":"_default_params  <code>class-attribute</code>","text":"<pre><code>_default_params: dict[str, Any] = {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'num_leaves': 31, 'min_child_samples': 20, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42, 'n_jobs': -1, 'verbosity': -1}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator._model_class","title":"_model_class  <code>class-attribute</code>","text":"<pre><code>_model_class: str = 'lightgbm.LGBMClassifier'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator._supports_early_stopping","title":"_supports_early_stopping  <code>class-attribute</code>","text":"<pre><code>_supports_early_stopping: bool = True\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator._tune_space","title":"_tune_space  <code>class-attribute</code>","text":"<pre><code>_tune_space: dict[str, tuple] = {'n_estimators': ('int', 50, 500), 'max_depth': ('int', 3, 12), 'learning_rate': ('log_float', 0.01, 0.3), 'num_leaves': ('int', 15, 127), 'min_child_samples': ('int', 5, 100), 'subsample': ('float', 0.6, 1.0), 'colsample_bytree': ('float', 0.6, 1.0)}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.1\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator.max_depth","title":"max_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_depth: int = 6\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator.n_estimators","title":"n_estimators  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_estimators: int = 100\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator.num_leaves","title":"num_leaves  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>num_leaves: int = 31\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    super().__post_init__()\n    # Merge instance params into model_params\n    self.model_params = {\n        **self.model_params,\n        \"n_estimators\": self.n_estimators,\n        \"max_depth\": self.max_depth,\n        \"learning_rate\": self.learning_rate,\n        \"num_leaves\": self.num_leaves,\n    }\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LightGBMValidator._get_early_stopping_kwargs","title":"_get_early_stopping_kwargs","text":"<pre><code>_get_early_stopping_kwargs(X_val: ndarray, y_val: ndarray) -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def _get_early_stopping_kwargs(\n    self,\n    X_val: np.ndarray,\n    y_val: np.ndarray,\n) -&gt; dict[str, Any]:\n    import lightgbm\n\n    return {\n        \"eval_set\": [(X_val, y_val)],\n        \"callbacks\": [lightgbm.early_stopping(self.early_stopping_rounds, verbose=False)],\n    }\n</code></pre>"},{"location":"api/validator/#xgboost-validator","title":"XGBoost Validator","text":""},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator","title":"signalflow.validator.sklearn_validator.XGBoostValidator  <code>dataclass</code>","text":"<pre><code>XGBoostValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', tune_metric: str = 'roc_auc', tune_cv_folds: int = 5, tune_n_trials: int = 50, tune_timeout: int = 600, early_stopping_rounds: int = 50, n_estimators: int = 100, max_depth: int = 6, learning_rate: float = 0.1)\n</code></pre> <p>               Bases: <code>SklearnValidatorBase</code></p> <p>XGBoost-based signal validator.</p> <p>Robust gradient boosting with regularization. Supports early stopping with validation data.</p> Example <p>validator = XGBoostValidator(n_estimators=200) validator.fit(X_train, y_train, X_val, y_val) validated = validator.validate_signals(signals, features)</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator._default_params","title":"_default_params  <code>class-attribute</code>","text":"<pre><code>_default_params: dict[str, Any] = {'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.1, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42, 'n_jobs': -1, 'verbosity': 0, 'use_label_encoder': False, 'eval_metric': 'logloss'}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator._model_class","title":"_model_class  <code>class-attribute</code>","text":"<pre><code>_model_class: str = 'xgboost.XGBClassifier'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator._supports_early_stopping","title":"_supports_early_stopping  <code>class-attribute</code>","text":"<pre><code>_supports_early_stopping: bool = True\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator._tune_space","title":"_tune_space  <code>class-attribute</code>","text":"<pre><code>_tune_space: dict[str, tuple] = {'n_estimators': ('int', 50, 500), 'max_depth': ('int', 3, 12), 'learning_rate': ('log_float', 0.01, 0.3), 'subsample': ('float', 0.6, 1.0), 'colsample_bytree': ('float', 0.6, 1.0), 'min_child_weight': ('int', 1, 10), 'gamma': ('float', 0, 0.5)}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator.learning_rate","title":"learning_rate  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>learning_rate: float = 0.1\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator.max_depth","title":"max_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_depth: int = 6\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator.n_estimators","title":"n_estimators  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_estimators: int = 100\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    super().__post_init__()\n    self.model_params = {\n        **self.model_params,\n        \"n_estimators\": self.n_estimators,\n        \"max_depth\": self.max_depth,\n        \"learning_rate\": self.learning_rate,\n    }\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.XGBoostValidator._get_early_stopping_kwargs","title":"_get_early_stopping_kwargs","text":"<pre><code>_get_early_stopping_kwargs(X_val: ndarray, y_val: ndarray) -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def _get_early_stopping_kwargs(\n    self,\n    X_val: np.ndarray,\n    y_val: np.ndarray,\n) -&gt; dict[str, Any]:\n    return {\n        \"eval_set\": [(X_val, y_val)],\n        \"early_stopping_rounds\": self.early_stopping_rounds,\n        \"verbose\": False,\n    }\n</code></pre>"},{"location":"api/validator/#random-forest-validator","title":"Random Forest Validator","text":""},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator","title":"signalflow.validator.sklearn_validator.RandomForestValidator  <code>dataclass</code>","text":"<pre><code>RandomForestValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', tune_metric: str = 'roc_auc', tune_cv_folds: int = 5, tune_n_trials: int = 50, tune_timeout: int = 600, early_stopping_rounds: int = 50, n_estimators: int = 100, max_depth: int = 10)\n</code></pre> <p>               Bases: <code>SklearnValidatorBase</code></p> <p>Random Forest-based signal validator.</p> <p>Ensemble of decision trees with bagging.</p> Example <p>validator = RandomForestValidator(n_estimators=200) validator.fit(X_train, y_train) validated = validator.validate_signals(signals, features)</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator._default_params","title":"_default_params  <code>class-attribute</code>","text":"<pre><code>_default_params: dict[str, Any] = {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'random_state': 42, 'n_jobs': -1}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator._model_class","title":"_model_class  <code>class-attribute</code>","text":"<pre><code>_model_class: str = 'sklearn.ensemble.RandomForestClassifier'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator._supports_early_stopping","title":"_supports_early_stopping  <code>class-attribute</code>","text":"<pre><code>_supports_early_stopping: bool = False\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator._tune_space","title":"_tune_space  <code>class-attribute</code>","text":"<pre><code>_tune_space: dict[str, tuple] = {'n_estimators': ('int', 50, 300), 'max_depth': ('int', 5, 30), 'min_samples_split': ('int', 2, 20), 'min_samples_leaf': ('int', 1, 10)}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator.max_depth","title":"max_depth  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_depth: int = 10\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator.n_estimators","title":"n_estimators  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>n_estimators: int = 100\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.RandomForestValidator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    super().__post_init__()\n    self.model_params = {\n        **self.model_params,\n        \"n_estimators\": self.n_estimators,\n        \"max_depth\": self.max_depth,\n    }\n</code></pre>"},{"location":"api/validator/#logistic-regression-validator","title":"Logistic Regression Validator","text":""},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator","title":"signalflow.validator.sklearn_validator.LogisticRegressionValidator  <code>dataclass</code>","text":"<pre><code>LogisticRegressionValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', tune_metric: str = 'roc_auc', tune_cv_folds: int = 5, tune_n_trials: int = 50, tune_timeout: int = 600, early_stopping_rounds: int = 50, C: float = 1.0, max_iter: int = 1000)\n</code></pre> <p>               Bases: <code>SklearnValidatorBase</code></p> <p>Logistic Regression-based signal validator.</p> <p>Linear classifier with regularization.</p> Example <p>validator = LogisticRegressionValidator(C=0.1) validator.fit(X_train, y_train) validated = validator.validate_signals(signals, features)</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator.C","title":"C  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>C: float = 1.0\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator._default_params","title":"_default_params  <code>class-attribute</code>","text":"<pre><code>_default_params: dict[str, Any] = {'C': 1.0, 'max_iter': 1000, 'random_state': 42, 'n_jobs': -1}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator._model_class","title":"_model_class  <code>class-attribute</code>","text":"<pre><code>_model_class: str = 'sklearn.linear_model.LogisticRegression'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator._supports_early_stopping","title":"_supports_early_stopping  <code>class-attribute</code>","text":"<pre><code>_supports_early_stopping: bool = False\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator._tune_space","title":"_tune_space  <code>class-attribute</code>","text":"<pre><code>_tune_space: dict[str, tuple] = {'C': ('log_float', 0.0001, 100), 'penalty': ('categorical', ['l1', 'l2']), 'solver': ('categorical', ['saga'])}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator.max_iter","title":"max_iter  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>max_iter: int = 1000\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.LogisticRegressionValidator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    super().__post_init__()\n    self.model_params = {\n        **self.model_params,\n        \"C\": self.C,\n        \"max_iter\": self.max_iter,\n    }\n</code></pre>"},{"location":"api/validator/#svm-validator","title":"SVM Validator","text":""},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator","title":"signalflow.validator.sklearn_validator.SVMValidator  <code>dataclass</code>","text":"<pre><code>SVMValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', tune_metric: str = 'roc_auc', tune_cv_folds: int = 5, tune_n_trials: int = 50, tune_timeout: int = 600, early_stopping_rounds: int = 50, C: float = 1.0, kernel: str = 'rbf')\n</code></pre> <p>               Bases: <code>SklearnValidatorBase</code></p> <p>Support Vector Machine-based signal validator.</p> <p>SVM classifier with RBF kernel by default.</p> Example <p>validator = SVMValidator(C=10.0, kernel=\"rbf\") validator.fit(X_train, y_train) validated = validator.validate_signals(signals, features)</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator.C","title":"C  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>C: float = 1.0\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator._default_params","title":"_default_params  <code>class-attribute</code>","text":"<pre><code>_default_params: dict[str, Any] = {'C': 1.0, 'kernel': 'rbf', 'probability': True, 'random_state': 42}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator._model_class","title":"_model_class  <code>class-attribute</code>","text":"<pre><code>_model_class: str = 'sklearn.svm.SVC'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator._supports_early_stopping","title":"_supports_early_stopping  <code>class-attribute</code>","text":"<pre><code>_supports_early_stopping: bool = False\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator._tune_space","title":"_tune_space  <code>class-attribute</code>","text":"<pre><code>_tune_space: dict[str, tuple] = {'C': ('log_float', 0.001, 100), 'kernel': ('categorical', ['rbf', 'linear', 'poly']), 'gamma': ('categorical', ['scale', 'auto'])}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator.kernel","title":"kernel  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>kernel: str = 'rbf'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SVMValidator.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    super().__post_init__()\n    self.model_params = {\n        **self.model_params,\n        \"C\": self.C,\n        \"kernel\": self.kernel,\n    }\n</code></pre>"},{"location":"api/validator/#auto-select-validator","title":"Auto-Select Validator","text":""},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator","title":"signalflow.validator.sklearn_validator.AutoSelectValidator  <code>dataclass</code>","text":"<pre><code>AutoSelectValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', tune_metric: str = 'roc_auc', tune_cv_folds: int = 5, tune_n_trials: int = 50, tune_timeout: int = 600, early_stopping_rounds: int = 50, auto_select_metric: str = 'roc_auc', auto_select_cv_folds: int = 5, selected_validator: SklearnValidatorBase | None = None)\n</code></pre> <p>               Bases: <code>SklearnValidatorBase</code></p> <p>Auto-selecting signal validator.</p> <p>Automatically selects the best model via cross-validation. Tests LightGBM, XGBoost, Random Forest, and Logistic Regression.</p> Example <p>validator = AutoSelectValidator() validator.fit(X_train, y_train)  # Selects best model print(validator.selected_validator)  # Shows which was selected</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator._default_params","title":"_default_params  <code>class-attribute</code>","text":"<pre><code>_default_params: dict[str, Any] = {}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator._model_class","title":"_model_class  <code>class-attribute</code>","text":"<pre><code>_model_class: str = ''\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator._supports_early_stopping","title":"_supports_early_stopping  <code>class-attribute</code>","text":"<pre><code>_supports_early_stopping: bool = False\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator._tune_space","title":"_tune_space  <code>class-attribute</code>","text":"<pre><code>_tune_space: dict[str, tuple] = {}\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator.auto_select_cv_folds","title":"auto_select_cv_folds  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>auto_select_cv_folds: int = 5\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator.auto_select_metric","title":"auto_select_metric  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>auto_select_metric: str = 'roc_auc'\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator.selected_validator","title":"selected_validator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>selected_validator: SklearnValidatorBase | None = field(default=None, repr=False)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator.fit","title":"fit","text":"<pre><code>fit(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; AutoSelectValidator\n</code></pre> <p>Train the validator, auto-selecting the best model.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def fit(\n    self,\n    X_train: pl.DataFrame,\n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None,\n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; \"AutoSelectValidator\":\n    \"\"\"Train the validator, auto-selecting the best model.\"\"\"\n    from sklearn.model_selection import cross_val_score\n\n    X_np = self._extract_features(X_train, fit_mode=True)\n    y_np = self._extract_labels(y_train)\n\n    best_score = -np.inf\n    best_validator_cls = None\n\n    for validator_cls in AUTO_SELECT_VALIDATORS:\n        try:\n            validator = validator_cls()\n            model = validator._create_model()\n\n            scores = cross_val_score(\n                model,\n                X_np,\n                y_np,\n                cv=self.auto_select_cv_folds,\n                scoring=self.auto_select_metric,\n                n_jobs=-1,\n            )\n            mean_score = scores.mean()\n\n            if mean_score &gt; best_score:\n                best_score = mean_score\n                best_validator_cls = validator_cls\n\n        except ImportError:\n            continue\n        except Exception:\n            continue\n\n    if best_validator_cls is None:\n        raise RuntimeError(\"No suitable model found. Install lightgbm, xgboost, or scikit-learn.\")\n\n    # Create and fit the selected validator\n    self.selected_validator = best_validator_cls(feature_columns=self.feature_columns)\n    self.selected_validator.fit(X_train, y_train, X_val, y_val)\n    self.model = self.selected_validator.model\n\n    return self\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator.predict","title":"predict","text":"<pre><code>predict(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def predict(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    if self.selected_validator is None:\n        raise ValueError(\"Model not fitted. Call fit() first.\")\n    return self.selected_validator.predict(signals, X)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def predict_proba(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    if self.selected_validator is None:\n        raise ValueError(\"Model not fitted. Call fit() first.\")\n    return self.selected_validator.predict_proba(signals, X)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.AutoSelectValidator.tune","title":"tune","text":"<pre><code>tune(*args, **kwargs) -&gt; dict[str, Any]\n</code></pre> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def tune(self, *args, **kwargs) -&gt; dict[str, Any]:\n    raise NotImplementedError(\"AutoSelectValidator does not support tune(). Use a specific validator.\")\n</code></pre>"},{"location":"ecosystem/","title":"SignalFlow Ecosystem","text":"<p>SignalFlow is a modular ecosystem of Python packages for algorithmic trading. Each package focuses on a specific domain while sharing the core framework's component registry, data containers, and pipeline patterns.</p>"},{"location":"ecosystem/#packages","title":"Packages","text":"<ul> <li> <p> signalflow-trading (Core)</p> <p>Core framework: data containers, signal detection, backtesting, strategy execution. Component registry, Polars-first processing, DuckDB storage.</p> <pre><code>pip install signalflow-trading\n</code></pre> <p> GitHub</p> </li> <li> <p> signalflow-ta (Technical Analysis)</p> <p>199+ technical indicators across 8 modules: momentum, overlap, volatility, volume, trend, statistics, performance, divergence. Includes physics-based market analogs and preset pipeline factories.</p> <pre><code>pip install signalflow-ta\n</code></pre> <p> GitHub</p> </li> <li> <p> signalflow-nn (Neural Networks)</p> <p>Deep learning signal validators built on PyTorch Lightning. Encoder + Head composition pattern with LSTM, GRU encoders and MLP, Attention, Residual, Distribution, Ordinal, and Confidence heads.</p> <pre><code>pip install signalflow-nn\n</code></pre> <p> GitHub</p> </li> </ul>"},{"location":"ecosystem/#architecture","title":"Architecture","text":"<p>All packages share the SignalFlow component registry via the <code>@sf_component</code> decorator. This means indicators from signalflow-ta and validators from signalflow-nn are automatically discoverable by the core framework:</p> <pre><code>from signalflow.core import default_registry, SfComponentType\n\n# After installing signalflow-ta, its indicators are available:\nrsi_cls = default_registry.get(SfComponentType.FEATURE, \"momentum/rsi\")\n\n# After installing signalflow-nn, its validators are available:\nvalidator_cls = default_registry.get(SfComponentType.VALIDATOR, \"temporal_validator\")\n</code></pre>"},{"location":"ecosystem/#dependency-chain","title":"Dependency Chain","text":"<pre><code>signalflow-trading          # Core (required)\n\u251c\u2500\u2500 signalflow-ta           # extends with 199+ indicators\n\u2514\u2500\u2500 signalflow-nn           # extends with neural network validators\n</code></pre> <p>Both extension packages depend on <code>signalflow-trading</code> and extend it using Python namespace packages under <code>signalflow.*</code>.</p>"},{"location":"ecosystem/signalflow-nn/","title":"signalflow-nn - Neural Networks","text":"<p>signalflow-nn extends SignalFlow with deep learning signal validators built on PyTorch and Lightning. It provides a composable architecture where encoders and classification heads are mixed and matched via the component registry.</p>"},{"location":"ecosystem/signalflow-nn/#installation","title":"Installation","text":"<pre><code>pip install signalflow-nn\n</code></pre> <p>Requires <code>signalflow-trading &gt;= 1.1.0</code>, <code>torch &gt;= 2.2</code>, <code>lightning &gt;= 2.5</code>.</p> <p>For GPU support: <pre><code># Check CUDA version: nvidia-smi\npip install torch --index-url https://download.pytorch.org/whl/cu121\npip install signalflow-nn\n</code></pre></p>"},{"location":"ecosystem/signalflow-nn/#architecture","title":"Architecture","text":"<p>signalflow-nn uses an Encoder + Head composition pattern:</p> <pre><code>Input [batch, seq_len, features]\n  \u2192 Encoder (LSTM, GRU)\n    \u2192 [batch, embedding_size]\n  \u2192 Head (MLP, Attention, Residual, ...)\n    \u2192 [batch, num_classes]\n</code></pre> <p>Components are loaded from the signalflow registry, making architectures fully configurable:</p> <pre><code>from signalflow.nn.model import TemporalClassificator\n\nmodel = TemporalClassificator(\n    encoder_type=\"encoder/lstm\",\n    encoder_params={\"input_size\": 10, \"hidden_size\": 128, \"num_layers\": 2},\n    head_type=\"head/cls/mlp\",\n    head_params={\"hidden_sizes\": [64, 32]},\n    num_classes=3,\n)\n</code></pre>"},{"location":"ecosystem/signalflow-nn/#quick-start","title":"Quick Start","text":""},{"location":"ecosystem/signalflow-nn/#training-a-neural-validator","title":"Training a Neural Validator","text":"<pre><code>from pathlib import Path\nfrom signalflow.nn.validator import TemporalValidator\nfrom signalflow.nn.data import TimeSeriesPreprocessor, ScalerConfig\n\n# Configure preprocessing\npreprocessor = TimeSeriesPreprocessor(\n    default_config=ScalerConfig(method=\"robust\", scope=\"group\")\n)\n\n# Create validator\nvalidator = TemporalValidator(\n    encoder_type=\"encoder/lstm\",\n    encoder_params={\"input_size\": 10, \"hidden_size\": 64, \"num_layers\": 2},\n    head_type=\"head/cls/mlp\",\n    head_params={\"hidden_sizes\": [128]},\n    num_classes=3,\n    preprocessor=preprocessor,\n    window_size=60,\n)\n\n# Train on signalflow DataFrames\nvalidator.fit(X_train, y_train, log_dir=Path(\"./logs\"))\n\n# Predict - returns Signals with probability columns\nvalidated_signals = validator.validate_signals(signals, features)\n\n# Save/load model\nvalidator.save(\"model.pkl\")\nloaded = TemporalValidator.load(\"model.pkl\")\n</code></pre>"},{"location":"ecosystem/signalflow-nn/#using-temporalclassificator-directly","title":"Using TemporalClassificator Directly","text":"<pre><code>import lightning as L\nfrom signalflow.nn.model import TemporalClassificator\nfrom signalflow.nn.data import SignalDataModule\n\n# Create model\nmodel = TemporalClassificator(\n    encoder_type=\"encoder/gru\",\n    encoder_params={\"input_size\": 15, \"hidden_size\": 128},\n    head_type=\"head/cls/attention\",\n    head_params={\"num_heads\": 4},\n    num_classes=3,\n)\n\n# Create data module\ndata_module = SignalDataModule(\n    train_df=train_df,\n    val_df=val_df,\n    feature_cols=feature_cols,\n    label_col=\"label\",\n    window_size=60,\n)\n\n# Train with Lightning\ntrainer = L.Trainer(max_epochs=20, accelerator=\"auto\")\ntrainer.fit(model, data_module)\n</code></pre>"},{"location":"ecosystem/signalflow-nn/#components","title":"Components","text":""},{"location":"ecosystem/signalflow-nn/#encoders","title":"Encoders","text":"<p>Sequence encoders that process windowed time series into fixed-size embeddings.</p> Class Registry Name Description <code>LSTMEncoder</code> <code>encoder/lstm</code> Long Short-Term Memory, configurable depth and bidirectional <code>GRUEncoder</code> <code>encoder/gru</code> Gated Recurrent Unit, faster than LSTM with similar performance <p>Parameters:</p> <ul> <li><code>input_size</code> - Number of input features per timestep</li> <li><code>hidden_size</code> - Hidden state dimensionality (default: 64)</li> <li><code>num_layers</code> - Number of stacked RNN layers (default: 2)</li> <li><code>bidirectional</code> - Process sequence in both directions (default: False)</li> </ul>"},{"location":"ecosystem/signalflow-nn/#classification-heads","title":"Classification Heads","text":"<p>Output heads that convert encoder embeddings to class predictions.</p> Class Registry Name Description <code>MLPClassifierHead</code> <code>head/cls/mlp</code> Standard MLP with configurable hidden layers <code>LinearClassifierHead</code> <code>head/cls/linear</code> Single linear projection <code>ResidualClassifierHead</code> <code>head/cls/residual</code> MLP with residual skip connections <code>AttentionClassifierHead</code> <code>head/cls/attention</code> Multi-head self-attention based"},{"location":"ecosystem/signalflow-nn/#specialized-heads","title":"Specialized Heads","text":"Class Registry Name Description <code>DistributionHead</code> <code>head/cls/distribution</code> Soft label output via temperature-scaled softmax <code>OrdinalRegressionHead</code> <code>head/cls/ordinal</code> Ordered classification (fall &lt; neutral &lt; rise) <code>ClassificationWithConfidenceHead</code> <code>head/cls/confidence</code> Dual output: class logits + confidence score <p>DistributionHead is useful for KL-divergence training with soft labels. OrdinalRegressionHead exploits the natural ordering of signal classes. ClassificationWithConfidenceHead allows filtering predictions by model confidence.</p>"},{"location":"ecosystem/signalflow-nn/#data-pipeline","title":"Data Pipeline","text":""},{"location":"ecosystem/signalflow-nn/#timeseriespreprocessor","title":"TimeSeriesPreprocessor","text":"<p>Per-asset feature scaling with configurable methods:</p> <pre><code>from signalflow.nn.data import TimeSeriesPreprocessor, ScalerConfig\n\npreprocessor = TimeSeriesPreprocessor(\n    default_config=ScalerConfig(\n        method=\"robust\",     # robust | standard | minmax\n        scope=\"group\",       # group (per-asset) | global\n    )\n)\n\n# Fit on training data only\npreprocessor.fit(train_df, asset_col=\"pair\")\n\n# Transform\ntrain_scaled = preprocessor.transform(train_df)\ntest_scaled = preprocessor.transform(test_df)\n</code></pre>"},{"location":"ecosystem/signalflow-nn/#signalwindowdataset","title":"SignalWindowDataset","text":"<p>Creates 3D tensors <code>[window_size, features]</code> at signal timestamps only:</p> <pre><code>from signalflow.nn.data import SignalWindowDataset\n\ndataset = SignalWindowDataset(\n    df=scaled_df,\n    signal_timestamps=signal_timestamps,\n    feature_cols=feature_cols,\n    label_col=\"label\",\n    window_size=60,\n    window_timeframe=1,  # 1 = every bar, 5 = dilated sampling\n)\n</code></pre>"},{"location":"ecosystem/signalflow-nn/#signaldatamodule","title":"SignalDataModule","text":"<p>Lightning DataModule with flexible splitting strategies:</p> <pre><code>from signalflow.nn.data import SignalDataModule\n\ndm = SignalDataModule(\n    train_df=train_df,\n    val_df=val_df,\n    test_df=test_df,\n    feature_cols=feature_cols,\n    label_col=\"label\",\n    window_size=60,\n    batch_size=64,\n)\n</code></pre> <p>Split strategies: temporal (chronological), random, pair-based.</p>"},{"location":"ecosystem/signalflow-nn/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>All components support Optuna integration:</p> <pre><code>import optuna\n\ndef objective(trial):\n    config = TemporalClassificator.tune(trial, model_size=\"medium\")\n    model = TemporalClassificator(**config)\n    # train and evaluate...\n    return val_accuracy\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50, timeout=3600)\n</code></pre> <p>Model size presets (<code>small</code>, <code>medium</code>, <code>large</code>) control search ranges for hidden dimensions, layer counts, and learning rates.</p>"},{"location":"ecosystem/signalflow-nn/#integration-with-signalflow","title":"Integration with SignalFlow","text":"<p><code>TemporalValidator</code> inherits from <code>SignalValidator</code> and works identically to <code>SklearnSignalValidator</code>:</p> <pre><code># Both validators share the same interface\nfrom signalflow.validator import SklearnSignalValidator\nfrom signalflow.nn.validator import TemporalValidator\n\n# Sklearn-based\nsklearn_val = SklearnSignalValidator(model_type=\"lightgbm\")\nsklearn_val.fit(X_train, y_train)\nresult = sklearn_val.validate_signals(signals, features)\n\n# Neural network-based\nnn_val = TemporalValidator(\n    encoder_type=\"encoder/lstm\",\n    encoder_params={\"input_size\": 10, \"hidden_size\": 64},\n    head_type=\"head/cls/mlp\",\n    head_params={\"hidden_sizes\": [64]},\n    num_classes=3,\n    window_size=60,\n)\nnn_val.fit(X_train, y_train)\nresult = nn_val.validate_signals(signals, features)\n</code></pre> <p>Both return <code>Signals</code> with <code>probability_none</code>, <code>probability_rise</code>, and <code>probability_fall</code> columns.</p>"},{"location":"ecosystem/signalflow-nn/#links","title":"Links","text":"<ul> <li> GitHub Repository</li> <li> PyPI Package</li> </ul>"},{"location":"ecosystem/signalflow-ta/","title":"signalflow-ta - Technical Analysis","text":"<p>signalflow-ta extends SignalFlow with 199+ technical analysis indicators organized into 8 modules. Each indicator is a standard <code>Feature</code> class that integrates directly with <code>FeaturePipeline</code> and the component registry.</p>"},{"location":"ecosystem/signalflow-ta/#installation","title":"Installation","text":"<pre><code>pip install signalflow-ta\n</code></pre> <p>Requires <code>signalflow-trading &gt;= 0.3.5</code>.</p>"},{"location":"ecosystem/signalflow-ta/#indicator-modules","title":"Indicator Modules","text":"Module Count Description Momentum 18 RSI, MACD, Stochastic, ROC, CCI, Williams %R, and kinematics analogs Overlap 26 SMA, EMA, DEMA, TEMA, HMA, KAMA, ALMA, JMA, and price transforms Volatility 40 ATR, Bollinger, Keltner, Donchian bands, and energy-based indicators Volume 16 OBV, A/D, MFI, CMF, KVO, and market dynamics analogs Trend 22 ADX, Aroon, Supertrend, PSAR, Ichimoku, and physics-based strength Statistics 73 Dispersion, distribution, memory, cycles, complexity, DSP, regression Performance 2 Log return, percent return Divergence 2 RSI divergence, MACD divergence detectors"},{"location":"ecosystem/signalflow-ta/#quick-start","title":"Quick Start","text":""},{"location":"ecosystem/signalflow-ta/#using-individual-indicators","title":"Using Individual Indicators","text":"<pre><code>import signalflow.ta as ta\nfrom signalflow.feature import FeaturePipeline\n\n# Create indicators\nrsi = ta.RsiMom(period=14)\nbbands = ta.BollingerVol(period=20, num_std=2.0)\natr = ta.AtrVol(period=14)\n\n# Use in pipeline\npipeline = FeaturePipeline(features=[rsi, bbands, atr])\nfeatures_df = pipeline.compute(df)\n</code></pre>"},{"location":"ecosystem/signalflow-ta/#using-preset-factories","title":"Using Preset Factories","text":"<p>Preset factories provide curated indicator sets:</p> <pre><code>from signalflow.ta.pipes import (\n    momentum_core_pipe,\n    volatility_bands_pipe,\n    all_ta_pipe,\n)\nfrom signalflow.feature import FeaturePipeline\n\n# Compose a custom set\npipeline = FeaturePipeline(features=[\n    *momentum_core_pipe(normalized=True),\n    *volatility_bands_pipe(),\n])\n\n# Or use all indicators at once\nfull_pipeline = FeaturePipeline(features=all_ta_pipe(normalized=True))\n</code></pre> <p>Available factories:</p> Factory Indicators <code>smoothers_pipe()</code> SMA, EMA, WMA, HMA, DEMA, TEMA, ... <code>momentum_core_pipe()</code> RSI, ROC, MOM, CMO <code>momentum_oscillators_pipe()</code> Stochastic, StochRSI, Williams %R, CCI, UO, AO <code>momentum_pipe()</code> All momentum indicators <code>volatility_range_pipe()</code> True Range, ATR, NATR <code>volatility_bands_pipe()</code> Bollinger, Keltner, Donchian, AccBands <code>volatility_pipe()</code> All volatility indicators <code>volume_cumulative_pipe()</code> OBV, A/D, PVT, NVI, PVI <code>volume_oscillators_pipe()</code> MFI, CMF, EFI, EOM, KVO <code>volume_pipe()</code> All volume indicators <code>trend_strength_pipe()</code> ADX, Aroon, Vortex, VHF, CHOP <code>trend_stops_pipe()</code> PSAR, Supertrend, Chandelier, HiLo, CKSP <code>trend_pipe()</code> All trend indicators <code>stat_dispersion_pipe()</code> Variance, Std, MAD, Z-Score, CV, Range <code>stat_cycle_pipe()</code> Instantaneous Amplitude, Phase, Frequency <code>stat_pipe()</code> All statistical indicators <code>performance_pipe()</code> LogReturn, PctReturn <code>divergence_pipe()</code> RSI divergence, MACD divergence <code>all_ta_pipe()</code> All 199+ indicators"},{"location":"ecosystem/signalflow-ta/#indicator-architecture","title":"Indicator Architecture","text":"<p>Every indicator follows a consistent pattern:</p> <pre><code>from dataclasses import dataclass\nfrom signalflow.core import sf_component\nfrom signalflow.feature.base import Feature\n\n@dataclass\n@sf_component(name=\"momentum/rsi\")\nclass RsiMom(Feature):\n    period: int = 14\n\n    requires = [\"close\"]                # input columns\n    outputs = [\"rsi_{period}\"]          # output column template\n\n    def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        # Pure NumPy computation on single pair\n        ...\n\n    @property\n    def warmup(self) -&gt; int:\n        return self.period * 10         # minimum stable bars\n</code></pre> <p>Key design principles:</p> <ul> <li>Deterministic: Pure lookback computation, always reproducible</li> <li>Polars-native: DataFrame in/out, native rolling operations</li> <li>Warmup support: Each indicator declares minimum bars for stable output</li> <li>Normalization: Optional <code>normalized=True</code> parameter for value rescaling</li> </ul>"},{"location":"ecosystem/signalflow-ta/#normalization","title":"Normalization","text":"<p>Indicators support two normalization strategies:</p> <p>Bounded indicators (RSI, Williams %R, CCI): <pre><code>rsi = ta.RsiMom(period=14, normalized=True)\n# Rescales [0, 100] \u2192 [0, 1]\n</code></pre></p> <p>Unbounded indicators (MACD, SMA, ROC): <pre><code>sma = ta.SmaSmooth(period=20, normalized=True)\n# Rolling z-score normalization\n</code></pre></p>"},{"location":"ecosystem/signalflow-ta/#autofeaturenormalizer","title":"AutoFeatureNormalizer","text":"<p>For automatic per-feature normalization:</p> <pre><code>from signalflow.ta.auto_norm import AutoFeatureNormalizer\n\nnormalizer = AutoFeatureNormalizer(window=256, warmup=256)\ndf_normalized = normalizer.fit_transform(df)\n\n# Save for production use\nnormalizer.artifact.save(\"normalizer.json\")\n</code></pre> <p>The normalizer analyzes each feature's distribution (skewness, outlier ratio, CV) and selects the best method: rolling robust, rolling z-score, rolling rank, rolling winsorization, or signed log1p.</p>"},{"location":"ecosystem/signalflow-ta/#physics-based-indicators","title":"Physics-Based Indicators","text":"<p>signalflow-ta includes novel indicators that model market dynamics using physics analogs:</p>"},{"location":"ecosystem/signalflow-ta/#volatility-energy","title":"Volatility Energy","text":"<ul> <li>Kinetic Energy: <code>KE = 0.5 * v\u00b2</code> - measures price velocity</li> <li>Potential Energy: <code>PE = (ln(Close) - ln(MA))\u00b2</code> - measures deviation</li> <li>Total Energy: <code>TE = KE + PE</code> - overall market energy</li> <li>Temperature: <code>T = KE / degrees_of_freedom</code> - thermal regime</li> <li>Free Energy: <code>F = E - T*S</code> - actionable energy (entropy-adjusted)</li> </ul>"},{"location":"ecosystem/signalflow-ta/#volume-dynamics","title":"Volume Dynamics","text":"<ul> <li>Market Force: <code>F = volume \u00d7 acceleration</code> - buying/selling pressure</li> <li>Impulse: <code>J = \u03a3(F \u00d7 dt)</code> - accumulated force</li> <li>Market Momentum: <code>p = volume \u00d7 velocity</code> - directional volume</li> <li>Market Capacitance: <code>C = volume / \u0394Price</code> - liquidity measure</li> </ul>"},{"location":"ecosystem/signalflow-ta/#trend-physics","title":"Trend Physics","text":"<ul> <li>Viscosity: Resistance to velocity change</li> <li>Reynolds Number: Laminar vs turbulent market regime</li> <li>Impedance: <code>Z = V/I</code> electrical analogy for trend resistance</li> </ul>"},{"location":"ecosystem/signalflow-ta/#module-reference","title":"Module Reference","text":""},{"location":"ecosystem/signalflow-ta/#momentum","title":"Momentum","text":"Class Component Name Description <code>RsiMom</code> <code>momentum/rsi</code> Relative Strength Index <code>RocMom</code> <code>momentum/roc</code> Rate of Change <code>MomMom</code> <code>momentum/mom</code> Momentum <code>CmoMom</code> <code>momentum/cmo</code> Chande Momentum Oscillator <code>StochMom</code> <code>momentum/stoch</code> Stochastic Oscillator <code>StochRsiMom</code> <code>momentum/stoch_rsi</code> Stochastic RSI <code>WillrMom</code> <code>momentum/willr</code> Williams %R <code>CciMom</code> <code>momentum/cci</code> Commodity Channel Index <code>UoMom</code> <code>momentum/uo</code> Ultimate Oscillator <code>AoMom</code> <code>momentum/ao</code> Awesome Oscillator <code>MacdMom</code> <code>momentum/macd</code> MACD <code>PpoMom</code> <code>momentum/ppo</code> Percentage Price Oscillator <code>TsiMom</code> <code>momentum/tsi</code> True Strength Index <code>TrixMom</code> <code>momentum/trix</code> Triple EMA Rate of Change <code>AccelerationMom</code> <code>momentum/acceleration</code> Price Acceleration <code>JerkMom</code> <code>momentum/jerk</code> Rate of Acceleration Change <code>AngularMomentumMom</code> <code>momentum/angular_momentum</code> Rotational Momentum <code>TorqueMom</code> <code>momentum/torque</code> Rate of Angular Momentum"},{"location":"ecosystem/signalflow-ta/#overlap-smoothers-price-transforms","title":"Overlap (Smoothers &amp; Price Transforms)","text":"Class Component Name Description <code>SmaSmooth</code> <code>overlap/sma</code> Simple Moving Average <code>EmaSmooth</code> <code>overlap/ema</code> Exponential Moving Average <code>WmaSmooth</code> <code>overlap/wma</code> Weighted Moving Average <code>RmaSmooth</code> <code>overlap/rma</code> Running Moving Average <code>DemaSmooth</code> <code>overlap/dema</code> Double EMA <code>TemaSmooth</code> <code>overlap/tema</code> Triple EMA <code>HmaSmooth</code> <code>overlap/hma</code> Hull Moving Average <code>KamaSmooth</code> <code>overlap/kama</code> Kaufman Adaptive MA <code>AlmaSmooth</code> <code>overlap/alma</code> Arnaud Legoux MA <code>JmaSmooth</code> <code>overlap/jma</code> Jurik MA <code>T3Smooth</code> <code>overlap/t3</code> Tillson T3 <code>ZlmaSmooth</code> <code>overlap/zlma</code> Zero-Lag MA <code>McGinleySmooth</code> <code>overlap/mcginley</code> McGinley Dynamic <code>FramaSmooth</code> <code>overlap/frama</code> Fractal Adaptive MA"},{"location":"ecosystem/signalflow-ta/#volatility","title":"Volatility","text":"Class Component Name Description <code>AtrVol</code> <code>volatility/atr</code> Average True Range <code>NatrVol</code> <code>volatility/natr</code> Normalized ATR <code>BollingerVol</code> <code>volatility/bollinger</code> Bollinger Bands <code>KeltnerVol</code> <code>volatility/keltner</code> Keltner Channels <code>DonchianVol</code> <code>volatility/donchian</code> Donchian Channels <code>MassIndexVol</code> <code>volatility/mass_index</code> Mass Index <code>UlcerIndexVol</code> <code>volatility/ulcer_index</code> Ulcer Index <code>KineticEnergyVol</code> <code>volatility/kinetic_energy</code> Market Kinetic Energy <code>PotentialEnergyVol</code> <code>volatility/potential_energy</code> Market Potential Energy <code>TemperatureVol</code> <code>volatility/temperature</code> Market Temperature"},{"location":"ecosystem/signalflow-ta/#trend","title":"Trend","text":"Class Component Name Description <code>AdxTrend</code> <code>trend/adx</code> Average Directional Index <code>AroonTrend</code> <code>trend/aroon</code> Aroon Indicator <code>SupertrendTrend</code> <code>trend/supertrend</code> Supertrend <code>PsarTrend</code> <code>trend/psar</code> Parabolic SAR <code>IchimokuTrend</code> <code>trend/ichimoku</code> Ichimoku Cloud <code>VortexTrend</code> <code>trend/vortex</code> Vortex Indicator"},{"location":"ecosystem/signalflow-ta/#statistics","title":"Statistics","text":"Submodule Count Indicators Dispersion 9 Variance, Std, MAD, Z-Score, CV, Range, IQR Distribution 11 Median, Quantile, PctRank, Skew, Kurtosis, Entropy Memory 12 Hurst, Autocorrelation, Variance Ratio, Diffusion Coefficient Cycle 9 Instantaneous Amplitude, Phase, Frequency, Spectral Centroid Complexity 5 Permutation Entropy, Sample Entropy, Lempel-Ziv, DFA Information 5 KL Divergence, JS Divergence, Renyi Entropy, Mutual Info Regression 6 Correlation, Beta, R-Squared, Linear Regression Slope Realized Vol 5 Realized, Parkinson, Garman-Klass, Rogers-Satchell, Yang-Zhang DSP 10 Spectral Flux, Zero Crossing Rate, Spectral Rolloff, MFCC Control 5 Kalman Innovation, AR Coefficient, Lyapunov Exponent"},{"location":"ecosystem/signalflow-ta/#links","title":"Links","text":"<ul> <li> GitHub Repository</li> <li> PyPI Package</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get SignalFlow up and running in minutes.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>4GB RAM minimum (16GB recommended for backtesting)</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":""},{"location":"getting-started/installation/#standard-installation","title":"Standard Installation","text":"<pre><code>pip install signalflow-trading\n</code></pre>"},{"location":"getting-started/installation/#with-technical-analysis-indicators","title":"With Technical Analysis Indicators","text":"<p>199+ indicators (momentum, volatility, trend, statistics, and more):</p> <pre><code>pip install signalflow-ta\n</code></pre>"},{"location":"getting-started/installation/#with-neural-networks","title":"With Neural Networks","text":"<p>Deep learning validators (LSTM, GRU, Attention heads) via PyTorch Lightning:</p> <pre><code>pip install signalflow-nn\n</code></pre>"},{"location":"getting-started/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<pre><code># Create environment\npython -m venv signalflow-env\nsource signalflow-env/bin/activate  # Windows: signalflow-env\\Scripts\\activate\n\n# Install core + extensions\npip install signalflow-trading\npip install signalflow-ta    # technical analysis indicators\npip install signalflow-nn    # neural network validators\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import signalflow\nfrom signalflow.core import RawData, Signals\nfrom signalflow.detector import ExampleSmaCrossDetector\n\nprint(f\"SignalFlow {signalflow.__version__} installed\")\n</code></pre>"},{"location":"getting-started/installation/#platform-notes","title":"Platform Notes","text":"LinuxmacOSWindows <p>Works out of the box.</p> <p>Supports both Intel and Apple Silicon (M1/M2/M3).</p> <p>Works in Command Prompt or PowerShell.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors? <pre><code>pip install --force-reinstall signalflow-trading\n</code></pre></p> <p>GPU not detected (for signalflow-nn)? <pre><code># Check CUDA version first: nvidia-smi\npip install torch --index-url https://download.pytorch.org/whl/cu121\npip install signalflow-nn\n</code></pre></p> <p>Python version too old? <pre><code>python --version  # Must be 3.12+\n</code></pre></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start</p> <p>Build your first strategy in 10 minutes</p> </li> <li> <p> Ecosystem</p> <p>signalflow-ta and signalflow-nn extensions</p> </li> </ul> <p>Need help? pathway2nothing@gmail.com</p>"},{"location":"guide/advanced-strategies/","title":"Advanced Strategy Components","text":"<p>This guide covers advanced strategy components for position sizing, entry filtering, and signal aggregation.</p>"},{"location":"guide/advanced-strategies/#overview","title":"Overview","text":"<p>SignalFlow provides injectable components for customizing trade execution:</p> <pre><code>Signals (from detectors)\n    |\n    v\n[SignalAggregator] -----&gt; Aggregated Signals\n    |\n    v\nSignalEntryRule.check_entries()\n    |\n    +---&gt; [EntryFilter(s)] -----&gt; allow_entry() -&gt; bool\n    |\n    +---&gt; [PositionSizer] -----&gt; compute_size() -&gt; notional\n    |\n    v\nOrder with computed qty\n</code></pre>"},{"location":"guide/advanced-strategies/#position-sizing","title":"Position Sizing","text":"<p>Position sizers determine how much capital to allocate per trade.</p>"},{"location":"guide/advanced-strategies/#available-sizers","title":"Available Sizers","text":"Sizer Strategy Use Case <code>FixedFractionSizer</code> Fixed % of equity Simple, consistent sizing <code>SignalStrengthSizer</code> Scale by probability More capital on high-confidence signals <code>KellyCriterionSizer</code> Optimal f* formula Maximize long-term growth <code>VolatilityTargetSizer</code> Inverse volatility Equal risk per position <code>RiskParitySizer</code> Equal risk budget Portfolio diversification <code>MartingaleSizer</code> Grid/DCA scaling Grid trading strategies"},{"location":"guide/advanced-strategies/#quick-examples","title":"Quick Examples","text":"<pre><code>from signalflow.strategy.component.sizing import (\n    FixedFractionSizer,\n    KellyCriterionSizer,\n    VolatilityTargetSizer,\n    MartingaleSizer,\n)\n\n# 2% of equity per trade\nsizer = FixedFractionSizer(fraction=0.02)\n\n# Half-Kelly sizing (conservative)\nsizer = KellyCriterionSizer(kelly_fraction=0.5)\n\n# Target 1% volatility contribution\nsizer = VolatilityTargetSizer(target_volatility=0.01)\n\n# Grid trading: $100 -&gt; $150 -&gt; $225 -&gt; ...\nsizer = MartingaleSizer(base_size=100, multiplier=1.5, max_grid_levels=5)\n</code></pre>"},{"location":"guide/advanced-strategies/#kelly-criterion","title":"Kelly Criterion","text":"<p>The Kelly Criterion computes optimal position sizing:</p> <p>$$f^* = \\frac{p \\cdot b - q}{b}$$</p> <p>Where:</p> <ul> <li>$p$ = win probability</li> <li>$q$ = 1 - p (loss probability)</li> <li>$b$ = payoff ratio (avg win / avg loss)</li> </ul> <pre><code>sizer = KellyCriterionSizer(\n    kelly_fraction=0.5,          # Half-Kelly (safer)\n    use_signal_probability=True, # Use signal.probability as p\n    default_payoff_ratio=1.5,    # Expected win/loss ratio\n    max_fraction=0.25,           # Never exceed 25%\n)\n</code></pre> <p>Half-Kelly</p> <p>Full Kelly can be volatile. Half-Kelly (kelly_fraction=0.5) captures most of the edge with less variance.</p>"},{"location":"guide/advanced-strategies/#entry-filters","title":"Entry Filters","text":"<p>Entry filters validate signals before opening positions.</p>"},{"location":"guide/advanced-strategies/#available-filters","title":"Available Filters","text":"Filter Blocks When Key Params <code>RegimeFilter</code> Signal doesn't match regime <code>signal_regime_map</code>, <code>allowed_regimes_bullish/bearish</code> <code>VolatilityFilter</code> Vol outside range <code>min_volatility</code>, <code>max_volatility</code> <code>DrawdownFilter</code> Drawdown exceeds limit <code>max_drawdown</code>, <code>recovery_threshold</code> <code>CorrelationFilter</code> Too many correlated positions <code>max_correlation</code> <code>TimeOfDayFilter</code> Outside trading hours <code>allowed_hours</code>, <code>blocked_hours</code> <code>PriceDistanceFilter</code> Price too close to last entry <code>min_distance_pct</code> <code>SignalAccuracyFilter</code> Detector accuracy drops <code>min_accuracy</code>"},{"location":"guide/advanced-strategies/#composing-filters","title":"Composing Filters","text":"<p>Use <code>CompositeEntryFilter</code> to combine multiple filters:</p> <pre><code>from signalflow.strategy.component.entry import (\n    CompositeEntryFilter,\n    DrawdownFilter,\n    RegimeFilter,\n    VolatilityFilter,\n    TimeOfDayFilter,\n)\n\n# All must pass (AND logic)\ncomposite = CompositeEntryFilter(\n    filters=[\n        DrawdownFilter(max_drawdown=0.10, recovery_threshold=0.05),\n        RegimeFilter(),\n        VolatilityFilter(max_volatility=0.03),\n        TimeOfDayFilter(blocked_hours=[0, 1, 2, 3, 4, 5]),\n    ],\n    require_all=True,\n)\n\n# Check if entry is allowed\nallowed, reason = composite.allow_entry(signal_ctx, state, prices)\nif not allowed:\n    print(f\"Blocked: {reason}\")\n</code></pre>"},{"location":"guide/advanced-strategies/#drawdown-protection","title":"Drawdown Protection","text":"<p>Pause trading after significant losses:</p> <pre><code>filter_ = DrawdownFilter(\n    max_drawdown=0.10,        # Pause at 10% drawdown\n    recovery_threshold=0.05,  # Resume when back to 5%\n)\n</code></pre> <p>The filter maintains state across calls - once paused, it stays paused until recovery.</p>"},{"location":"guide/advanced-strategies/#signal-aggregation","title":"Signal Aggregation","text":"<p>Combine signals from multiple detectors using voting logic.</p>"},{"location":"guide/advanced-strategies/#voting-modes","title":"Voting Modes","text":"Mode Description <code>MAJORITY</code> Most common signal wins (with min agreement) <code>WEIGHTED</code> Probability-weighted average <code>UNANIMOUS</code> All must agree <code>ANY</code> Any non-NONE passes (highest prob wins) <code>META_LABELING</code> Detector direction \u00d7 validator confidence"},{"location":"guide/advanced-strategies/#examples","title":"Examples","text":"<pre><code>from signalflow.strategy.component.entry import SignalAggregator, VotingMode\n\n# Majority voting\nagg = SignalAggregator(\n    voting_mode=VotingMode.MAJORITY,\n    min_agreement=0.6,  # Need 60% agreement\n)\ncombined = agg.aggregate([signals_1, signals_2, signals_3])\n\n# Weighted by custom weights\nagg = SignalAggregator(\n    voting_mode=VotingMode.WEIGHTED,\n    weights=[2.0, 1.0, 1.0],  # First detector 2x weight\n)\n\n# Meta-labeling: detector * validator\nagg = SignalAggregator(voting_mode=VotingMode.META_LABELING)\ncombined = agg.aggregate([detector_signals, validator_signals])\n# probability = detector_prob * validator_prob\n</code></pre>"},{"location":"guide/advanced-strategies/#integration-with-signalentryrule","title":"Integration with SignalEntryRule","text":"<p>Inject sizers and filters into entry rules:</p> <pre><code>from signalflow.strategy.component.entry import SignalEntryRule\nfrom signalflow.strategy.component.sizing import VolatilityTargetSizer\n\nentry_rule = SignalEntryRule(\n    # Custom position sizer\n    position_sizer=VolatilityTargetSizer(target_volatility=0.01),\n\n    # Entry filters\n    entry_filters=CompositeEntryFilter(\n        filters=[\n            DrawdownFilter(max_drawdown=0.10),\n            TimeOfDayFilter(allowed_hours=list(range(8, 20))),\n        ],\n    ),\n\n    # Standard parameters still work\n    max_positions_per_pair=2,\n    max_total_positions=10,\n)\n</code></pre>"},{"location":"guide/advanced-strategies/#grid-trading-strategy","title":"Grid Trading Strategy","text":"<p>Complete grid trading setup using <code>MartingaleSizer</code> + <code>PriceDistanceFilter</code>:</p> <pre><code>from signalflow.strategy.component.entry import (\n    SignalEntryRule,\n    CompositeEntryFilter,\n    PriceDistanceFilter,\n    RegimeFilter,\n)\nfrom signalflow.strategy.component.sizing import MartingaleSizer\n\n# Grid configuration\ngrid_entry = SignalEntryRule(\n    position_sizer=MartingaleSizer(\n        base_size=200.0,      # First entry: $200\n        multiplier=1.5,       # Each level: 1.5x\n        max_grid_levels=5,    # Max 5 levels\n    ),\n    entry_filters=CompositeEntryFilter(\n        filters=[\n            # Only add when price drops 2%\n            PriceDistanceFilter(min_distance_pct=0.02, direction_aware=True),\n            RegimeFilter(),\n        ],\n    ),\n    max_positions_per_pair=5,  # Allow grid\n)\n</code></pre> <p>Grid Progression:</p> Level Price Drop Position Size 1 Entry $200 2 -2% $300 3 -4% $450 4 -6% $675 5 -8% $1,012"},{"location":"guide/advanced-strategies/#data-requirements","title":"Data Requirements","text":"<p>Components access runtime data through <code>StrategyState</code>:</p> <pre><code># Setup state with required data\nstate.runtime[\"atr\"] = {\"BTCUSDT\": 1000.0}  # For VolatilityTargetSizer\nstate.runtime[\"regime\"] = {\"BTCUSDT\": \"trend_up\"}  # For RegimeFilter\nstate.runtime[\"correlations\"] = {(\"BTCUSDT\", \"ETHUSDT\"): 0.85}  # For CorrelationFilter\nstate.metrics[\"current_drawdown\"] = 0.05  # For DrawdownFilter\n</code></pre>"},{"location":"guide/advanced-strategies/#best-practices","title":"Best Practices","text":""},{"location":"guide/advanced-strategies/#1-start-conservative","title":"1. Start Conservative","text":"<p>Use Half-Kelly and moderate position sizing:</p> <pre><code>sizer = KellyCriterionSizer(\n    kelly_fraction=0.5,\n    max_fraction=0.15,  # Cap at 15%\n)\n</code></pre>"},{"location":"guide/advanced-strategies/#2-layer-filters","title":"2. Layer Filters","text":"<p>Combine multiple filters for defense in depth:</p> <pre><code>filters = CompositeEntryFilter(\n    filters=[\n        DrawdownFilter(max_drawdown=0.10),  # Risk management\n        VolatilityFilter(max_volatility=0.05),  # Market conditions\n        TimeOfDayFilter(blocked_hours=[0, 1, 2, 3]),  # Liquidity\n    ],\n)\n</code></pre>"},{"location":"guide/advanced-strategies/#3-test-components-individually","title":"3. Test Components Individually","text":"<p>Validate each component before combining:</p> <pre><code># Test sizer\nsignal = SignalContext(pair=\"BTCUSDT\", signal_type=\"rise\", probability=0.8, price=50000)\nsize = sizer.compute_size(signal, state, prices)\nassert size &gt; 0\n\n# Test filter\nallowed, reason = filter_.allow_entry(signal, state, prices)\nprint(f\"Allowed: {allowed}, Reason: {reason}\")\n</code></pre>"},{"location":"guide/advanced-strategies/#see-also","title":"See Also","text":"<ul> <li>API Reference: Detailed component documentation</li> <li>Quick Start: Basic strategy setup</li> </ul>"},{"location":"guide/custom-data-types/","title":"Custom Data Types","text":"<p>SignalFlow ships with three built-in raw data types: SPOT, FUTURES, and PERPETUAL. Each type defines a set of required columns that the framework uses for validation, feature computation, and visualization.</p> <p>You can register your own data types to work with non-standard data sources such as limit order books, tick data, or any custom schema.</p>"},{"location":"guide/custom-data-types/#built-in-types","title":"Built-in Types","text":"Type Columns <code>SPOT</code> pair, timestamp, open, high, low, close, volume <code>FUTURES</code> SPOT + open_interest <code>PERPETUAL</code> SPOT + funding_rate, open_interest <p>Access columns programmatically:</p> <pre><code>from signalflow.core.enums import RawDataType\n\nprint(RawDataType.SPOT.columns)\n# {'pair', 'timestamp', 'open', 'high', 'low', 'close', 'volume'}\n</code></pre>"},{"location":"guide/custom-data-types/#registering-a-custom-type","title":"Registering a Custom Type","text":"<p>Use <code>default_registry.register_raw_data_type()</code> to add a new data type:</p> <pre><code>from signalflow.core.registry import default_registry\n\ndefault_registry.register_raw_data_type(\n    name=\"lob\",\n    columns=[\"pair\", \"timestamp\", \"bid\", \"ask\", \"bid_size\", \"ask_size\", \"depth\"],\n)\n</code></pre> <p>After registration, the type is available everywhere in the framework:</p> <pre><code># Look up columns\ncols = default_registry.get_raw_data_columns(\"lob\")\n# {'pair', 'timestamp', 'bid', 'ask', 'bid_size', 'ask_size', 'depth'}\n\n# List all registered types\nprint(default_registry.list_raw_data_types())\n# ['futures', 'lob', 'perpetual', 'spot']\n</code></pre>"},{"location":"guide/custom-data-types/#using-custom-types-with-components","title":"Using Custom Types with Components","text":""},{"location":"guide/custom-data-types/#featurepipeline","title":"FeaturePipeline","text":"<p>Pass the custom type name as a string to <code>raw_data_type</code>:</p> <pre><code>from signalflow.feature import FeaturePipeline\n\npipeline = FeaturePipeline(\n    features=[MyLobFeature(depth_levels=5)],\n    raw_data_type=\"lob\",  # custom type\n)\n</code></pre> <p>The pipeline validates that each feature's required columns are satisfied by the registered column set. If a feature requires a column not present in your type, you get a clear error at construction time.</p>"},{"location":"guide/custom-data-types/#signaldetector","title":"SignalDetector","text":"<pre><code>from signalflow.detector import SignalDetector\n\nclass LobImbalanceDetector(SignalDetector):\n    raw_data_type = \"lob\"\n\n    def detect(self, df, context=None):\n        # df is guaranteed to have lob columns\n        imbalance = df[\"bid_size\"] - df[\"ask_size\"]\n        ...\n</code></pre>"},{"location":"guide/custom-data-types/#rawdataview","title":"RawDataView","text":"<pre><code>from signalflow.core import RawData, RawDataView\nfrom signalflow.core.enums import DataFrameType\n\nraw = RawData(\n    datetime_start=start,\n    datetime_end=end,\n    pairs=[\"BTCUSDT\"],\n    data={\"lob\": lob_dataframe},\n)\n\nview = RawDataView(raw=raw)\n\n# Access by custom type name\nlob_pl = view.get_data(\"lob\", DataFrameType.POLARS)\nlob_pd = view.get_data(\"lob\", DataFrameType.PANDAS)\n</code></pre>"},{"location":"guide/custom-data-types/#overriding-built-in-types","title":"Overriding Built-in Types","text":"<p>If you need to extend or redefine columns for a built-in type, pass <code>override=True</code>:</p> <pre><code>default_registry.register_raw_data_type(\n    name=\"spot\",\n    columns=[\"pair\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"trades\"],\n    override=True,\n)\n</code></pre> <p>Warning</p> <p>Overriding built-in types affects all components that reference them. Use with caution.</p>"},{"location":"guide/custom-data-types/#full-example-tick-data-pipeline","title":"Full Example: Tick Data Pipeline","text":"<pre><code>from signalflow.core.registry import default_registry\nfrom signalflow.core import RawData, RawDataView\nfrom signalflow.feature import Feature, FeaturePipeline\nfrom signalflow.core.enums import DataFrameType\nfrom dataclasses import dataclass\nfrom typing import ClassVar\nfrom datetime import datetime\nimport polars as pl\n\n# 1. Register custom type\ndefault_registry.register_raw_data_type(\n    name=\"tick\",\n    columns=[\"pair\", \"timestamp\", \"price\", \"qty\", \"is_buyer_maker\"],\n)\n\n# 2. Define a feature for tick data\n@dataclass\nclass TickVwapFeature(Feature):\n    window: int = 100\n\n    requires: ClassVar[list[str]] = [\"price\", \"qty\"]\n    outputs: ClassVar[list[str]] = [\"vwap_{window}\"]\n\n    def compute_pair(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        pq = (pl.col(\"price\") * pl.col(\"qty\")).rolling_sum(self.window)\n        q = pl.col(\"qty\").rolling_sum(self.window)\n        return df.with_columns((pq / q).alias(f\"vwap_{self.window}\"))\n\n# 3. Build pipeline\npipeline = FeaturePipeline(\n    features=[TickVwapFeature(window=100)],\n    raw_data_type=\"tick\",\n)\n\n# 4. Create data and compute\ntick_df = pl.DataFrame({\n    \"pair\": [\"BTCUSDT\"] * 200,\n    \"timestamp\": [datetime(2024, 1, 1, 10, 0, i) for i in range(200)],\n    \"price\": [45000.0 + i * 0.5 for i in range(200)],\n    \"qty\": [0.1] * 200,\n    \"is_buyer_maker\": [True, False] * 100,\n})\n\nraw = RawData(\n    datetime_start=datetime(2024, 1, 1),\n    datetime_end=datetime(2024, 1, 2),\n    pairs=[\"BTCUSDT\"],\n    data={\"tick\": tick_df},\n)\n\nview = RawDataView(raw=raw)\nresult = pipeline.run(view)\nprint(result.select(\"pair\", \"timestamp\", \"price\", \"vwap_100\").tail(5))\n</code></pre>"},{"location":"guide/custom-data-types/#api-reference","title":"API Reference","text":"<p>See the full API documentation for the registry methods:</p> <ul> <li><code>register_raw_data_type()</code> - Register a new data type</li> <li><code>get_raw_data_columns()</code> - Look up columns for any type</li> <li><code>list_raw_data_types()</code> - List all registered types</li> </ul>"},{"location":"guide/custom-detectors/","title":"Custom Detector Development Guide","text":"<p>This guide covers how to develop custom signal detectors in SignalFlow.</p>"},{"location":"guide/custom-detectors/#overview","title":"Overview","text":"<p>A SignalDetector is a component that analyzes market data and generates trading signals. The detector pipeline:</p> <pre><code>RawDataView \u2192 preprocess() \u2192 detect() \u2192 Signals\n</code></pre> <ol> <li>preprocess(): Extract features from raw OHLCV data</li> <li>detect(): Apply detection logic and generate signals</li> <li>Signals: Container with detected signals</li> </ol>"},{"location":"guide/custom-detectors/#quick-start","title":"Quick Start","text":""},{"location":"guide/custom-detectors/#minimal-detector","title":"Minimal Detector","text":"<pre><code>from dataclasses import dataclass\nimport polars as pl\n\nfrom signalflow.core import Signals, sf_component\nfrom signalflow.detector import SignalDetector\n\n\n@dataclass\n@sf_component(name=\"my/rsi_oversold\")\nclass RsiOversoldDetector(SignalDetector):\n    \"\"\"Detect oversold conditions using RSI.\"\"\"\n\n    rsi_period: int = 14\n    oversold_threshold: float = 30.0\n\n    def __post_init__(self):\n        # Setup features for RSI computation\n        from signalflow.feature import ExampleRsiFeature\n        self.features = ExampleRsiFeature(period=self.rsi_period)\n        self.rsi_col = f\"RSI_{self.rsi_period}\"\n\n    def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n        signals_df = (\n            features\n            .filter(pl.col(self.rsi_col) &lt; self.oversold_threshold)\n            .select([\n                self.pair_col,\n                self.ts_col,\n                pl.lit(\"rise\").alias(\"signal_type\"),\n                pl.lit(1).alias(\"signal\"),\n            ])\n        )\n        return Signals(signals_df)\n\n\n# Usage\ndetector = RsiOversoldDetector(rsi_period=14, oversold_threshold=25)\nsignals = detector.run(raw_data_view)\n</code></pre>"},{"location":"guide/custom-detectors/#core-concepts","title":"Core Concepts","text":""},{"location":"guide/custom-detectors/#signal-categories","title":"Signal Categories","text":"<p>Every detector declares which category of signals it produces:</p> <pre><code>from signalflow.core.enums import SignalCategory\n\nclass MyDetector(SignalDetector):\n    signal_category = SignalCategory.PRICE_DIRECTION  # default\n</code></pre> <p>Available categories:</p> Category Description Example signal_type values <code>PRICE_DIRECTION</code> Price movement direction <code>rise</code>, <code>fall</code>, <code>flat</code> <code>PRICE_STRUCTURE</code> Price patterns <code>local_max</code>, <code>local_min</code>, <code>breakout_up</code> <code>TREND_MOMENTUM</code> Trend state <code>trend_start</code>, <code>trend_reversal</code>, <code>overbought</code> <code>VOLATILITY</code> Volatility regime <code>high_volatility</code>, <code>low_volatility</code>, <code>volatility_expansion</code> <code>VOLUME_LIQUIDITY</code> Volume patterns <code>abnormal_volume</code>, <code>illiquidity</code>, <code>accumulation</code> <code>MARKET_WIDE</code> Cross-pair events <code>market_crash</code>, <code>synchronization</code>, <code>structural_break</code> <code>ANOMALY</code> Anomalous events <code>extreme_positive_anomaly</code>, <code>extreme_negative_anomaly</code>"},{"location":"guide/custom-detectors/#signal-types","title":"Signal Types","text":"<p>Use plain string values for all <code>signal_type</code> values:</p> <pre><code># Price direction\npl.lit(\"rise\").alias(\"signal_type\")\npl.lit(\"fall\").alias(\"signal_type\")\npl.lit(\"flat\").alias(\"signal_type\")\n\n# Volatility regime\npl.lit(\"high_volatility\").alias(\"signal_type\")\npl.lit(\"low_volatility\").alias(\"signal_type\")\n\n# No signal: use null (not \"none\")\npl.lit(None, dtype=pl.Utf8).alias(\"signal_type\")\n</code></pre> <p>SignalType enum is deprecated</p> <p>The <code>SignalType</code> enum (<code>SignalType.RISE</code>, <code>SignalType.FALL</code>, etc.) still works for backward compatibility but is deprecated. Use plain strings instead.</p>"},{"location":"guide/custom-detectors/#allowed-signal-types","title":"Allowed Signal Types","text":"<p>Declare which signal_type values your detector produces:</p> <pre><code>from dataclasses import field\n\n@dataclass\nclass MyVolatilityDetector(SignalDetector):\n    signal_category = SignalCategory.VOLATILITY\n\n    # Declare allowed values (for validation)\n    allowed_signal_types: set[str] | None = field(\n        default_factory=lambda: {\"high_volatility\", \"low_volatility\"}\n    )\n</code></pre>"},{"location":"guide/custom-detectors/#features-integration","title":"Features Integration","text":""},{"location":"guide/custom-detectors/#using-built-in-features","title":"Using Built-in Features","text":"<pre><code>from signalflow.feature import ExampleSmaFeature, ExampleRsiFeature, FeaturePipeline\n\n@dataclass\nclass SmaCrossDetector(SignalDetector):\n    fast_period: int = 10\n    slow_period: int = 20\n\n    def __post_init__(self):\n        # Option 1: List of features\n        self.features = [\n            ExampleSmaFeature(period=self.fast_period),\n            ExampleSmaFeature(period=self.slow_period),\n        ]\n\n        # Option 2: FeaturePipeline\n        self.features = FeaturePipeline([\n            ExampleSmaFeature(period=self.fast_period),\n            ExampleSmaFeature(period=self.slow_period),\n        ])\n\n        # Option 3: Single feature\n        self.features = ExampleRsiFeature(period=14)\n</code></pre>"},{"location":"guide/custom-detectors/#no-features-raw-ohlcv","title":"No Features (Raw OHLCV)","text":"<p>If <code>features=None</code> (default), <code>preprocess()</code> returns raw OHLCV data:</p> <pre><code>@dataclass\nclass SimplePriceDetector(SignalDetector):\n    # features = None  # default - raw OHLCV\n\n    def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n        # features has: pair, timestamp, open, high, low, close, volume\n        ...\n</code></pre>"},{"location":"guide/custom-detectors/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"guide/custom-detectors/#override-preprocess","title":"Override preprocess()","text":"<p>Add helper columns for your detection method:</p> <pre><code>@dataclass\nclass ZScoreDetector(SignalDetector):\n    target_feature: str = \"close\"\n    rolling_window: int = 100\n    threshold: float = 3.0\n\n    def preprocess(self, raw_data_view, context=None) -&gt; pl.DataFrame:\n        # 1. Base preprocessing (OHLCV + features)\n        df = super().preprocess(raw_data_view, context)\n\n        # 2. Add helper columns\n        df = df.with_columns([\n            pl.col(self.target_feature)\n                .rolling_mean(window_size=self.rolling_window)\n                .over(self.pair_col)\n                .alias(\"_rolling_mean\"),\n            pl.col(self.target_feature)\n                .rolling_std(window_size=self.rolling_window)\n                .over(self.pair_col)\n                .alias(\"_rolling_std\"),\n        ])\n\n        return df\n\n    def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n        z_score = (pl.col(self.target_feature) - pl.col(\"_rolling_mean\")) / pl.col(\"_rolling_std\")\n\n        signals_df = (\n            features\n            .filter(z_score.abs() &gt; self.threshold)\n            .select([\n                self.pair_col,\n                self.ts_col,\n                pl.when(z_score &gt; self.threshold)\n                    .then(pl.lit(\"positive_anomaly\"))\n                    .otherwise(pl.lit(\"negative_anomaly\"))\n                    .alias(\"signal_type\"),\n                pl.lit(1).alias(\"signal\"),\n                (z_score.abs() / self.threshold).clip(0, 1).alias(\"probability\"),\n            ])\n        )\n        return Signals(signals_df)\n</code></pre>"},{"location":"guide/custom-detectors/#market-wide-detectors","title":"Market-Wide Detectors","text":"<p>Detect cross-pair events (all pairs affected simultaneously):</p> <pre><code>@dataclass\n@sf_component(name=\"my/market_panic\")\nclass MarketPanicDetector(SignalDetector):\n    signal_category = SignalCategory.MARKET_WIDE\n    allowed_signal_types: set[str] | None = field(\n        default_factory=lambda: {\"market_panic\"}\n    )\n\n    agreement_threshold: float = 0.9\n    min_pairs: int = 5\n\n    def preprocess(self, raw_data_view, context=None) -&gt; pl.DataFrame:\n        df = super().preprocess(raw_data_view, context)\n\n        # Add log returns\n        df = df.with_columns(\n            (pl.col(\"close\") / pl.col(\"close\").shift(1))\n            .log()\n            .over(self.pair_col)\n            .alias(\"_ret\")\n        )\n        return df\n\n    def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n        # Compute cross-pair agreement per timestamp\n        agreement = (\n            features\n            .filter(pl.col(\"_ret\").is_not_null())\n            .group_by(self.ts_col)\n            .agg([\n                pl.col(\"_ret\").count().alias(\"_n_pairs\"),\n                (pl.col(\"_ret\") &lt; -0.01).sum().alias(\"_n_falling\"),\n            ])\n            .filter(pl.col(\"_n_pairs\") &gt;= self.min_pairs)\n            .with_columns(\n                (pl.col(\"_n_falling\") / pl.col(\"_n_pairs\")).alias(\"_agreement\")\n            )\n            .filter(pl.col(\"_agreement\") &gt;= self.agreement_threshold)\n        )\n\n        # Create signals with synthetic \"ALL\" pair\n        signals_df = agreement.select([\n            pl.lit(\"ALL\").alias(self.pair_col),\n            self.ts_col,\n            pl.lit(\"market_panic\").alias(\"signal_type\"),\n            pl.lit(1).alias(\"signal\"),\n            pl.col(\"_agreement\").alias(\"probability\"),\n        ])\n\n        return Signals(signals_df)\n</code></pre>"},{"location":"guide/custom-detectors/#probability-column","title":"Probability Column","text":"<p>Add probability (confidence) to signals:</p> <pre><code>def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n    rsi = pl.col(\"RSI_14\")\n\n    signals_df = (\n        features\n        .filter((rsi &lt; 30) | (rsi &gt; 70))\n        .select([\n            self.pair_col,\n            self.ts_col,\n            pl.when(rsi &lt; 30)\n                .then(pl.lit(\"rise\"))\n                .otherwise(pl.lit(\"fall\"))\n                .alias(\"signal_type\"),\n            pl.lit(1).alias(\"signal\"),\n            # Probability: distance from threshold normalized\n            pl.when(rsi &lt; 30)\n                .then((30 - rsi) / 30)  # 0 at 30, 1 at 0\n                .otherwise((rsi - 70) / 30)  # 0 at 70, 1 at 100\n                .clip(0, 1)\n                .alias(\"probability\"),\n        ])\n    )\n    return Signals(signals_df)\n</code></pre> <p>To require probability:</p> <pre><code>@dataclass\nclass MyDetector(SignalDetector):\n    require_probability: bool = True  # Validation will fail without probability\n</code></pre>"},{"location":"guide/custom-detectors/#keep-latest-signal-per-pair","title":"Keep Latest Signal Per Pair","text":"<p>For real-time trading, keep only the most recent signal:</p> <pre><code>@dataclass\nclass RealtimeDetector(SignalDetector):\n    keep_only_latest_per_pair: bool = True\n</code></pre>"},{"location":"guide/custom-detectors/#signals-output-schema","title":"Signals Output Schema","text":"<p>Required columns:</p> Column Type Description <code>pair</code> str Trading pair (e.g., \"BTCUSDT\") <code>timestamp</code> datetime Signal timestamp (timezone-naive) <code>signal_type</code> str Signal type value <p>Optional columns:</p> Column Type Description <code>signal</code> int/float Signal value (e.g., 1 for long, -1 for short) <code>probability</code> float Confidence score [0, 1]"},{"location":"guide/custom-detectors/#best-practices","title":"Best Practices","text":""},{"location":"guide/custom-detectors/#1-use-sf_component-decorator","title":"1. Use @sf_component decorator","text":"<p>Register your detector for serialization:</p> <pre><code>@dataclass\n@sf_component(name=\"my_namespace/detector_name\")\nclass MyDetector(SignalDetector):\n    ...\n</code></pre>"},{"location":"guide/custom-detectors/#2-validate-parameters-in-post_init","title":"2. Validate parameters in post_init","text":"<pre><code>def __post_init__(self):\n    if self.threshold &lt;= 0:\n        raise ValueError(\"threshold must be positive\")\n    if self.window &lt; 2:\n        raise ValueError(\"window must be &gt;= 2\")\n</code></pre>"},{"location":"guide/custom-detectors/#3-filter-null-values","title":"3. Filter null values","text":"<pre><code>def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n    # Always filter nulls before detection\n    df = features.filter(\n        pl.col(\"RSI_14\").is_not_null() &amp;\n        pl.col(\"close\").is_not_null()\n    )\n    ...\n</code></pre>"},{"location":"guide/custom-detectors/#4-use-overpair_col-for-per-pair-calculations","title":"4. Use .over(pair_col) for per-pair calculations","text":"<pre><code># Rolling stats per pair\ndf.with_columns(\n    pl.col(\"close\")\n        .rolling_mean(window_size=20)\n        .over(self.pair_col)  # Important!\n        .alias(\"sma_20\")\n)\n</code></pre>"},{"location":"guide/custom-detectors/#5-return-empty-signals-if-no-signals","title":"5. Return empty Signals if no signals","text":"<pre><code>def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n    signals_df = features.filter(...)  # May be empty\n    return Signals(signals_df)  # OK even if empty\n</code></pre>"},{"location":"guide/custom-detectors/#complete-example-bollinger-bands-detector","title":"Complete Example: Bollinger Bands Detector","text":"<pre><code>from dataclasses import dataclass, field\nfrom typing import Any\n\nimport polars as pl\n\nfrom signalflow.core import RawDataView, Signals, sf_component\nfrom signalflow.core.enums import SignalCategory\nfrom signalflow.detector import SignalDetector\n\n\n@dataclass\n@sf_component(name=\"technical/bollinger_bands\")\nclass BollingerBandsDetector(SignalDetector):\n    \"\"\"Detect overbought/oversold using Bollinger Bands.\n\n    Signals:\n        - RISE: Price touches lower band (oversold)\n        - FALL: Price touches upper band (overbought)\n    \"\"\"\n\n    signal_category = SignalCategory.PRICE_DIRECTION\n\n    window: int = 20\n    num_std: float = 2.0\n    price_col: str = \"close\"\n\n    def preprocess(\n        self,\n        raw_data_view: RawDataView,\n        context: dict[str, Any] | None = None,\n    ) -&gt; pl.DataFrame:\n        df = super().preprocess(raw_data_view, context)\n\n        price = pl.col(self.price_col)\n\n        df = df.with_columns([\n            price\n                .rolling_mean(window_size=self.window)\n                .over(self.pair_col)\n                .alias(\"_bb_middle\"),\n            price\n                .rolling_std(window_size=self.window)\n                .over(self.pair_col)\n                .alias(\"_bb_std\"),\n        ]).with_columns([\n            (pl.col(\"_bb_middle\") + self.num_std * pl.col(\"_bb_std\")).alias(\"_bb_upper\"),\n            (pl.col(\"_bb_middle\") - self.num_std * pl.col(\"_bb_std\")).alias(\"_bb_lower\"),\n        ])\n\n        return df\n\n    def detect(\n        self,\n        features: pl.DataFrame,\n        context: dict[str, Any] | None = None,\n    ) -&gt; Signals:\n        price = pl.col(self.price_col)\n        upper = pl.col(\"_bb_upper\")\n        lower = pl.col(\"_bb_lower\")\n\n        # Filter valid rows\n        df = features.filter(\n            pl.col(\"_bb_upper\").is_not_null() &amp;\n            pl.col(\"_bb_lower\").is_not_null()\n        )\n\n        # Detect touches\n        touch_lower = price &lt;= lower\n        touch_upper = price &gt;= upper\n\n        signals_df = (\n            df\n            .filter(touch_lower | touch_upper)\n            .select([\n                self.pair_col,\n                self.ts_col,\n                pl.when(touch_lower)\n                    .then(pl.lit(\"rise\"))\n                    .otherwise(pl.lit(\"fall\"))\n                    .alias(\"signal_type\"),\n                pl.when(touch_lower)\n                    .then(pl.lit(1))\n                    .otherwise(pl.lit(-1))\n                    .alias(\"signal\"),\n                # Probability: how far beyond band\n                pl.when(touch_lower)\n                    .then((lower - price) / pl.col(\"_bb_std\"))\n                    .otherwise((price - upper) / pl.col(\"_bb_std\"))\n                    .abs()\n                    .clip(0, 1)\n                    .alias(\"probability\"),\n            ])\n        )\n\n        return Signals(signals_df)\n\n\n# Usage\ndetector = BollingerBandsDetector(window=20, num_std=2.5)\nsignals = detector.run(raw_data_view)\n\n# Filter for bullish signals\nbullish = signals.value.filter(pl.col(\"signal_type\") == \"rise\")\n</code></pre>"},{"location":"guide/custom-detectors/#testing-your-detector","title":"Testing Your Detector","text":"<pre><code>import pytest\nfrom datetime import datetime, timedelta\nimport numpy as np\nimport polars as pl\n\nfrom signalflow.core import RawData, RawDataView, Signals\n\n\ndef _make_test_data(n: int = 100, n_pairs: int = 3) -&gt; RawDataView:\n    \"\"\"Generate test OHLCV data.\"\"\"\n    np.random.seed(42)\n    base = datetime(2024, 1, 1)\n    pairs = [f\"PAIR{i}\" for i in range(n_pairs)]\n\n    rows = []\n    for pair in pairs:\n        price = 100.0\n        for i in range(n):\n            price *= np.exp(np.random.randn() * 0.01)\n            rows.append({\n                \"pair\": pair,\n                \"timestamp\": base + timedelta(minutes=i),\n                \"open\": price * 0.999,\n                \"high\": price * 1.005,\n                \"low\": price * 0.995,\n                \"close\": price,\n                \"volume\": 1000.0,\n            })\n\n    df = pl.DataFrame(rows)\n    raw = RawData(\n        datetime_start=base,\n        datetime_end=base + timedelta(minutes=n),\n        pairs=pairs,\n        data={\"spot\": df},\n    )\n    return RawDataView(raw)\n\n\nclass TestMyDetector:\n    def test_returns_signals(self):\n        raw_view = _make_test_data()\n        detector = MyDetector(threshold=2.0)\n        signals = detector.run(raw_view)\n\n        assert isinstance(signals, Signals)\n        assert \"signal_type\" in signals.value.columns\n\n    def test_signal_schema(self):\n        raw_view = _make_test_data()\n        detector = MyDetector()\n        signals = detector.run(raw_view)\n\n        df = signals.value\n        assert \"pair\" in df.columns\n        assert \"timestamp\" in df.columns\n        assert \"signal_type\" in df.columns\n\n    def test_no_duplicate_signals(self):\n        raw_view = _make_test_data()\n        detector = MyDetector()\n        signals = detector.run(raw_view)\n\n        df = signals.value\n        dups = df.group_by([\"pair\", \"timestamp\"]).len().filter(pl.col(\"len\") &gt; 1)\n        assert dups.height == 0\n</code></pre>"},{"location":"guide/custom-detectors/#see-also","title":"See Also","text":"<ul> <li>SignalDetector API Reference</li> <li>Market-Wide Detectors</li> </ul>"},{"location":"guide/data-sources/","title":"Data Sources","text":"<p>SignalFlow provides async clients and loaders for downloading historical OHLCV data from cryptocurrency exchanges.</p>"},{"location":"guide/data-sources/#architecture-overview","title":"Architecture Overview","text":"<p>The data module follows a clear separation of concerns:</p> <pre><code>flowchart LR\n    A[Exchange API] --&gt; B[RawDataSource]\n    B --&gt; C[RawDataLoader]\n    C --&gt; D[RawDataStore]\n    D --&gt; E[RawData]\n\n    style A fill:#2563eb,stroke:#3b82f6,stroke-width:2px,color:#fff\n    style B fill:#ea580c,stroke:#f97316,stroke-width:2px,color:#fff\n    style C fill:#16a34a,stroke:#22c55e,stroke-width:2px,color:#fff\n    style D fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#fff\n    style E fill:#7c3aed,stroke:#8b5cf6,stroke-width:2px,color:#fff</code></pre> Component Responsibility <code>RawDataSource</code> API client (fetch klines, handle rate limits) <code>RawDataLoader</code> Orchestrates download/sync to storage <code>RawDataStore</code> Persists data (DuckDB, SQLite, PostgreSQL) <code>RawData</code> Immutable container for strategy consumption"},{"location":"guide/data-sources/#built-in-exchange-sources","title":"Built-in Exchange Sources","text":""},{"location":"guide/data-sources/#binance","title":"Binance","text":"<pre><code>from signalflow.data.source import BinanceClient, BinanceSpotLoader\nfrom signalflow.data.raw_store import DuckDbRawStore\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Direct API client usage\nasync with BinanceClient() as client:\n    klines = await client.get_klines(\"BTCUSDT\", \"1h\")\n\n    # With date range\n    klines = await client.get_klines_range(\n        pair=\"BTCUSDT\",\n        timeframe=\"1m\",\n        start_time=datetime(2024, 1, 1),\n        end_time=datetime(2024, 1, 31),\n    )\n\n# Using loader for automated sync\nstore = DuckDbRawStore(db_path=Path(\"data/spot.duckdb\"))\nloader = BinanceSpotLoader(store_path=Path(\"data/spot.duckdb\"))\n\nawait loader.sync(\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    days=30,\n    fill_gaps=True,\n)\n</code></pre> <p>Available loaders:</p> Loader Market Type Base URL <code>BinanceSpotLoader</code> Spot <code>api.binance.com</code> <code>BinanceFuturesUsdtLoader</code> USDT-M Futures <code>fapi.binance.com</code> <code>BinanceFuturesCoinLoader</code> COIN-M Futures <code>dapi.binance.com</code>"},{"location":"guide/data-sources/#bybit","title":"Bybit","text":"<pre><code>from signalflow.data.source import BybitClient, BybitSpotLoader\n\nasync with BybitClient() as client:\n    klines = await client.get_klines(\"BTCUSDT\", timeframe=\"1h\")\n\n    klines = await client.get_klines_range(\n        pair=\"BTCUSDT\",\n        category=\"spot\",  # or \"linear\" for futures\n        timeframe=\"1m\",\n        start_time=datetime(2024, 1, 1),\n        end_time=datetime(2024, 1, 31),\n    )\n</code></pre> Loader Category <code>BybitSpotLoader</code> <code>spot</code> <code>BybitFuturesLoader</code> <code>linear</code>"},{"location":"guide/data-sources/#okx","title":"OKX","text":"<pre><code>from signalflow.data.source import OkxClient, OkxSpotLoader\n\nasync with OkxClient() as client:\n    # OKX uses \"BTC-USDT\" format\n    klines = await client.get_klines(\"BTC-USDT\", timeframe=\"1h\")\n</code></pre> Loader Instrument Suffix <code>OkxSpotLoader</code> (none) <code>OkxFuturesLoader</code> <code>-SWAP</code>"},{"location":"guide/data-sources/#virtual-data-provider","title":"Virtual Data Provider","text":"<p>For testing without API access:</p> <pre><code>from signalflow.data.source import VirtualDataProvider, generate_ohlcv\nfrom signalflow.data.raw_store import DuckDbRawStore\nfrom pathlib import Path\n\nstore = DuckDbRawStore(db_path=Path(\"test.duckdb\"))\n\n# Generate synthetic data\nprovider = VirtualDataProvider(store=store, seed=42)\nprovider.download(pairs=[\"BTCUSDT\", \"ETHUSDT\"], n_bars=10_000)\n\n# Or generate in-memory\nohlcv_data = generate_ohlcv(n_bars=1000, seed=42)\n</code></pre>"},{"location":"guide/data-sources/#creating-a-custom-source","title":"Creating a Custom Source","text":""},{"location":"guide/data-sources/#step-1-implement-rawdatasource","title":"Step 1: Implement RawDataSource","text":"<pre><code>from dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional\nimport aiohttp\n\nfrom signalflow.core import sf_component\nfrom signalflow.data.source.base import RawDataSource\nfrom signalflow.data.source._helpers import dt_to_ms_utc, ms_to_dt_utc_naive\n\n\n@dataclass\n@sf_component(name=\"kraken\")\nclass KrakenClient(RawDataSource):\n    \"\"\"Async client for Kraken REST API.\"\"\"\n\n    base_url: str = \"https://api.kraken.com\"\n    max_retries: int = 3\n    _session: Optional[aiohttp.ClientSession] = field(default=None, repr=False)\n\n    async def __aenter__(self):\n        self._session = aiohttp.ClientSession()\n        return self\n\n    async def __aexit__(self, *args):\n        if self._session:\n            await self._session.close()\n            self._session = None\n\n    async def get_klines(\n        self,\n        pair: str,\n        timeframe: str = \"1m\",\n        since: Optional[datetime] = None,\n    ) -&gt; list[dict]:\n        \"\"\"Fetch OHLCV data from Kraken.\"\"\"\n        if self._session is None:\n            raise RuntimeError(\"KrakenClient must be used as async context manager\")\n\n        # Kraken uses different pair format (XXBTZUSD)\n        kraken_pair = self._to_kraken_pair(pair)\n\n        # Kraken interval in minutes\n        interval_map = {\"1m\": 1, \"5m\": 5, \"15m\": 15, \"1h\": 60, \"4h\": 240, \"1d\": 1440}\n        interval = interval_map.get(timeframe)\n        if interval is None:\n            raise ValueError(f\"Unsupported timeframe: {timeframe}\")\n\n        params = {\"pair\": kraken_pair, \"interval\": interval}\n        if since:\n            params[\"since\"] = int(since.timestamp())\n\n        async with self._session.get(\n            f\"{self.base_url}/0/public/OHLC\",\n            params=params,\n        ) as resp:\n            data = await resp.json()\n\n            if data.get(\"error\"):\n                raise RuntimeError(f\"Kraken API error: {data['error']}\")\n\n            result_key = list(data[\"result\"].keys())[0]\n            if result_key == \"last\":\n                result_key = list(data[\"result\"].keys())[1]\n\n            rows = data[\"result\"][result_key]\n\n        # Transform to canonical format\n        klines = []\n        for row in rows:\n            klines.append({\n                \"timestamp\": datetime.fromtimestamp(row[0]),\n                \"open\": float(row[1]),\n                \"high\": float(row[2]),\n                \"low\": float(row[3]),\n                \"close\": float(row[4]),\n                \"volume\": float(row[6]),\n                \"trades\": int(row[7]) if len(row) &gt; 7 else 0,\n            })\n\n        return klines\n\n    def _to_kraken_pair(self, pair: str) -&gt; str:\n        \"\"\"Convert BTCUSDT to XXBTZUSD format.\"\"\"\n        # Simplified mapping\n        mapping = {\n            \"BTCUSDT\": \"XXBTZUSD\",\n            \"ETHUSDT\": \"XETHZUSD\",\n            \"BTCUSD\": \"XXBTZUSD\",\n        }\n        return mapping.get(pair, pair)\n</code></pre>"},{"location":"guide/data-sources/#step-2-implement-rawdataloader","title":"Step 2: Implement RawDataLoader","text":"<pre><code>from dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom signalflow.core import sf_component\nfrom signalflow.data.source.base import RawDataLoader\nfrom signalflow.data.raw_store import DuckDbRawStore\n\n\n@dataclass\n@sf_component(name=\"kraken/spot\")\nclass KrakenSpotLoader(RawDataLoader):\n    \"\"\"Loader for Kraken spot data.\"\"\"\n\n    store_path: Path\n    timeframe: str = \"1m\"\n    _store: Optional[DuckDbRawStore] = None\n\n    def __post_init__(self):\n        self._store = DuckDbRawStore(db_path=self.store_path, data_type=\"spot\")\n\n    async def download(\n        self,\n        pairs: list[str],\n        start: datetime,\n        end: datetime,\n    ):\n        \"\"\"Download historical data.\"\"\"\n        async with KrakenClient() as client:\n            for pair in pairs:\n                klines = await client.get_klines(\n                    pair=pair,\n                    timeframe=self.timeframe,\n                    since=start,\n                )\n\n                # Filter by end date\n                klines = [k for k in klines if k[\"timestamp\"] &lt;= end]\n\n                if klines:\n                    self._store.insert_klines(pair, klines)\n                    print(f\"Downloaded {len(klines)} klines for {pair}\")\n\n    async def sync(\n        self,\n        pairs: list[str],\n        days: int = 7,\n    ):\n        \"\"\"Sync latest data.\"\"\"\n        end = datetime.now()\n\n        for pair in pairs:\n            # Get last timestamp\n            _, max_ts = self._store.get_time_bounds(pair)\n\n            if max_ts:\n                start = max_ts\n            else:\n                start = end - timedelta(days=days)\n\n            await self.download(pairs=[pair], start=start, end=end)\n</code></pre>"},{"location":"guide/data-sources/#step-3-register-and-use","title":"Step 3: Register and Use","text":"<pre><code>from signalflow.core.registry import default_registry\nfrom signalflow.core.enums import SfComponentType\n\n# Components are auto-registered via @sf_component decorator\n# Or register manually:\ndefault_registry.register(\n    SfComponentType.RAW_DATA_SOURCE,\n    \"kraken\",\n    KrakenClient,\n)\n\ndefault_registry.register(\n    SfComponentType.RAW_DATA_LOADER,\n    \"kraken/spot\",\n    KrakenSpotLoader,\n)\n\n# Usage\nloader = KrakenSpotLoader(store_path=Path(\"data/kraken.duckdb\"))\nawait loader.sync(pairs=[\"BTCUSDT\", \"ETHUSDT\"], days=30)\n</code></pre>"},{"location":"guide/data-sources/#shared-utilities","title":"Shared Utilities","text":"<p>The <code>_helpers</code> module provides common datetime utilities:</p> <pre><code>from signalflow.data.source._helpers import (\n    TIMEFRAME_MS,        # {\"1m\": 60_000, \"1h\": 3_600_000, ...}\n    dt_to_ms_utc,        # datetime -&gt; milliseconds\n    ms_to_dt_utc_naive,  # milliseconds -&gt; datetime (UTC naive)\n    ensure_utc_naive,    # normalize timezone\n)\n\n# Convert datetime to milliseconds\nms = dt_to_ms_utc(datetime(2024, 1, 1))  # 1704067200000\n\n# Convert back\ndt = ms_to_dt_utc_naive(1704067200000)  # datetime(2024, 1, 1, 0, 0)\n\n# Normalize timezone\ndt_naive = ensure_utc_naive(datetime(2024, 1, 1, tzinfo=timezone.utc))\n</code></pre>"},{"location":"guide/data-sources/#best-practices","title":"Best Practices","text":""},{"location":"guide/data-sources/#rate-limiting","title":"Rate Limiting","text":"<pre><code>import asyncio\n\n@dataclass\nclass MyClient(RawDataSource):\n    requests_per_minute: int = 60\n    _last_request: float = 0\n\n    async def _rate_limit(self):\n        elapsed = time.time() - self._last_request\n        min_interval = 60 / self.requests_per_minute\n        if elapsed &lt; min_interval:\n            await asyncio.sleep(min_interval - elapsed)\n        self._last_request = time.time()\n\n    async def get_klines(self, pair: str):\n        await self._rate_limit()\n        # ... make request\n</code></pre>"},{"location":"guide/data-sources/#retry-logic","title":"Retry Logic","text":"<pre><code>async def get_klines(self, pair: str):\n    last_error = None\n\n    for attempt in range(self.max_retries):\n        try:\n            async with self._session.get(url, params=params) as resp:\n                if resp.status == 429:  # Rate limited\n                    await asyncio.sleep(2 ** attempt)\n                    continue\n\n                if resp.status != 200:\n                    raise RuntimeError(f\"HTTP {resp.status}\")\n\n                return await resp.json()\n\n        except aiohttp.ClientError as e:\n            last_error = e\n            await asyncio.sleep(1)\n\n    raise RuntimeError(f\"Failed after {self.max_retries} attempts: {last_error}\")\n</code></pre>"},{"location":"guide/data-sources/#timestamp-convention","title":"Timestamp Convention","text":"<p>All SignalFlow sources use close time as the canonical timestamp:</p> <pre><code># Binance returns open time in k[0], close time in k[6]\nclose_ms = int(kline[6])\ntimestamp = ms_to_dt_utc_naive(close_ms)\n\n# For sources returning open time, add timeframe duration\nopen_time = datetime(2024, 1, 1, 10, 0)\nclose_time = open_time + timedelta(minutes=1)  # for 1m candle\n</code></pre>"},{"location":"guide/data-sources/#api-reference","title":"API Reference","text":"<p>See the full API documentation:</p> <ul> <li><code>RawDataSource</code> - Base class for data sources</li> <li><code>RawDataLoader</code> - Base class for data loaders</li> <li><code>BinanceClient</code> - Binance API client</li> <li><code>BybitClient</code> - Bybit API client</li> <li><code>OkxClient</code> - OKX API client</li> </ul>"},{"location":"guide/data-stores/","title":"Data Stores","text":"<p>SignalFlow provides multiple storage backends for persisting historical market data with support for different data types (spot, futures, perpetual).</p>"},{"location":"guide/data-stores/#available-backends","title":"Available Backends","text":"Backend Use Case Dependencies <code>DuckDbRawStore</code> High-performance local storage <code>duckdb</code> <code>SqliteRawStore</code> Lightweight, zero-config built-in <code>InMemoryRawStore</code> Testing and notebooks none <code>PgRawStore</code> Production PostgreSQL <code>psycopg2</code> (optional)"},{"location":"guide/data-stores/#quick-start","title":"Quick Start","text":""},{"location":"guide/data-stores/#duckdb-store-recommended","title":"DuckDB Store (Recommended)","text":"<pre><code>from signalflow.data.raw_store import DuckDbRawStore\nfrom datetime import datetime\nfrom pathlib import Path\n\n# Create store\nstore = DuckDbRawStore(\n    db_path=Path(\"data/spot.duckdb\"),\n    data_type=\"spot\",  # or \"futures\", \"perpetual\"\n)\n\n# Insert data\nklines = [\n    {\"timestamp\": datetime(2024, 1, 1, 0, 0), \"open\": 42000.0, \"high\": 42100.0,\n     \"low\": 41900.0, \"close\": 42050.0, \"volume\": 100.0, \"trades\": 500},\n    {\"timestamp\": datetime(2024, 1, 1, 0, 1), \"open\": 42050.0, \"high\": 42150.0,\n     \"low\": 41950.0, \"close\": 42100.0, \"volume\": 120.0, \"trades\": 600},\n]\nstore.insert_klines(\"BTCUSDT\", klines)\n\n# Load single pair\ndf = store.load(\"BTCUSDT\", hours=24)\n\n# Load date range\ndf = store.load(\n    \"BTCUSDT\",\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31),\n)\n\n# Load multiple pairs\ndf = store.load_many(\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31),\n)\n\n# Cleanup\nstore.close()\n</code></pre>"},{"location":"guide/data-stores/#sqlite-store","title":"SQLite Store","text":"<pre><code>from signalflow.data.raw_store import SqliteRawStore\nfrom pathlib import Path\n\nstore = SqliteRawStore(\n    db_path=Path(\"data/spot.sqlite\"),\n    data_type=\"spot\",\n)\n\n# Same API as DuckDB\nstore.insert_klines(\"BTCUSDT\", klines)\ndf = store.load(\"BTCUSDT\", hours=24)\nstore.close()\n</code></pre>"},{"location":"guide/data-stores/#in-memory-store","title":"In-Memory Store","text":"<pre><code>from signalflow.data.raw_store import InMemoryRawStore\n\nstore = InMemoryRawStore(data_type=\"spot\")\n\nstore.insert_klines(\"BTCUSDT\", klines)\ndf = store.load(\"BTCUSDT\")\n\n# Data is lost when store is closed\nstore.close()\n</code></pre>"},{"location":"guide/data-stores/#converting-to-rawdata","title":"Converting to RawData","text":"<p>The <code>to_raw_data()</code> method converts store data to an immutable <code>RawData</code> container:</p> <pre><code>from datetime import datetime\n\n# Direct from store\nraw_data = store.to_raw_data(\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n)\n\n# Access data\nspot_df = raw_data[\"spot\"]  # Polars DataFrame\nprint(raw_data.pairs)       # [\"BTCUSDT\", \"ETHUSDT\"]\nprint(raw_data.datetime_start)  # datetime(2024, 1, 1)\n</code></pre>"},{"location":"guide/data-stores/#custom-data-key","title":"Custom Data Key","text":"<pre><code># Use custom key instead of data_type\nraw_data = store.to_raw_data(\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n    data_key=\"binance_spot\",\n)\n\ndf = raw_data[\"binance_spot\"]\n</code></pre>"},{"location":"guide/data-stores/#rawdatafactory","title":"RawDataFactory","text":"<p>For combining data from multiple stores:</p> <pre><code>from signalflow.data import RawDataFactory\nfrom signalflow.data.raw_store import DuckDbRawStore\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Create stores\nspot_store = DuckDbRawStore(db_path=Path(\"data/spot.duckdb\"), data_type=\"spot\")\nfutures_store = DuckDbRawStore(db_path=Path(\"data/futures.duckdb\"), data_type=\"futures\")\n\n# Combine into single RawData\nraw_data = RawDataFactory.from_stores(\n    stores=[spot_store, futures_store],\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n)\n\n# Access both data types\nspot_df = raw_data[\"spot\"]\nfutures_df = raw_data[\"futures\"]\n</code></pre>"},{"location":"guide/data-stores/#legacy-factory-methods","title":"Legacy Factory Methods","text":"<pre><code># Single DuckDB store\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/spot.duckdb\"),\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n)\n\n# From Polars DataFrames\nraw_data = RawDataFactory.from_polars(\n    spot_df=my_polars_df,\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n)\n</code></pre>"},{"location":"guide/data-stores/#data-types-and-schema","title":"Data Types and Schema","text":""},{"location":"guide/data-stores/#built-in-data-types","title":"Built-in Data Types","text":"Type Columns <code>spot</code> pair, timestamp, open, high, low, close, volume, trades <code>futures</code> spot + open_interest <code>perpetual</code> spot + open_interest, funding_rate"},{"location":"guide/data-stores/#using-different-data-types","title":"Using Different Data Types","text":"<pre><code># Spot data\nspot_store = DuckDbRawStore(db_path=Path(\"spot.duckdb\"), data_type=\"spot\")\n\n# Futures data (includes open_interest)\nfutures_store = DuckDbRawStore(db_path=Path(\"futures.duckdb\"), data_type=\"futures\")\nfutures_klines = [\n    {\"timestamp\": datetime(2024, 1, 1), \"open\": 42000.0, \"high\": 42100.0,\n     \"low\": 41900.0, \"close\": 42050.0, \"volume\": 100.0, \"open_interest\": 50000.0},\n]\nfutures_store.insert_klines(\"BTCUSDT\", futures_klines)\n\n# Perpetual data (includes open_interest + funding_rate)\nperp_store = DuckDbRawStore(db_path=Path(\"perp.duckdb\"), data_type=\"perpetual\")\nperp_klines = [\n    {\"timestamp\": datetime(2024, 1, 1), \"open\": 42000.0, \"high\": 42100.0,\n     \"low\": 41900.0, \"close\": 42050.0, \"volume\": 100.0,\n     \"open_interest\": 50000.0, \"funding_rate\": 0.0001},\n]\nperp_store.insert_klines(\"BTCUSDT\", perp_klines)\n</code></pre>"},{"location":"guide/data-stores/#store-operations","title":"Store Operations","text":""},{"location":"guide/data-stores/#upsert-semantics","title":"Upsert Semantics","text":"<p>All stores use upsert (INSERT OR REPLACE) based on <code>(pair, timestamp)</code> key:</p> <pre><code># Insert initial data\nstore.insert_klines(\"BTCUSDT\", [\n    {\"timestamp\": datetime(2024, 1, 1, 0, 0), \"close\": 42000.0, ...}\n])\n\n# Re-insert with updated values (replaces existing)\nstore.insert_klines(\"BTCUSDT\", [\n    {\"timestamp\": datetime(2024, 1, 1, 0, 0), \"close\": 42100.0, ...}\n])\n\n# Result: only one row with close=42100.0\n</code></pre>"},{"location":"guide/data-stores/#time-bounds","title":"Time Bounds","text":"<pre><code># Get first and last timestamp for a pair\nfirst_ts, last_ts = store.get_time_bounds(\"BTCUSDT\")\nprint(f\"Data from {first_ts} to {last_ts}\")\n</code></pre>"},{"location":"guide/data-stores/#gap-detection","title":"Gap Detection","text":"<pre><code>from datetime import datetime\n\n# Find gaps in data\ngaps = store.find_gaps(\n    pair=\"BTCUSDT\",\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31),\n    tf_minutes=1,  # 1-minute candles\n)\n\nfor gap_start, gap_end in gaps:\n    print(f\"Missing data: {gap_start} to {gap_end}\")\n</code></pre>"},{"location":"guide/data-stores/#statistics","title":"Statistics","text":"<pre><code># Get per-pair statistics\nstats = store.get_stats()\nprint(stats)\n# shape: (2, 5)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 pair     \u2506 rows  \u2506 first_candle        \u2506 last_candle         \u2506 total_volume \u2502\n# \u2502 ---      \u2506 ---   \u2506 ---                 \u2506 ---                 \u2506 ---          \u2502\n# \u2502 str      \u2506 u32   \u2506 datetime[\u03bcs]        \u2506 datetime[\u03bcs]        \u2506 f64          \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 BTCUSDT  \u2506 10000 \u2506 2024-01-01 00:00:00 \u2506 2024-01-07 22:39:00 \u2506 1500000.0    \u2502\n# \u2502 ETHUSDT  \u2506 10000 \u2506 2024-01-01 00:00:00 \u2506 2024-01-07 22:39:00 \u2506 2100000.0    \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guide/data-stores/#creating-a-custom-store","title":"Creating a Custom Store","text":"<p>Extend <code>RawDataStore</code> to implement your own backend:</p> <pre><code>from dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Optional, Iterable\nimport polars as pl\n\nfrom signalflow.core import sf_component, SfComponentType\nfrom signalflow.core.containers.raw_data import RawData\nfrom signalflow.data.raw_store.base import RawDataStore\n\n\n@dataclass\n@sf_component(name=\"parquet/spot\")\nclass ParquetRawStore(RawDataStore):\n    \"\"\"Parquet file storage backend.\"\"\"\n\n    data_dir: Path\n    data_type: str = \"spot\"\n    _cache: dict[str, pl.DataFrame] = field(default_factory=dict, repr=False)\n\n    def __post_init__(self):\n        self.data_dir.mkdir(parents=True, exist_ok=True)\n\n    def _file_path(self, pair: str) -&gt; Path:\n        return self.data_dir / f\"{pair}.parquet\"\n\n    def insert_klines(self, pair: str, klines: list[dict]) -&gt; None:\n        if not klines:\n            return\n\n        new_df = pl.DataFrame(klines)\n        path = self._file_path(pair)\n\n        if path.exists():\n            existing = pl.read_parquet(path)\n            # Upsert: remove existing timestamps, add new\n            combined = pl.concat([\n                existing.filter(~pl.col(\"timestamp\").is_in(new_df[\"timestamp\"])),\n                new_df,\n            ]).sort(\"timestamp\")\n        else:\n            combined = new_df\n\n        combined.write_parquet(path)\n\n    def get_time_bounds(self, pair: str) -&gt; tuple[Optional[datetime], Optional[datetime]]:\n        path = self._file_path(pair)\n        if not path.exists():\n            return (None, None)\n\n        df = pl.read_parquet(path, columns=[\"timestamp\"])\n        if df.is_empty():\n            return (None, None)\n\n        return (df[\"timestamp\"].min(), df[\"timestamp\"].max())\n\n    def find_gaps(\n        self, pair: str, start: datetime, end: datetime, tf_minutes: int\n    ) -&gt; list[tuple[datetime, datetime]]:\n        # Implementation similar to other stores\n        pass\n\n    def load(\n        self,\n        pair: str,\n        hours: Optional[int] = None,\n        start: Optional[datetime] = None,\n        end: Optional[datetime] = None,\n    ) -&gt; pl.DataFrame:\n        path = self._file_path(pair)\n        if not path.exists():\n            return pl.DataFrame()\n\n        df = pl.read_parquet(path)\n\n        # Apply time filters\n        if hours is not None:\n            cutoff = datetime.now() - timedelta(hours=hours)\n            df = df.filter(pl.col(\"timestamp\") &gt; cutoff)\n        elif start and end:\n            df = df.filter(\n                (pl.col(\"timestamp\") &gt;= start) &amp; (pl.col(\"timestamp\") &lt;= end)\n            )\n\n        return df.sort(\"timestamp\")\n\n    def load_many(\n        self,\n        pairs: Iterable[str],\n        hours: Optional[int] = None,\n        start: Optional[datetime] = None,\n        end: Optional[datetime] = None,\n    ) -&gt; pl.DataFrame:\n        dfs = []\n        for pair in pairs:\n            df = self.load(pair, hours=hours, start=start, end=end)\n            if not df.is_empty():\n                dfs.append(df.with_columns(pl.lit(pair).alias(\"pair\")))\n\n        if not dfs:\n            return pl.DataFrame()\n\n        return pl.concat(dfs).sort([\"pair\", \"timestamp\"])\n\n    def load_many_pandas(self, pairs: list[str], start=None, end=None):\n        return self.load_many(pairs, start=start, end=end).to_pandas()\n\n    def get_stats(self) -&gt; pl.DataFrame:\n        # Implementation\n        pass\n\n    def close(self) -&gt; None:\n        self._cache.clear()\n\n    def to_raw_data(\n        self,\n        pairs: list[str],\n        start: datetime,\n        end: datetime,\n        data_key: Optional[str] = None,\n    ) -&gt; RawData:\n        key = data_key if data_key is not None else self.data_type\n        df = self.load_many(pairs=pairs, start=start, end=end)\n\n        return RawData(\n            datetime_start=start,\n            datetime_end=end,\n            pairs=pairs,\n            data={key: df},\n        )\n</code></pre>"},{"location":"guide/data-stores/#storefactory","title":"StoreFactory","text":"<p>Create stores dynamically using the factory:</p> <pre><code>from signalflow.data import StoreFactory\n\n# Create DuckDB store\nstore = StoreFactory.create_raw_store(\n    backend=\"duckdb\",\n    data_type=\"spot\",\n    db_path=\"data/spot.duckdb\",\n)\n\n# Create SQLite store\nstore = StoreFactory.create_raw_store(\n    backend=\"sqlite\",\n    data_type=\"futures\",\n    db_path=\"data/futures.sqlite\",\n)\n\n# Create in-memory store\nstore = StoreFactory.create_raw_store(\n    backend=\"memory\",\n    data_type=\"spot\",\n)\n</code></pre>"},{"location":"guide/data-stores/#postgresql-store-optional","title":"PostgreSQL Store (Optional)","text":"<p>For production deployments:</p> <pre><code># Requires: pip install signalflow[postgres]\nfrom signalflow.data.raw_store import PgRawStore\n\nstore = PgRawStore(\n    host=\"localhost\",\n    port=5432,\n    database=\"signalflow\",\n    user=\"postgres\",\n    password=\"secret\",\n    data_type=\"spot\",\n)\n\nstore.insert_klines(\"BTCUSDT\", klines)\ndf = store.load(\"BTCUSDT\", hours=24)\nstore.close()\n</code></pre>"},{"location":"guide/data-stores/#best-practices","title":"Best Practices","text":""},{"location":"guide/data-stores/#connection-management","title":"Connection Management","text":"<pre><code># Use try/finally\nstore = DuckDbRawStore(db_path=Path(\"data.duckdb\"))\ntry:\n    df = store.load(\"BTCUSDT\")\nfinally:\n    store.close()\n\n# Or context manager (if implemented)\nwith DuckDbRawStore(db_path=Path(\"data.duckdb\")) as store:\n    df = store.load(\"BTCUSDT\")\n</code></pre>"},{"location":"guide/data-stores/#batch-inserts","title":"Batch Inserts","text":"<pre><code># Prefer batch insert over single klines\nall_klines = fetch_many_klines()  # e.g., 10,000 klines\nstore.insert_klines(\"BTCUSDT\", all_klines)  # Single batch\n\n# Avoid\nfor kline in all_klines:\n    store.insert_klines(\"BTCUSDT\", [kline])  # 10,000 separate inserts\n</code></pre>"},{"location":"guide/data-stores/#data-validation","title":"Data Validation","text":"<pre><code># to_raw_data() validates data automatically:\n# - Checks required columns (pair, timestamp)\n# - Normalizes timestamps to UTC-naive\n# - Detects duplicate (pair, timestamp) combinations\n\ntry:\n    raw_data = store.to_raw_data(pairs=[\"BTCUSDT\"], start=start, end=end)\nexcept ValueError as e:\n    print(f\"Data validation failed: {e}\")\n</code></pre>"},{"location":"guide/data-stores/#api-reference","title":"API Reference","text":"<p>See the full API documentation:</p> <ul> <li><code>RawDataStore</code> - Base class for all stores</li> <li><code>DuckDbRawStore</code> - DuckDB implementation</li> <li><code>SqliteRawStore</code> - SQLite implementation</li> <li><code>InMemoryRawStore</code> - In-memory implementation</li> <li><code>RawDataFactory</code> - Factory for creating RawData</li> <li><code>StoreFactory</code> - Factory for creating stores</li> </ul>"},{"location":"guide/model-integration/","title":"External Model Integration","text":"<p>This guide covers integrating external ML/RL models with SignalFlow for automated trading decisions.</p>"},{"location":"guide/model-integration/#overview","title":"Overview","text":"<p>SignalFlow supports external model integration via a Protocol-based interface:</p> <pre><code>Backtest Bar\n    |\n    v\nBuild ModelContext (signals, metrics, positions)\n    |\n    v\nmodel.decide(context) --&gt; list[StrategyDecision]\n    |\n    +---&gt; CLOSE/CLOSE_ALL --&gt; ModelExitRule --&gt; Exit Orders\n    |\n    +---&gt; ENTER --&gt; ModelEntryRule --&gt; Entry Orders\n</code></pre> <p>Design Principle: Models receive signals and metrics, NOT raw OHLCV prices.</p>"},{"location":"guide/model-integration/#quick-start","title":"Quick Start","text":""},{"location":"guide/model-integration/#1-implement-the-protocol","title":"1. Implement the Protocol","text":"<pre><code>from signalflow.strategy.model import (\n    StrategyModel,\n    StrategyAction,\n    StrategyDecision,\n    ModelContext,\n)\n\n\nclass MyModel:\n    \"\"\"Your ML/RL model implementing StrategyModel protocol.\"\"\"\n\n    def decide(self, context: ModelContext) -&gt; list[StrategyDecision]:\n        decisions = []\n\n        for row in context.signals.value.iter_rows(named=True):\n            prob = row.get(\"probability\", 0.5)\n\n            if prob &gt; 0.7:\n                decisions.append(StrategyDecision(\n                    action=StrategyAction.ENTER,\n                    pair=row[\"pair\"],\n                    confidence=prob,\n                ))\n\n        return decisions\n</code></pre>"},{"location":"guide/model-integration/#2-create-rules","title":"2. Create Rules","text":"<pre><code>from signalflow.strategy.model import ModelEntryRule, ModelExitRule\n\nmodel = MyModel()\n\nentry_rule = ModelEntryRule(\n    model=model,\n    base_position_size=0.02,\n    max_positions=5,\n    min_confidence=0.6,\n)\n\nexit_rule = ModelExitRule(\n    model=model,\n    min_confidence=0.7,\n)\n</code></pre>"},{"location":"guide/model-integration/#3-run-backtest","title":"3. Run Backtest","text":"<pre><code>from signalflow.strategy.runner import BacktestRunner\nfrom signalflow.strategy.broker import BacktestBroker\nfrom signalflow.strategy.broker.executor import VirtualSpotExecutor\n\nrunner = BacktestRunner(\n    strategy_id=\"model_strategy\",\n    broker=BacktestBroker(executor=VirtualSpotExecutor(fee_rate=0.001)),\n    entry_rules=[entry_rule],\n    exit_rules=[exit_rule],\n    initial_capital=10_000.0,\n)\n\nstate = runner.run(raw_data, signals)\n</code></pre>"},{"location":"guide/model-integration/#decision-types","title":"Decision Types","text":"<p>Models return <code>StrategyDecision</code> objects with these actions:</p> Action Description Required Fields <code>ENTER</code> Open new position <code>pair</code>, optionally <code>size_multiplier</code> <code>SKIP</code> Skip this signal <code>pair</code> <code>CLOSE</code> Close specific position <code>pair</code>, <code>position_id</code> <code>CLOSE_ALL</code> Close all positions for pair <code>pair</code> <code>HOLD</code> Do nothing -"},{"location":"guide/model-integration/#entry-decision","title":"Entry Decision","text":"<pre><code>StrategyDecision(\n    action=StrategyAction.ENTER,\n    pair=\"BTCUSDT\",\n    size_multiplier=1.5,  # 1.5x base position size\n    confidence=0.85,\n    meta={\"reason\": \"high_confidence_signal\"},\n)\n</code></pre>"},{"location":"guide/model-integration/#exit-decision","title":"Exit Decision","text":"<pre><code>StrategyDecision(\n    action=StrategyAction.CLOSE,\n    pair=\"BTCUSDT\",\n    position_id=\"pos_abc123\",  # Required for CLOSE\n    confidence=0.9,\n    meta={\"reason\": \"take_profit\"},\n)\n</code></pre>"},{"location":"guide/model-integration/#close-all-positions","title":"Close All Positions","text":"<pre><code>StrategyDecision(\n    action=StrategyAction.CLOSE_ALL,\n    pair=\"BTCUSDT\",  # Close all BTC positions\n    confidence=0.8,\n    meta={\"reason\": \"risk_off\"},\n)\n</code></pre>"},{"location":"guide/model-integration/#modelcontext","title":"ModelContext","text":"<p>The context passed to <code>model.decide()</code> contains:</p> Field Type Description <code>timestamp</code> <code>datetime</code> Current bar timestamp <code>signals</code> <code>Signals</code> Current bar signals (from detectors) <code>prices</code> <code>dict[str, float]</code> Current prices per pair <code>positions</code> <code>list[Position]</code> Open positions <code>metrics</code> <code>dict[str, float]</code> Portfolio metrics (equity, drawdown, etc.) <code>runtime</code> <code>dict[str, Any]</code> Custom state (regime, ATR, etc.)"},{"location":"guide/model-integration/#accessing-context-data","title":"Accessing Context Data","text":"<pre><code>def decide(self, context: ModelContext) -&gt; list[StrategyDecision]:\n    # Check portfolio state\n    equity = context.metrics.get(\"equity\", 0)\n    drawdown = context.metrics.get(\"max_drawdown\", 0)\n\n    # Risk check\n    if drawdown &gt; 0.15:\n        return []  # No trading during high drawdown\n\n    # Process signals\n    for row in context.signals.value.iter_rows(named=True):\n        pair = row[\"pair\"]\n        signal_type = row[\"signal_type\"]\n        prob = row[\"probability\"]\n\n        # Get current price\n        price = context.prices.get(pair, 0)\n\n        # Check existing positions\n        pair_positions = [p for p in context.positions if p.pair == pair]\n\n        # Your model logic here...\n</code></pre>"},{"location":"guide/model-integration/#model-based-exit-management","title":"Model-Based Exit Management","text":"<p>Models can manage exits by analyzing open positions:</p> <pre><code>def decide(self, context: ModelContext) -&gt; list[StrategyDecision]:\n    decisions = []\n\n    for pos in context.positions:\n        price = context.prices.get(pos.pair, pos.entry_price)\n        pnl_pct = (price - pos.entry_price) / pos.entry_price\n\n        # Take profit\n        if pnl_pct &gt; 0.05:  # 5% profit\n            decisions.append(StrategyDecision(\n                action=StrategyAction.CLOSE,\n                pair=pos.pair,\n                position_id=pos.id,\n                confidence=0.9,\n                meta={\"reason\": \"model_take_profit\"},\n            ))\n\n        # Stop loss\n        elif pnl_pct &lt; -0.02:  # 2% loss\n            decisions.append(StrategyDecision(\n                action=StrategyAction.CLOSE,\n                pair=pos.pair,\n                position_id=pos.id,\n                confidence=0.95,\n                meta={\"reason\": \"model_stop_loss\"},\n            ))\n\n    return decisions\n</code></pre>"},{"location":"guide/model-integration/#combining-with-traditional-rules","title":"Combining with Traditional Rules","text":"<p>Model rules work alongside traditional rules:</p> <pre><code>from signalflow.strategy.component.exit import TakeProfitStopLossExit\n\nrunner = BacktestRunner(\n    strategy_id=\"hybrid_strategy\",\n    broker=broker,\n    entry_rules=[\n        ModelEntryRule(model=model, base_position_size=0.02),\n    ],\n    exit_rules=[\n        ModelExitRule(model=model),  # Model-based exits first\n        TakeProfitStopLossExit(      # Fallback TP/SL\n            take_profit_pct=0.05,\n            stop_loss_pct=0.03,\n        ),\n    ],\n    initial_capital=10_000.0,\n)\n</code></pre> <p>Exit rules are processed in order - model exits run first, then traditional rules catch remaining positions.</p>"},{"location":"guide/model-integration/#decision-caching","title":"Decision Caching","text":"<p>The model is called once per bar. Decisions are cached in <code>state.runtime</code>:</p> <pre><code>Bar Start\n    |\n    v\nExitRule.check_exits()\n    \u251c\u2500\u2500 Check cache for decisions\n    \u251c\u2500\u2500 If empty: call model.decide(), cache result\n    \u2514\u2500\u2500 Process CLOSE/CLOSE_ALL decisions\n    |\n    v\nEntryRule.check_entries()\n    \u251c\u2500\u2500 Check cache (uses cached decisions)\n    \u2514\u2500\u2500 Process ENTER decisions\n    |\n    v\nstate.reset_tick_cache() clears cache for next bar\n</code></pre> <p>This ensures consistent decisions across entry and exit processing.</p>"},{"location":"guide/model-integration/#training-data-export","title":"Training Data Export","text":"<p>Export backtest results for model training:</p> <pre><code>from pathlib import Path\nfrom signalflow.strategy.exporter import BacktestExporter\n\nexporter = BacktestExporter()\n\n# During backtest\nfor ts in timestamps:\n    # ... process bar ...\n    exporter.export_bar(ts, signals, state.metrics, state)\n\n# When positions close\nexporter.export_position_close(position, exit_time, exit_price, \"take_profit\")\n\n# Write to disk\nexporter.finalize(Path(\"./training_data\"))\n</code></pre>"},{"location":"guide/model-integration/#exported-files","title":"Exported Files","text":"<p>bars.parquet - Per-bar state: <pre><code>import polars as pl\n\nbars = pl.read_parquet(\"./training_data/bars.parquet\")\n# Columns: timestamp, pair, signal_type, probability,\n#          metric_equity, metric_max_drawdown, ...\n</code></pre></p> <p>trades.parquet - Completed trades: <pre><code>trades = pl.read_parquet(\"./training_data/trades.parquet\")\n# Columns: position_id, pair, entry_time, exit_time,\n#          entry_price, exit_price, realized_pnl, exit_reason, ...\n</code></pre></p>"},{"location":"guide/model-integration/#rl-model-example","title":"RL Model Example","text":"<p>Complete example with reinforcement learning patterns:</p> <pre><code>import numpy as np\nfrom signalflow.strategy.model import (\n    StrategyAction,\n    StrategyDecision,\n    ModelContext,\n)\n\n\nclass RLTradingModel:\n    \"\"\"RL model for trading decisions.\"\"\"\n\n    def __init__(self, model_path: str, epsilon: float = 0.0):\n        self.model = self._load_model(model_path)\n        self.epsilon = epsilon  # Exploration rate\n\n    def decide(self, context: ModelContext) -&gt; list[StrategyDecision]:\n        decisions = []\n\n        # Build state features\n        features = self._build_features(context)\n\n        # Get Q-values from model\n        q_values = self.model.predict(features)\n\n        # Epsilon-greedy action selection (for training)\n        if np.random.random() &lt; self.epsilon:\n            action_idx = np.random.choice(len(q_values))\n        else:\n            action_idx = np.argmax(q_values)\n\n        # Map to trading action\n        action = self._map_action(action_idx, context)\n        if action:\n            decisions.append(action)\n\n        return decisions\n\n    def _build_features(self, context: ModelContext) -&gt; np.ndarray:\n        \"\"\"Build feature vector from context.\"\"\"\n        features = []\n\n        # Portfolio state\n        features.append(context.metrics.get(\"equity\", 10000) / 10000)\n        features.append(context.metrics.get(\"max_drawdown\", 0))\n\n        # Position count\n        features.append(len(context.positions) / 10)\n\n        # Signal features\n        for row in context.signals.value.head(5).iter_rows(named=True):\n            features.append(row.get(\"probability\", 0.5))\n            features.append(1 if row.get(\"signal_type\") == \"rise\" else -1)\n\n        # Pad if fewer signals\n        while len(features) &lt; 15:\n            features.append(0)\n\n        return np.array(features[:15])\n\n    def _map_action(\n        self, action_idx: int, context: ModelContext\n    ) -&gt; StrategyDecision | None:\n        \"\"\"Map action index to StrategyDecision.\"\"\"\n        # 0: HOLD, 1: ENTER, 2: CLOSE_ALL\n        if action_idx == 1 and context.signals.value.height &gt; 0:\n            row = context.signals.value.row(0, named=True)\n            return StrategyDecision(\n                action=StrategyAction.ENTER,\n                pair=row[\"pair\"],\n                confidence=0.8,\n            )\n        elif action_idx == 2 and context.positions:\n            return StrategyDecision(\n                action=StrategyAction.CLOSE_ALL,\n                pair=context.positions[0].pair,\n                confidence=0.9,\n            )\n        return None\n</code></pre>"},{"location":"guide/model-integration/#best-practices","title":"Best Practices","text":""},{"location":"guide/model-integration/#1-confidence-thresholds","title":"1. Confidence Thresholds","text":"<p>Set appropriate confidence thresholds:</p> <pre><code>entry_rule = ModelEntryRule(\n    model=model,\n    min_confidence=0.6,  # Only act on confident entries\n)\n\nexit_rule = ModelExitRule(\n    model=model,\n    min_confidence=0.7,  # Higher threshold for exits\n)\n</code></pre>"},{"location":"guide/model-integration/#2-position-size-scaling","title":"2. Position Size Scaling","text":"<p>Use <code>size_multiplier</code> to scale by confidence:</p> <pre><code>StrategyDecision(\n    action=StrategyAction.ENTER,\n    pair=\"BTCUSDT\",\n    size_multiplier=min(confidence, 1.5),  # Cap at 1.5x\n    confidence=confidence,\n)\n</code></pre>"},{"location":"guide/model-integration/#3-risk-management-in-model","title":"3. Risk Management in Model","text":"<p>Check portfolio state before trading:</p> <pre><code>def decide(self, context: ModelContext) -&gt; list[StrategyDecision]:\n    # Skip during high drawdown\n    if context.metrics.get(\"max_drawdown\", 0) &gt; 0.15:\n        return []\n\n    # Limit position count\n    if len(context.positions) &gt;= 5:\n        return []  # Or only return exit decisions\n\n    # Your trading logic...\n</code></pre>"},{"location":"guide/model-integration/#4-meta-in-decisions","title":"4. Meta in Decisions","text":"<p>Include debugging info in <code>meta</code>:</p> <pre><code>StrategyDecision(\n    action=StrategyAction.ENTER,\n    pair=\"BTCUSDT\",\n    confidence=0.85,\n    meta={\n        \"model_version\": \"v2.1\",\n        \"signal_type\": \"rise\",\n        \"features\": {\"rsi\": 35, \"trend\": \"up\"},\n    },\n)\n</code></pre>"},{"location":"guide/model-integration/#see-also","title":"See Also","text":"<ul> <li>API Reference: Detailed component documentation</li> <li>Advanced Strategies: Position sizing and entry filters</li> <li>Quick Start: Basic strategy setup</li> </ul>"},{"location":"guide/signal-architecture/","title":"Signal Architecture &amp; Meta-Labeling","text":"<p>This page describes the core philosophy behind SignalFlow: the signal-driven approach to algorithmic trading based on Marcos Lopez de Prado's meta-labeling methodology.</p>"},{"location":"guide/signal-architecture/#the-core-idea","title":"The Core Idea","text":"<p>Traditional algorithmic trading systems make binary decisions: buy or sell based on some rule. SignalFlow takes a fundamentally different approach inspired by machine learning research in quantitative finance:</p> <ol> <li>Signal Detection -- identify potential market changes</li> <li>Signal Validation -- use ML to estimate the probability that a signal is correct</li> <li>Strategy Execution -- use validated signals to make trading decisions</li> </ol> <p>This separation is critical: the detector's job is to have high recall (detect as many real opportunities as possible), while the validator's job is to have high precision (filter out false positives).</p> <pre><code>flowchart LR\n    D[Detector] --&gt;|raw signals| V[Validator]\n    V --&gt;|filtered signals + probability| S[Strategy]\n    L[Labeler] --&gt;|historical labels| T[Train Validator]\n    T --&gt; V\n\n    style D fill:#ea580c,stroke:#f97316,color:#fff\n    style V fill:#16a34a,stroke:#22c55e,color:#fff\n    style S fill:#dc2626,stroke:#ef4444,color:#fff\n    style L fill:#7c3aed,stroke:#8b5cf6,color:#fff\n    style T fill:#0891b2,stroke:#06b6d4,color:#fff</code></pre>"},{"location":"guide/signal-architecture/#what-is-a-signal","title":"What is a Signal?","text":"<p>A signal is a prediction that a specific type of market change is occurring or is about to occur. Signals are not trading orders -- they are informational events that the strategy layer interprets.</p>"},{"location":"guide/signal-architecture/#two-level-signal-model","title":"Two-Level Signal Model","text":"<p>SignalFlow uses a two-level signal model:</p> Level Field Type Purpose Category <code>signal_category</code> <code>SignalCategory</code> enum Broad classification for routing Type <code>signal_type</code> <code>str</code> Specific signal value within category <pre><code>from signalflow.core.enums import SignalCategory\n\n# A signal has both a category and a type\nsignal_category = SignalCategory.PRICE_DIRECTION  # broad: about price movement\nsignal_type = \"rise\"                               # specific: price is rising\n</code></pre> <p>This design allows the system to:</p> <ul> <li>Route signals based on category (e.g., directional signals go to entry   rules, volatility signals go to position sizing)</li> <li>Extend signal types without modifying core code (any string is valid as a   <code>signal_type</code>)</li> <li>Keep multiple signal categories per timestamp (e.g., a bar can   simultaneously have a <code>price_direction=rise</code> signal and a   <code>volatility=high_volatility</code> signal)</li> </ul>"},{"location":"guide/signal-architecture/#signal-categories","title":"Signal Categories","text":"Category Description Example Types <code>PRICE_DIRECTION</code> Price movement direction <code>rise</code>, <code>fall</code>, <code>flat</code> <code>PRICE_STRUCTURE</code> Price patterns and extrema <code>local_max</code>, <code>local_min</code>, <code>breakout_up</code> <code>TREND_MOMENTUM</code> Trend state and momentum <code>trend_start</code>, <code>trend_reversal</code>, <code>overbought</code> <code>VOLATILITY</code> Volatility regime <code>high_volatility</code>, <code>low_volatility</code>, <code>volatility_expansion</code> <code>VOLUME_LIQUIDITY</code> Volume and liquidity patterns <code>abnormal_volume</code>, <code>illiquidity</code>, <code>accumulation</code> <code>MARKET_WIDE</code> Cross-pair market events <code>market_crash</code>, <code>regime_shift</code>, <code>synchronization</code> <code>ANOMALY</code> Anomalous events <code>extreme_positive_anomaly</code>, <code>extreme_negative_anomaly</code> <p>The full registry of known signal types is in <code>signalflow.core.signal_registry.KNOWN_SIGNALS</code>. This registry is advisory -- new signal types can be used without modifying it.</p>"},{"location":"guide/signal-architecture/#null-vs-flat-uncertainty-vs-market-state","title":"Null vs FLAT: Uncertainty vs Market State","text":"<p>A critical distinction in SignalFlow:</p> Value Meaning Example <code>\"flat\"</code> The market is moving sideways. This is a valid signal -- a real market state. After a strong trend, price consolidates in a range. The labeler identifies this as <code>\"flat\"</code>. <code>null</code> The labeler/detector doesn't know. Epistemic uncertainty. An unexpected tweet from a public figure causes unpredictable volatility. The labeler cannot determine direction and outputs <code>null</code>. <p>This matters for ML training: <code>\"flat\"</code> is a valid class label that the model should learn to predict. <code>null</code> means \"skip this sample\" -- the labeler has no information about this timestamp, and the model should not be trained on it.</p> <pre><code># FLAT = real market state (valid signal)\npl.lit(\"flat\").alias(\"signal_type\")     # \"I know the market is sideways\"\n\n# null = uncertainty (no information)\npl.lit(None, dtype=pl.Utf8).alias(\"signal_type\")  # \"I don't know what's happening\"\n</code></pre>"},{"location":"guide/signal-architecture/#the-dual-pipeline-labelers-vs-detectors","title":"The Dual Pipeline: Labelers vs Detectors","text":"<p>Every signal category in SignalFlow can have two complementary implementations:</p>"},{"location":"guide/signal-architecture/#labeler-historical-forward-looking","title":"Labeler (Historical, Forward-Looking)","text":"<ul> <li>Lives in <code>signalflow.target</code> module</li> <li>Knows the future -- uses data after timestamp <code>t</code> to label <code>t</code></li> <li>Purpose: generate training labels for ML models</li> <li>Extends <code>Labeler</code> base class, implements <code>compute_group()</code></li> <li>Length-preserving: output has the same number of rows as input</li> </ul> <pre><code>from signalflow.target import AnomalyLabeler\n\nlabeler = AnomalyLabeler(\n    threshold_return_std=4.0,\n    horizon=60,\n    mask_to_signals=False,\n)\nlabeled_df = labeler.compute(ohlcv_df)\n# Each row gets a label: \"extreme_positive_anomaly\", \"extreme_negative_anomaly\", or null\n</code></pre>"},{"location":"guide/signal-architecture/#detector-real-time-backward-looking","title":"Detector (Real-Time, Backward-Looking)","text":"<ul> <li>Lives in <code>signalflow.detector</code> module</li> <li>Only uses past and current data -- safe for live trading</li> <li>Purpose: generate trading signals in real-time</li> <li>Extends <code>SignalDetector</code> base class, implements <code>detect()</code></li> <li>Outputs a <code>Signals</code> DataFrame (only rows with detected signals)</li> </ul> <pre><code>from signalflow.detector import AnomalyDetector\n\ndetector = AnomalyDetector(\n    threshold_return_std=4.0,\n    vol_window=1440,\n)\nsignals = detector.run(raw_data_view)\n# Only bars with anomalies are returned\n</code></pre>"},{"location":"guide/signal-architecture/#why-both","title":"Why Both?","text":"<p>The labeler knows the future, so its labels are more accurate but can only be used for training. The detector works in real-time but is less precise.</p> <p>The training pipeline:</p> <ol> <li>Run labeler on historical data to create perfect (or near-perfect) labels</li> <li>Train a validator (ML model) to predict labeler output from features</li> <li>In production, the detector generates candidate signals</li> <li>The validator filters and scores them</li> </ol> <pre><code>flowchart TB\n    subgraph Training [\"Training (offline)\"]\n        H[Historical Data] --&gt; L[Labeler]\n        L --&gt;|labels| ML[Train ML Model]\n        H --&gt; F1[Features]\n        F1 --&gt; ML\n    end\n\n    subgraph Production [\"Production (real-time)\"]\n        M[Market Data] --&gt; D[Detector]\n        M --&gt; F2[Features]\n        D --&gt;|candidate signals| V[Validator / ML Model]\n        F2 --&gt; V\n        V --&gt;|validated signals| S[Strategy]\n    end\n\n    ML -.-&gt;|trained model| V\n\n    style L fill:#7c3aed,stroke:#8b5cf6,color:#fff\n    style D fill:#ea580c,stroke:#f97316,color:#fff\n    style V fill:#16a34a,stroke:#22c55e,color:#fff\n    style ML fill:#0891b2,stroke:#06b6d4,color:#fff</code></pre>"},{"location":"guide/signal-architecture/#available-labelers-detectors","title":"Available Labelers &amp; Detectors","text":""},{"location":"guide/signal-architecture/#price-direction","title":"Price Direction","text":"<p>The most basic signal category. Predicts whether price will rise or fall.</p> Component Algorithm Signal Types <code>FixedHorizonLabeler</code> Forward return sign over N bars <code>rise</code>, <code>fall</code>, <code>null</code> <code>TripleBarrierLabeler</code> Triple barrier method (De Prado) <code>rise</code>, <code>fall</code>, <code>null</code> <code>TakeProfitLabeler</code> Symmetric TP/SL barrier <code>rise</code>, <code>fall</code>, <code>null</code> <code>TrendScanningLabeler</code> OLS t-statistic across windows (De Prado) <code>rise</code>, <code>fall</code>, <code>null</code> <code>ExampleSmaCrossDetector</code> SMA crossover (real-time) <code>rise</code>, <code>fall</code>"},{"location":"guide/signal-architecture/#anomaly","title":"Anomaly","text":"<p>Detects extreme, unexpected market events (anomalous returns).</p> Component Algorithm Signal Types <code>AnomalyLabeler</code> Forward return magnitude vs rolling vol <code>extreme_positive_anomaly</code>, <code>extreme_negative_anomaly</code>, <code>null</code> <code>AnomalyDetector</code> Current return magnitude vs rolling vol <code>extreme_positive_anomaly</code>, <code>extreme_negative_anomaly</code>"},{"location":"guide/signal-architecture/#volatility-regime","title":"Volatility Regime","text":"<p>Classifies current volatility state.</p> Component Algorithm Signal Types <code>VolatilityRegimeLabeler</code> Forward realized vol percentile <code>high_volatility</code>, <code>low_volatility</code>, <code>null</code> <code>VolatilityDetector</code> Backward realized vol percentile <code>high_volatility</code>, <code>low_volatility</code>"},{"location":"guide/signal-architecture/#price-structure","title":"Price Structure","text":"<p>Identifies local price extrema.</p> Component Algorithm Signal Types <code>StructureLabeler</code> Symmetric window extrema (look-ahead) <code>local_max</code>, <code>local_min</code>, <code>null</code> <code>StructureDetector</code> Backward zigzag with confirmation delay <code>local_max</code>, <code>local_min</code>"},{"location":"guide/signal-architecture/#volume-regime","title":"Volume Regime","text":"<p>Classifies volume patterns.</p> Component Algorithm Signal Types <code>VolumeRegimeLabeler</code> Forward volume ratio vs SMA <code>abnormal_volume</code>, <code>illiquidity</code>, <code>null</code>"},{"location":"guide/signal-architecture/#imperfect-labels-are-expected","title":"Imperfect Labels Are Expected","text":"<p>A fundamental principle of SignalFlow:</p> <p>Labels are not ground truth</p> <p>No labeler produces perfect labels. Different labelers can contradict each other for the same timestamp. Some labelers have large periods of <code>null</code> (uncertainty). This is by design.</p> <p>The philosophy:</p> <ul> <li>Each labeler implements a specific definition of what constitutes a signal</li> <li>The <code>FixedHorizonLabeler</code> says \"rise\" means the price went up over N bars</li> <li>The <code>TrendScanningLabeler</code> says \"rise\" means there's a statistically   significant upward OLS trend</li> <li>The <code>TripleBarrierLabeler</code> says \"rise\" means the take-profit barrier was hit   first</li> </ul> <p>These definitions do not always agree. A model that can predict any one of these labelers with reasonable accuracy is considered a good result.</p> <p>The task of choosing the right labeler for a given model architecture is itself an important research question. Different model architectures may perform better with different labeling strategies:</p> <ul> <li>Linear models may work best with <code>FixedHorizonLabeler</code></li> <li>Sequence models (LSTM, Transformer) may benefit from <code>TrendScanningLabeler</code></li> <li>Tree models may handle the noise in <code>TripleBarrierLabeler</code> well</li> </ul>"},{"location":"guide/signal-architecture/#meta-labeling-the-two-stage-approach","title":"Meta-Labeling: The Two-Stage Approach","text":"<p>De Prado's meta-labeling methodology works in two stages:</p>"},{"location":"guide/signal-architecture/#stage-1-primary-model-detector","title":"Stage 1: Primary Model (Detector)","text":"<p>The primary model detects the direction of potential trades. It should have high recall -- it's better to detect too many signals than to miss real ones.</p> <pre><code>from signalflow.detector import ExampleSmaCrossDetector\n\ndetector = ExampleSmaCrossDetector(fast_period=20, slow_period=50)\nsignals = detector.run(raw_data_view)\n# Many signals, some false positives\n</code></pre>"},{"location":"guide/signal-architecture/#stage-2-secondary-model-validator","title":"Stage 2: Secondary Model (Validator)","text":"<p>The secondary model (meta-labeler) predicts the probability of success for each signal from the primary model. It acts as a filter.</p> <pre><code>from signalflow.validator import LightGBMValidator\n\nvalidator = LightGBMValidator(n_estimators=100)\nvalidator.fit(X_train=features, y_train=labels)\n\nvalidated = validator.validate_signals(signals, features)\n# Each signal now has a probability estimate\n</code></pre>"},{"location":"guide/signal-architecture/#combined-in-strategy","title":"Combined in Strategy","text":"<p>The <code>SignalAggregator</code> supports a dedicated <code>META_LABELING</code> voting mode for combining detector signals with validator confidence:</p> <pre><code>from signalflow.strategy.component.entry.aggregation import (\n    SignalAggregator, VotingMode,\n)\n\naggregator = SignalAggregator(\n    voting_mode=VotingMode.META_LABELING,\n    probability_threshold=0.6,\n)\ncombined = aggregator.aggregate([detector_signals, validator_signals])\n# Direction from detector, confidence from validator\n</code></pre>"},{"location":"guide/signal-architecture/#from-signals-to-actions","title":"From Signals to Actions","text":"<p>Not all signals map directly to buy/sell decisions. A <code>high_volatility</code> signal doesn't mean \"buy\" or \"sell\" -- it means \"volatility is high\", which might affect position sizing or risk management.</p> <p>The decision of what to do with a signal is the strategy layer's responsibility. This logic can range from simple rules to complex RL policies:</p> <ul> <li>Simple: <code>rise -&gt; BUY</code>, <code>fall -&gt; SELL</code> (built into entry rules)</li> <li>Configurable: <code>local_min -&gt; BUY</code>, <code>overbought -&gt; SELL</code> (via <code>signal_type_map</code>)</li> <li>Contextual: <code>high_volatility + local_min -&gt; BUY with larger size</code></li> <li>Learned: RL model that optimizes actions over signal combinations</li> </ul>"},{"location":"guide/signal-architecture/#configurable-signal-to-action-mapping","title":"Configurable Signal-to-Action Mapping","text":"<p>Entry rules (<code>SignalEntryRule</code>, <code>FixedSizeEntryRule</code>, <code>ModelEntryRule</code>) support a <code>signal_type_map</code> parameter that maps any <code>signal_type</code> to an order side:</p> <pre><code>from signalflow.strategy.component.entry import SignalEntryRule\n\n# Custom mapping: trade structure signals\nentry = SignalEntryRule(\n    signal_type_map={\n        \"local_min\": \"BUY\",\n        \"local_max\": \"SELL\",\n        \"oversold\": \"BUY\",\n        \"overbought\": \"SELL\",\n    },\n    base_position_size=200.0,\n)\n</code></pre> <p>When <code>signal_type_map=None</code> (default), legacy behavior is used: only <code>\"rise\"</code> and <code>\"fall\"</code> signals are recognized.</p>"},{"location":"guide/signal-architecture/#directional_signal_map","title":"DIRECTIONAL_SIGNAL_MAP","text":"<p>The <code>DIRECTIONAL_SIGNAL_MAP</code> in <code>signalflow.core.signal_registry</code> provides a global registry of inherently directional signal types. Use the <code>from_directional_map()</code> classmethod to create an entry rule that trades all known directional signals:</p> <pre><code>from signalflow.core.signal_registry import DIRECTIONAL_SIGNAL_MAP\n\n# Global registry of directional signal types\nDIRECTIONAL_SIGNAL_MAP = {\n    \"rise\": \"BUY\", \"fall\": \"SELL\",\n    \"local_min\": \"BUY\", \"local_max\": \"SELL\",\n    \"breakout_up\": \"BUY\", \"breakout_down\": \"SELL\",\n    \"oversold\": \"BUY\", \"overbought\": \"SELL\",\n}\n\n# Create entry rule that trades all directional signals\nentry = SignalEntryRule.from_directional_map(base_position_size=200.0)\n</code></pre>"},{"location":"guide/signal-architecture/#smooth-labeling-planned","title":"Smooth Labeling (Planned)","text":"<p>Standard categorical labels (<code>\"rise\"</code>, <code>\"fall\"</code>) lose information about the magnitude of the signal. A barely-rising market and a strongly-rising market both get the same label.</p> <p>Smooth labeling preserves this information:</p> <pre><code>signal_category = \"price_direction\"\nsignal_type = \"rise\"           # categorical (for routing and strategy logic)\nsignal_value = 0.73            # continuous (magnitude/confidence for ML training)\n</code></pre> <p>The <code>signal_type</code> remains categorical for routing purposes, while <code>signal_value</code> (stored in the existing <code>signal</code> column of the <code>Signals</code> DataFrame) carries the continuous magnitude. This improves ML training by providing a richer target variable.</p> <p>Status</p> <p>Smooth labeling is planned for a future release. The <code>TrendScanningLabeler</code> already provides a <code>t_stat</code> meta column that can serve as a continuous signal value when <code>include_meta=True</code>.</p>"},{"location":"guide/signal-architecture/#extending-the-signal-taxonomy","title":"Extending the Signal Taxonomy","text":"<p>Adding a new signal category requires minimal changes:</p>"},{"location":"guide/signal-architecture/#1-create-a-labeler","title":"1. Create a Labeler","text":"<pre><code>from dataclasses import dataclass\nfrom signalflow.core import sf_component\nfrom signalflow.core.enums import SignalCategory\nfrom signalflow.target.base import Labeler\n\n@dataclass\n@sf_component(name=\"my_custom_labeler\")\nclass MyLabeler(Labeler):\n    signal_category = SignalCategory.MARKET_WIDE  # or any category\n\n    def compute_group(self, group_df, data_context=None):\n        # Your forward-looking labeling algorithm\n        ...\n        return group_df.with_columns(\n            pl.when(condition)\n            .then(pl.lit(\"my_signal_type\"))\n            .otherwise(pl.lit(None, dtype=pl.Utf8))\n            .alias(self.out_col)\n        )\n</code></pre>"},{"location":"guide/signal-architecture/#2-create-a-detector-optional","title":"2. Create a Detector (optional)","text":"<pre><code>from dataclasses import dataclass, field\nfrom signalflow.core import sf_component\nfrom signalflow.core.enums import SignalCategory\nfrom signalflow.detector.base import SignalDetector\n\n@dataclass\n@sf_component(name=\"my_custom_detector\")\nclass MyDetector(SignalDetector):\n    signal_category = SignalCategory.MARKET_WIDE\n    allowed_signal_types: set[str] | None = field(\n        default_factory=lambda: {\"my_signal_type\"}\n    )\n\n    def detect(self, features, context=None):\n        # Your backward-looking detection algorithm\n        ...\n        return Signals(signals_df)\n</code></pre>"},{"location":"guide/signal-architecture/#3-register-in-signal_registry-optional","title":"3. Register in signal_registry (optional)","text":"<p>Update <code>KNOWN_SIGNALS</code> and <code>DIRECTIONAL_SIGNAL_MAP</code> if the signal types should be discoverable or have directional mappings.</p>"},{"location":"guide/signal-architecture/#architecture-summary","title":"Architecture Summary","text":"<pre><code>flowchart TB\n    subgraph Signals [\"Signal Categories\"]\n        direction TB\n        PD[\"Price Direction&lt;br/&gt;rise, fall, flat\"]\n        PS[\"Price Structure&lt;br/&gt;local_max, local_min\"]\n        TM[\"Trend Momentum&lt;br/&gt;trend_start, trend_reversal\"]\n        VOL[\"Volatility&lt;br/&gt;high_volatility, low_volatility\"]\n        VL[\"Volume Liquidity&lt;br/&gt;abnormal_volume, illiquidity\"]\n        MW[\"Market Wide&lt;br/&gt;market_crash, synchronization\"]\n        AN[\"Anomaly&lt;br/&gt;extreme_positive_anomaly\"]\n    end\n\n    subgraph Pipeline [\"Processing Pipeline\"]\n        direction LR\n        DET[Detectors] --&gt; ROUTER[Signal Router]\n        ROUTER --&gt; |directional| ENTRY[Entry Rules]\n        ROUTER --&gt; |non-directional| SIZING[Position Sizing]\n        ROUTER --&gt; |anomaly| RISK[Risk Management]\n        ENTRY --&gt; EXEC[Strategy Execution]\n        SIZING --&gt; EXEC\n        RISK --&gt; EXEC\n    end\n\n    subgraph Training [\"Training Pipeline\"]\n        direction LR\n        LAB[Labelers] --&gt; TRAIN[Train Validator]\n        TRAIN --&gt; VAL[Validator]\n        VAL --&gt; DET\n    end\n\n    Signals --&gt; DET\n    Signals --&gt; LAB\n\n    style PD fill:#3b82f6,color:#fff\n    style PS fill:#8b5cf6,color:#fff\n    style TM fill:#06b6d4,color:#fff\n    style VOL fill:#f97316,color:#fff\n    style VL fill:#22c55e,color:#fff\n    style MW fill:#ef4444,color:#fff\n    style AN fill:#dc2626,color:#fff</code></pre>"},{"location":"guide/validators/","title":"Signal Validators (Meta-Labeling)","text":"<p>This guide covers training and using signal validators - the meta-labeling approach from Marcos Lopez de Prado for filtering and scoring trading signals.</p>"},{"location":"guide/validators/#overview","title":"Overview","text":"<p>A validator (meta-labeler) is a secondary model that predicts whether a primary signal will be successful. This two-stage approach separates:</p> <ol> <li>Detection (high recall) - find as many potential signals as possible</li> <li>Validation (high precision) - filter out false positives</li> </ol> <pre><code>Detector \u2192 candidate signals \u2192 Validator \u2192 scored signals \u2192 Strategy\n</code></pre>"},{"location":"guide/validators/#available-validators","title":"Available Validators","text":"Validator Model Use Case <code>LightGBMValidator</code> LGBMClassifier Default choice, fast training <code>XGBoostValidator</code> XGBClassifier Robust, good regularization <code>RandomForestValidator</code> RandomForestClassifier Interpretable, no hyperparams <code>LogisticRegressionValidator</code> LogisticRegression Fast, linear relationships <code>SVMValidator</code> SVC Small datasets <code>AutoSelectValidator</code> Auto Automatic model selection"},{"location":"guide/validators/#quick-start","title":"Quick Start","text":""},{"location":"guide/validators/#1-prepare-data","title":"1. Prepare Data","text":"<pre><code>import polars as pl\nfrom signalflow.detector import ExampleSmaCrossDetector\nfrom signalflow.target import FixedHorizonLabeler\nfrom signalflow.feature import FeaturePipeline\n\n# Load your data\nraw_data = ...  # RawData with OHLCV\n\n# Generate signals (what we want to validate)\ndetector = ExampleSmaCrossDetector(fast_period=20, slow_period=50)\nsignals = detector.run(raw_data.to_view())\n\n# Generate labels (ground truth for training)\nlabeler = FixedHorizonLabeler(horizon=60, threshold_pct=0.5)\nlabeled_df = labeler.compute(raw_data.to_polars(\"spot\"))\n\n# Compute features\npipeline = FeaturePipeline([...])\nfeatures_df = pipeline.run(raw_data.to_polars(\"spot\"))\n</code></pre>"},{"location":"guide/validators/#2-split-data","title":"2. Split Data","text":"<pre><code># Time-based split (no lookahead)\ntrain_end = features_df[\"timestamp\"].quantile(0.7)\nval_end = features_df[\"timestamp\"].quantile(0.85)\n\ntrain_df = features_df.filter(pl.col(\"timestamp\") &lt;= train_end)\nval_df = features_df.filter(\n    (pl.col(\"timestamp\") &gt; train_end) &amp; (pl.col(\"timestamp\") &lt;= val_end)\n)\ntest_df = features_df.filter(pl.col(\"timestamp\") &gt; val_end)\n\n# Filter to active signals only (not NONE/FLAT)\ntrain_df = train_df.filter(pl.col(\"signal_type\").is_in([\"rise\", \"fall\"]))\nval_df = val_df.filter(pl.col(\"signal_type\").is_in([\"rise\", \"fall\"]))\n</code></pre>"},{"location":"guide/validators/#3-train-validator","title":"3. Train Validator","text":"<pre><code>from signalflow.validator import LightGBMValidator\n\n# Define feature columns (exclude pair, timestamp, label)\nfeature_cols = [c for c in train_df.columns\n                if c not in [\"pair\", \"timestamp\", \"label\", \"signal_type\"]]\n\n# Create validator\nvalidator = LightGBMValidator(\n    n_estimators=200,\n    learning_rate=0.05,\n    max_depth=6,\n)\n\n# Train with early stopping\nvalidator.fit(\n    X_train=train_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_train=train_df.select(\"label\"),\n    X_val=val_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_val=val_df.select(\"label\"),\n)\n</code></pre>"},{"location":"guide/validators/#4-validate-signals","title":"4. Validate Signals","text":"<pre><code>from signalflow.core import Signals\n\n# Wrap test signals\ntest_signals = Signals(test_df.select([\"pair\", \"timestamp\", \"signal\", \"signal_type\"]))\n\n# Get probabilities\nvalidated = validator.validate_signals(\n    signals=test_signals,\n    features=test_df.select([\"pair\", \"timestamp\"] + feature_cols),\n)\n\n# Access results\ndf = validated.value\nprint(df.columns)\n# ['pair', 'timestamp', 'signal', 'signal_type', 'probability_0', 'probability_1']\n\n# Filter high-confidence signals\nconfident = df.filter(\n    (pl.col(\"signal_type\") == \"rise\") &amp;\n    (pl.col(\"probability_1\") &gt; 0.7)\n)\n</code></pre>"},{"location":"guide/validators/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>Each validator supports Optuna-based hyperparameter tuning:</p> <pre><code>from signalflow.validator import RandomForestValidator\n\nvalidator = RandomForestValidator()\n\n# Configure tuning\nvalidator.tune_params = {\n    \"n_trials\": 100,      # Number of Optuna trials\n    \"cv_folds\": 5,        # Cross-validation folds\n    \"timeout\": 1800,      # Max seconds\n}\nvalidator.tune_metric = \"roc_auc\"  # Optimization metric\n\n# Run tuning\nbest_params = validator.tune(\n    X_train=train_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_train=train_df.select(\"label\"),\n)\n\nprint(f\"Best params: {best_params}\")\n\n# Fit with best params (already set)\nvalidator.fit(\n    X_train=train_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_train=train_df.select(\"label\"),\n)\n</code></pre>"},{"location":"guide/validators/#tune-spaces","title":"Tune Spaces","text":"<p>Each validator has a predefined tuning space:</p> <p>LightGBMValidator: - <code>n_estimators</code>: 50-500 - <code>max_depth</code>: 3-12 - <code>learning_rate</code>: 0.01-0.3 (log scale) - <code>num_leaves</code>: 15-127 - <code>min_child_samples</code>: 5-100 - <code>subsample</code>: 0.6-1.0 - <code>colsample_bytree</code>: 0.6-1.0</p> <p>RandomForestValidator: - <code>n_estimators</code>: 50-300 - <code>max_depth</code>: 5-30 - <code>min_samples_split</code>: 2-20 - <code>min_samples_leaf</code>: 1-10</p>"},{"location":"guide/validators/#auto-model-selection","title":"Auto Model Selection","text":"<p>Use <code>AutoSelectValidator</code> to automatically find the best model:</p> <pre><code>from signalflow.validator import AutoSelectValidator\n\nvalidator = AutoSelectValidator(\n    auto_select_metric=\"roc_auc\",\n    auto_select_cv_folds=5,\n)\n\nvalidator.fit(X_train, y_train)\n\n# Check selected model\nprint(f\"Selected: {validator.selected_validator.__class__.__name__}\")\n# e.g., \"LightGBMValidator\"\n</code></pre> <p>The auto-selector tests LightGBM, XGBoost, Random Forest, and Logistic Regression using cross-validation and selects the best performing model.</p>"},{"location":"guide/validators/#early-stopping-boosting-models","title":"Early Stopping (Boosting Models)","text":"<p>LightGBM and XGBoost support early stopping to prevent overfitting:</p> <pre><code>from signalflow.validator import LightGBMValidator\n\nvalidator = LightGBMValidator(\n    n_estimators=1000,  # Max iterations\n    early_stopping_rounds=50,  # Stop if no improvement for 50 rounds\n)\n\n# Provide validation data for early stopping\nvalidator.fit(\n    X_train=train_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_train=train_df.select(\"label\"),\n    X_val=val_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_val=val_df.select(\"label\"),\n)\n\n# Check actual iterations used\nprint(f\"Best iteration: {validator.model.best_iteration_}\")\n</code></pre>"},{"location":"guide/validators/#save-and-load","title":"Save and Load","text":"<pre><code># Save trained validator\nvalidator.save(\"models/my_validator.pkl\")\n\n# Load later\nfrom signalflow.validator import LightGBMValidator\n\nloaded = LightGBMValidator.load(\"models/my_validator.pkl\")\nvalidated = loaded.validate_signals(signals, features)\n</code></pre>"},{"location":"guide/validators/#integration-with-strategy","title":"Integration with Strategy","text":"<p>Use validated signals in your trading strategy:</p> <pre><code>from signalflow.strategy.component.entry import SignalEntryRule\n\nclass ValidatedEntryRule(SignalEntryRule):\n    \"\"\"Entry rule that filters by validator probability.\"\"\"\n\n    min_probability: float = 0.6\n\n    def check_entries(self, signals, state, context):\n        # Filter to high-confidence signals\n        confident = signals.value.filter(\n            pl.col(\"probability_1\") &gt; self.min_probability\n        )\n\n        # Proceed with filtered signals\n        return super().check_entries(\n            Signals(confident), state, context\n        )\n</code></pre> <p>Or use with <code>SignalAggregator</code> in META_LABELING mode:</p> <pre><code>from signalflow.strategy.component.entry.aggregation import (\n    SignalAggregator,\n    VotingMode,\n)\n\naggregator = SignalAggregator(\n    voting_mode=VotingMode.META_LABELING,\n    probability_threshold=0.6,\n)\n\n# Combines detector signals with validator probabilities\ncombined = aggregator.aggregate([detector_signals, validated_signals])\n</code></pre>"},{"location":"guide/validators/#best-practices","title":"Best Practices","text":""},{"location":"guide/validators/#1-use-time-based-splits","title":"1. Use Time-Based Splits","text":"<p>Always use time-based train/val/test splits to avoid lookahead bias:</p> <pre><code># Good: Time-based split\ntrain = df.filter(pl.col(\"timestamp\") &lt; cutoff)\ntest = df.filter(pl.col(\"timestamp\") &gt;= cutoff)\n\n# Bad: Random split (causes lookahead)\ntrain, test = train_test_split(df, test_size=0.2)  # Don't do this!\n</code></pre>"},{"location":"guide/validators/#2-filter-to-active-signals","title":"2. Filter to Active Signals","text":"<p>Only train on signals that require a decision (not NONE/FLAT):</p> <pre><code>train_df = train_df.filter(pl.col(\"signal_type\").is_in([\"rise\", \"fall\"]))\n</code></pre>"},{"location":"guide/validators/#3-handle-class-imbalance","title":"3. Handle Class Imbalance","text":"<p>If your labels are imbalanced, consider:</p> <pre><code># LightGBM with class weights\nvalidator = LightGBMValidator()\nvalidator.model_params[\"class_weight\"] = \"balanced\"\n</code></pre>"},{"location":"guide/validators/#4-feature-engineering","title":"4. Feature Engineering","text":"<p>Good features for meta-labeling: - Volatility metrics (ATR, realized vol) - Volume indicators - Market regime features - Signal confidence from detector - Time-of-day features</p>"},{"location":"guide/validators/#5-monitor-overfitting","title":"5. Monitor Overfitting","text":"<p>Use early stopping and check validation metrics:</p> <pre><code>validator.fit(X_train, y_train, X_val, y_val)\n\n# Check train vs val performance\nfrom sklearn.metrics import roc_auc_score\n\ntrain_preds = validator.model.predict_proba(X_train_np)[:, 1]\nval_preds = validator.model.predict_proba(X_val_np)[:, 1]\n\nprint(f\"Train AUC: {roc_auc_score(y_train_np, train_preds):.3f}\")\nprint(f\"Val AUC: {roc_auc_score(y_val_np, val_preds):.3f}\")\n</code></pre>"},{"location":"guide/validators/#see-also","title":"See Also","text":"<ul> <li>API Reference: Detailed class documentation</li> <li>Signal Architecture: Meta-labeling theory</li> <li>Custom Detectors: Building primary models</li> </ul>"},{"location":"notebooks/","title":"Tutorials","text":"<p>Interactive Jupyter notebooks demonstrating SignalFlow capabilities.</p>"},{"location":"notebooks/#adding-notebooks","title":"Adding Notebooks","text":"<p>To add a notebook to documentation:</p> <ol> <li>Place your <code>.ipynb</code> file in <code>docs/notebooks/</code></li> <li>Add it to <code>mkdocs.yml</code> nav:</li> </ol> <pre><code>nav:\n  - Tutorials:\n    - notebooks/index.md\n    - My Tutorial: notebooks/my_tutorial.ipynb\n</code></pre> <ol> <li>Run <code>mkdocs serve</code> to preview</li> </ol> <p>All cell outputs (charts, tables, images) will be rendered automatically.</p>"},{"location":"notebooks/#running-locally","title":"Running Locally","text":"<pre><code># Install dependencies\npip install -e \".[dev]\"\n\n# Launch Jupyter\njupyter lab\n</code></pre>"}]}