{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to SignalFlow-trading 0.2.0","text":"<p>SignalFlow is a high-performance Python framework for algorithmic trading and quantitative finance. Built on a signal processing pipeline architecture, it transforms market data into validated trading signals and executable strategies.</p> <ul> <li> <p> High Performance</p> <p>Powered by Polars for blazing-fast processing of large datasets (100+ trading pairs, 500k+ candles)</p> </li> <li> <p> Modular Design</p> <p>Component registry system with pluggable detectors, validators, features, and strategies</p> </li> <li> <p> Production Ready</p> <p>Seamless transition from research to production with unified backtesting and live trading interfaces</p> </li> <li> <p> ML-Powered</p> <p>Built-in support for scikit-learn, XGBoost, LightGBM, and PyTorch-based signal validation</p> </li> </ul>"},{"location":"#the-signal-processing-pipeline","title":"The Signal Processing Pipeline","text":"<p>SignalFlow organizes algorithmic trading into four distinct stages:</p> <pre><code>flowchart LR\n    A[Market Data] --&gt; B[Signal Detection]\n    B --&gt; C[Signal Validation]\n    C --&gt; D[Strategy Execution]\n\n    style A fill:#2563eb,stroke:#3b82f6,stroke-width:2px,color:#fff\n    style B fill:#ea580c,stroke:#f97316,stroke-width:2px,color:#fff\n    style C fill:#16a34a,stroke:#22c55e,stroke-width:2px,color:#fff\n    style D fill:#dc2626,stroke:#ef4444,stroke-width:2px,color:#fff</code></pre>"},{"location":"#1-data-features","title":"1. Data &amp; Features","text":"<p>Load and process market data with efficient storage and feature engineering:</p> <ul> <li>Flexible data loaders (Binance Spot, Futures, custom sources)</li> <li>DuckDB and Parquet storage backends</li> <li>Technical indicators via pandas-ta and custom Polars extractors</li> </ul>"},{"location":"#2-signal-detection","title":"2. Signal Detection","text":"<p>Identify potential trading opportunities from market patterns:</p> <ul> <li>Classical algorithms (SMA crossover, MACD, RSI thresholds)</li> <li>Pattern recognition (candlestick patterns, chart formations)</li> <li>Neural network outputs (CNN, LSTM, Transformer predictions)</li> </ul>"},{"location":"#3-signal-validation-meta-labeling","title":"3. Signal Validation (Meta-Labeling)","text":"<p>Filter signals using machine learning to predict success probability:</p> <ul> <li>Implements Lopez de Prado's meta-labeling methodology</li> <li>Support for scikit-learn, XGBoost, LightGBM classifiers</li> <li>Deep learning validators via PyTorch Lightning (signalflow-nn)</li> </ul>"},{"location":"#4-strategy-execution","title":"4. Strategy Execution","text":"<p>Convert validated signals into trades with risk management:</p> <ul> <li>Entry/exit rules with take-profit and stop-loss</li> <li>Position sizing and risk controls</li> <li>Unified interface for backtesting and live trading</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from signalflow.core import RawDataView\nfrom signalflow.detector import SmaCrossSignalDetector\nfrom signalflow.validator import SklearnSignalValidator\nfrom signalflow.strategy import SimpleStrategy\n\n# Load data\ndata = RawDataView.load_from_duckdb(\"market_data.duckdb\")\n\n# Detect signals\ndetector = SmaCrossSignalDetector(fast_period=20, slow_period=50)\nsignals = detector.run(data)\n\n# Validate with ML\nvalidator = SklearnSignalValidator(model_type=\"lightgbm\")\nvalidator.fit(X_train, y_train)\nvalidated_signals = validator.validate_signals(signals, features)\n\n# Execute strategy\nstrategy = SimpleStrategy(\n    initial_capital=10000,\n    take_profit=0.02,\n    stop_loss=0.01\n)\nportfolio = strategy.run(validated_signals, data)\n\n# Analyze results\nprint(portfolio.metrics())\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#polars-first-performance","title":"Polars-First Performance","text":"<p>Core data processing uses Polars for extreme efficiency on large datasets, with seamless Pandas compatibility for prototyping.</p>"},{"location":"#component-registry","title":"Component Registry","text":"<p>All components (detectors, validators, features) are registered via <code>@sf_component</code> decorator for easy customization:</p> <pre><code>from signalflow.core import sf_component, SignalDetector\n\n@sf_component(name=\"my_detector\")\nclass CustomDetector(SignalDetector):\n    def detect(self, data):\n        # Your logic here\n        return signals\n</code></pre>"},{"location":"#advanced-labeling","title":"Advanced Labeling","text":"<p>Built-in support for sophisticated labeling strategies:</p> <ul> <li>Triple Barrier Method: Combines take-profit, stop-loss, and time barriers</li> <li>Fixed Horizon: Label signals based on future returns</li> <li>Numba-accelerated for performance (45s \u2192 0.3s on large datasets)</li> </ul>"},{"location":"#kedro-integration","title":"Kedro Integration","text":"<p>Full compatibility with Kedro for MLOps pipelines, experiment tracking, and production deployment.</p>"},{"location":"#technology-stack","title":"Technology Stack","text":"<p>=== \"Data Processing\"     - Polars - High-performance DataFrames     - Pandas - Legacy compatibility &amp; prototyping     - DuckDB - Embedded analytics database     - NumPy - Numerical computing</p> <p>=== \"Machine Learning\"     - scikit-learn - Classical ML models     - XGBoost - Gradient boosting     - LightGBM - Fast gradient boosting     - PyTorch - Deep learning framework     - Lightning - PyTorch training framework     - Optuna - Hyperparameter optimization</p> <p>=== \"Trading Tools\"     - pandas-ta - Technical analysis indicators     - Numba - JIT compilation for speed     - Plotly - Interactive visualizations</p> <p>=== \"Infrastructure\"     - Kedro - Pipeline orchestration     - MLflow - Experiment tracking (planned)     - DuckDB - Local data storage</p>"},{"location":"#signalflow-ecosystem","title":"SignalFlow Ecosystem","text":"<p>SignalFlow is growing into a multi-repository ecosystem:</p>"},{"location":"#signalflow-core","title":"signalflow (Core)","text":"<p>The main library with foundational components:</p> <ul> <li>Core data containers and abstractions</li> <li>Binance connectors and data loaders</li> <li>Backtesting infrastructure</li> <li>Basic detectors and validators</li> <li>Strategy execution framework</li> </ul>"},{"location":"#signalflow-nn-neural-networks","title":"signalflow-nn (Neural Networks)","text":"<p>Specialized repository for deep learning:</p> <ul> <li>PyTorch Lightning-based validators</li> <li>Time series architectures (LSTM, GRU, Transformers)</li> <li>VAE and autoencoder implementations</li> <li>Temporal feature extractors</li> </ul>"},{"location":"#signalflow-kedro-mlops-material-pipeline","title":"signalflow-kedro (MLOps) :material-pipeline:","text":"<p>Kedro project template for production workflows:</p> <ul> <li>End-to-end ML pipelines</li> <li>Model training and versioning</li> <li>Experiment tracking integration</li> <li>Custom component development environment</li> </ul>"},{"location":"#philosophy","title":"Philosophy","text":"<p>Minimize time from successful experiment to production deployment.</p> <p>SignalFlow bridges the research-production gap by:</p> <ol> <li>Unified API: Same code works for backtesting and live trading</li> <li>Performance: Polars-optimized for production-scale data</li> <li>Modularity: Swap components without rewriting strategies</li> <li>Testability: Every stage independently analyzable</li> </ol>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to build your first trading strategy?</p> <ul> <li> <p> Installation</p> <p>Install SignalFlow and set up your development environment</p> </li> <li> <p> Quick Start</p> <p>Build your first signal detector and backtest a strategy</p> </li> <li> <p> User Guide</p> <p>Learn core concepts and advanced features</p> </li> <li> <p> API Reference</p> <p>Detailed documentation for all classes and methods</p> </li> </ul>"},{"location":"#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub: github.com/pathway2nothing/signalflow-trading</li> <li>Issues: Report bugs or request features</li> <li>Email: pathway2nothing@gmail.com</li> </ul>"},{"location":"#license","title":"License","text":"<p>SignalFlow is open source software released under the MIT License.</p> <p>Disclaimer</p> <p>SignalFlow is provided for research purposes. Trading financial instruments carries risk. Past performance does not guarantee future results. Use at your own risk.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>We welcome contributions to SignalFlow!</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Submit a pull request</li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/pathway2nothing/signalflow-trading.git\ncd signalflow-trading\npip install -e \".[dev,docs,nn]\"\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Use Black for formatting</li> <li>Use Ruff for linting</li> <li>Add type hints</li> <li>Write Google-style docstrings</li> </ul>"},{"location":"contributing/#testing","title":"Testing","text":"<pre><code>pytest\npytest --cov=signalflow\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<pre><code>mkdocs serve\n</code></pre>"},{"location":"contributing/#contact","title":"Contact","text":"<ul> <li>Email: pathway2nothing@gmail.com</li> <li>Issues: GitHub Issues</li> </ul>"},{"location":"quickstart/","title":"Quick Start Guide","text":"<p>This guide walks you through building your first algorithmic trading strategy with SignalFlow in under 10 minutes.</p>"},{"location":"quickstart/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>Python 3.12 or higher</li> <li>Basic understanding of pandas/Polars DataFrames</li> <li>Familiarity with trading concepts (OHLCV data, signals)</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":"<p>Install SignalFlow via pip:</p> <pre><code>pip install signalflow-trading\n</code></pre> <p>For development installation with all extras:</p> <pre><code>pip install signalflow-trading[nn]\n</code></pre> <p>This includes:</p> <ul> <li><code>dev</code>: Development tools (pytest, black, mypy)</li> <li><code>nn</code>: Neural network support (PyTorch, Lightning)</li> </ul>"},{"location":"quickstart/#your-first-strategy-sma-crossover","title":"Your First Strategy: SMA Crossover","text":"<p>We'll build a classic Simple Moving Average (SMA) crossover strategy step by step.</p>"},{"location":"quickstart/#step-1-load-market-data","title":"Step 1: Load Market Data","text":"<p>First, download historical data from Binance:</p> <pre><code>import asyncio\nfrom pathlib import Path\nfrom datetime import datetime\nfrom signalflow.data import BinanceSpotLoader\n\n# Initialize loader\nloader = BinanceSpotLoader(\n    store_path=Path(\"data/binance_spot.duckdb\"),\n    timeframe=\"1m\"  # 1-minute candles\n)\n\n# Download 30 days of data for BTC and ETH\nasync def download_data():\n    await loader.download(\n        pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n        days=30,\n        fill_gaps=True\n    )\n\n# Run download\nasyncio.run(download_data())\n</code></pre> <p>Data Source</p> <p>SignalFlow currently supports Binance Spot data. Support for Binance Futures and other exchanges is planned for future releases.</p>"},{"location":"quickstart/#step-2-load-data-into-framework","title":"Step 2: Load Data into Framework","text":"<p>Convert stored data to SignalFlow's <code>RawData</code> format:</p> <pre><code>from signalflow.data import RawDataFactory\n\n# Load data from DuckDB\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/binance_spot.duckdb\"),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 11, 1),\n    end=datetime(2024, 12, 1),\n    data_types=[\"spot\"]\n)\n\n# Inspect loaded data\nprint(f\"Loaded {len(raw_data['spot'])} candles\")\nprint(f\"Pairs: {raw_data.pairs}\")\nprint(f\"Date range: {raw_data.datetime_start} to {raw_data.datetime_end}\")\n</code></pre> <p>??? info \"RawData Container\"     <code>RawData</code> is an immutable container that stores market data with metadata. It provides unified access to different data types (spot, futures, LOB) and validates data integrity.</p>"},{"location":"quickstart/#step-3-detect-signals","title":"Step 3: Detect Signals","text":"<p>Use the built-in SMA crossover detector:</p> <pre><code>from signalflow.detector import SmaCrossSignalDetector\n\n# Create detector (20/50 SMA crossover)\ndetector = SmaCrossSignalDetector(\n    fast_period=20,\n    slow_period=50,\n    price_col=\"close\"\n)\n\n# Detect signals\nsignals = detector.run(raw_data)\n\n# Inspect signals\nsignals_df = signals.value\nprint(f\"Detected {len(signals_df)} signals\")\nprint(signals_df.filter(pl.col(\"signal_type\") != \"none\").head())\n</code></pre> <p>Signal Types:</p> <ul> <li><code>RISE</code>: Fast SMA crosses above slow SMA (bullish signal)</li> <li><code>FALL</code>: Fast SMA crosses below slow SMA (bearish signal)</li> <li><code>NONE</code>: No crossover detected</li> </ul>"},{"location":"quickstart/#step-4-label-signals-for-ml","title":"Step 4: Label Signals for ML","text":"<p>Add labels to signals using the Triple Barrier Method:</p> <pre><code>from signalflow.labeler import TripleBarrierLabeler\n\n# Create labeler\nlabeler = TripleBarrierLabeler(\n    take_profit=0.02,   # 2% profit target\n    stop_loss=0.01,     # 1% stop loss\n    max_holding=1440    # 24 hours (1440 minutes)\n)\n\n# Label signals\nlabeled_signals = labeler.label(signals, raw_data)\n\n# Check label distribution\nlabel_counts = (\n    labeled_signals.value\n    .group_by(\"label\")\n    .count()\n)\nprint(label_counts)\n</code></pre> <p>??? note \"Triple Barrier Method\"     This labeling technique from Lopez de Prado's \"Advances in Financial Machine Learning\" assigns labels based on which barrier is hit first:</p> <pre><code>- **Take Profit** \u2192 Label: RISE (profitable trade)\n- **Stop Loss** \u2192 Label: FALL (losing trade)\n- **Time Limit** \u2192 Label: NONE (inconclusive)\n</code></pre>"},{"location":"quickstart/#step-5-extract-features","title":"Step 5: Extract Features","text":"<p>Create features for machine learning validation:</p> <pre><code>from signalflow.feature import FeatureSet\nfrom signalflow.feature import (\n    RsiExtractor,\n    BollingerBandsExtractor,\n    AtrExtractor\n)\n\n# Define feature set\nfeature_set = FeatureSet([\n    RsiExtractor(window=14),\n    BollingerBandsExtractor(window=20, num_std=2),\n    AtrExtractor(window=14)\n])\n\n# Extract features\nfeatures = feature_set.extract(raw_data, data_type=\"spot\")\n\n# Check extracted features\nprint(features.columns)\n# Output: ['pair', 'timestamp', 'rsi_14', 'bb_upper', 'bb_middle', 'bb_lower', 'atr_14']\n</code></pre>"},{"location":"quickstart/#step-6-train-signal-validator-optional","title":"Step 6: Train Signal Validator (Optional)","text":"<p>Filter and validate signals using machine learning:</p> <pre><code>import polars as pl\nfrom signalflow.validator import SklearnSignalValidator\n\n# Prepare training data (filter to active signals only)\nactive_signals = labeled_signals.value.filter(\n    pl.col(\"signal_type\") != \"none\"\n)\n\n# Join features\ntrain_data = active_signals.join(\n    features,\n    on=[\"pair\", \"timestamp\"],\n    how=\"inner\"\n)\n\n# Split train/test (80/20)\nsplit_idx = int(len(train_data) * 0.8)\ntrain_df = train_data[:split_idx]\ntest_df = train_data[split_idx:]\n\n# Define feature columns\nfeature_cols = [\"rsi_14\", \"bb_upper\", \"bb_middle\", \"bb_lower\", \"atr_14\"]\n\n# Create validator with LightGBM\nvalidator = SklearnSignalValidator(\n    model_type=\"lightgbm\",\n    model_params={\n        \"n_estimators\": 100,\n        \"learning_rate\": 0.05,\n        \"max_depth\": 5\n    }\n)\n\n# Train\nvalidator.fit(\n    X_train=train_df.select([\"pair\", \"timestamp\"] + feature_cols),\n    y_train=train_df.select(\"label\")\n)\n\n# Validate test signals\ntest_signals = Signals(test_df.select([\"pair\", \"timestamp\", \"signal_type\", \"signal\"]))\nvalidated_signals = validator.validate_signals(test_signals, test_df.select([\"pair\", \"timestamp\"] + feature_cols))\n\n# Filter high-confidence signals\nhigh_confidence = validated_signals.value.filter(\n    pl.col(\"probability_rise\") &gt; 0.7  # Only signals with &gt;70% success probability\n)\nprint(f\"High confidence signals: {len(high_confidence)}\")\n</code></pre> <p>Model Selection</p> <p><code>SklearnSignalValidator</code> supports multiple models:</p> <ul> <li><code>lightgbm</code> (recommended for speed)</li> <li><code>xgboost</code> (good accuracy)</li> <li><code>random_forest</code> (sklearn Random Forest)</li> <li><code>logistic_regression</code> (linear baseline)</li> <li><code>auto</code> (automatic model selection via cross-validation)</li> </ul>"},{"location":"quickstart/#step-7-backtest-strategy","title":"Step 7: Backtest Strategy","text":"<p>Simulate trading with your signals:</p> <pre><code>from signalflow.strategy import BacktestRunner, VirtualSpotExecutor\nfrom signalflow.strategy import TakeProfitStopLossExit\n\n# Configure execution\nexecutor = VirtualSpotExecutor()\n\n# Configure exit rules\nexit_rule = TakeProfitStopLossExit(\n    take_profit=0.02,  # 2% profit target\n    stop_loss=0.01     # 1% stop loss\n)\n\n# Create backtest runner\nrunner = BacktestRunner(\n    strategy_id=\"sma_cross_backtest\",\n    initial_capital=10000.0,\n    executor=executor,\n    exit_rule=exit_rule,\n    data_key=\"spot\"\n)\n\n# Run backtest\nfinal_state = runner.run(\n    raw_data=raw_data,\n    signals=signals  # Or use validated_signals for filtered backtest\n)\n\n# Print results\nprint(f\"Final Portfolio Value: ${final_state.portfolio.total_value():.2f}\")\nprint(f\"Total Trades: {len(final_state.trades)}\")\nprint(f\"Open Positions: {len(final_state.portfolio.open_positions())}\")\n\n# Compute metrics\nfrom signalflow.strategy.metrics import PortfolioMetrics\n\nmetrics = PortfolioMetrics.compute(final_state)\nprint(f\"\\nBacktest Metrics:\")\nprint(f\"Total Return: {metrics['total_return']:.2%}\")\nprint(f\"Sharpe Ratio: {metrics['sharpe_ratio']:.2f}\")\nprint(f\"Max Drawdown: {metrics['max_drawdown']:.2%}\")\nprint(f\"Win Rate: {metrics['win_rate']:.2%}\")\n</code></pre>"},{"location":"quickstart/#step-8-analyze-results","title":"Step 8: Analyze Results","text":"<p>Visualize backtest performance:</p> <pre><code>import plotly.graph_objects as go\nfrom signalflow.utils.visualization import plot_equity_curve\n\n# Plot equity curve\nfig = plot_equity_curve(final_state)\nfig.show()\n\n# Plot signals on price chart\nfrom signalflow.utils.visualization import plot_signals_overlay\n\nfig = plot_signals_overlay(\n    raw_data=raw_data,\n    signals=signals,\n    pair=\"BTCUSDT\",\n    start=datetime(2024, 11, 1),\n    end=datetime(2024, 11, 7)\n)\nfig.show()\n</code></pre>"},{"location":"quickstart/#complete-example","title":"Complete Example","text":"<p>Here's the full workflow in one script:</p> <pre><code>import asyncio\nimport polars as pl\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom signalflow.data import BinanceSpotLoader, RawDataFactory\nfrom signalflow.detector import SmaCrossSignalDetector\nfrom signalflow.target import TripleBarrierLabeler\nfrom signalflow.feature import FeatureSet, RsiExtractor, BollingerBandsExtractor\nfrom signalflow.validator import SklearnSignalValidator\nfrom signalflow.strategy import BacktestRunner, VirtualSpotExecutor, TakeProfitStopLossExit\nfrom signalflow.core import Signals\n\n# 1. Download data\nasync def main():\n    loader = BinanceSpotLoader()\n    await loader.download(pairs=[\"BTCUSDT\"], days=30)\n\nasyncio.run(main())\n\n# 2. Load data\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"raw_data.duckdb\"),\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2024, 11, 1),\n    end=datetime(2024, 12, 1)\n)\n\n# 3. Detect signals\ndetector = SmaCrossSignalDetector(fast_period=20, slow_period=50)\nsignals = detector.run(raw_data)\n\n# 4. Label for ML (optional)\nlabeler = TripleBarrierLabeler(take_profit=0.02, stop_loss=0.01)\nlabeled_signals = labeler.label(signals, raw_data)\n\n# 5. Extract features (optional)\nfeatures = FeatureSet([\n    RsiExtractor(window=14),\n    BollingerBandsExtractor(window=20)\n]).extract(raw_data, data_type=\"spot\")\n\n# 6. Train validator (optional)\ntrain_data = labeled_signals.value.filter(\n    pl.col(\"signal_type\") != \"none\"\n).join(features, on=[\"pair\", \"timestamp\"])\n\nvalidator = SklearnSignalValidator(model_type=\"lightgbm\")\nvalidator.fit(\n    X_train=train_data.select([\"pair\", \"timestamp\", \"rsi_14\", \"bb_upper\", \"bb_middle\", \"bb_lower\"]),\n    y_train=train_data.select(\"label\")\n)\n\n# 7. Backtest\nrunner = BacktestRunner(\n    strategy_id=\"quickstart\",\n    initial_capital=10000,\n    executor=VirtualSpotExecutor(),\n    exit_rule=TakeProfitStopLossExit(take_profit=0.02, stop_loss=0.01)\n)\n\nfinal_state = runner.run(raw_data=raw_data, signals=signals)\nprint(f\"Final Value: ${final_state.portfolio.total_value():.2f}\")\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<p>Congratulations! You've built your first trading strategy with SignalFlow. Here's what to explore next:</p> <ul> <li> <p> User Guide</p> <p>Deep dive into core concepts: signals, features, validators, and strategies</p> </li> <li> <p> API Reference</p> <p>Complete documentation for all classes and methods</p> </li> <li> <p> Examples</p> <p>More advanced examples: ML integration, custom components, live trading</p> </li> <li> <p> Configuration</p> <p>Customize SignalFlow behavior and component parameters</p> </li> </ul>"},{"location":"quickstart/#common-patterns","title":"Common Patterns","text":""},{"location":"quickstart/#using-the-component-registry","title":"Using the Component Registry","text":"<p>Register and discover components dynamically:</p> <pre><code>from signalflow.core import sf_component, default_registry, SfComponentType\n\n# Register custom detector\n@sf_component(name=\"my_detector\")\nclass MyCustomDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n\n    def detect(self, features, context=None):\n        # Your logic\n        return Signals(...)\n\n# List available detectors\ndetectors = default_registry.list(SfComponentType.DETECTOR)\nprint(f\"Available: {detectors}\")\n\n# Create from registry\ndetector = default_registry.create(\n    SfComponentType.DETECTOR,\n    \"my_detector\",\n    custom_param=42\n)\n</code></pre>"},{"location":"quickstart/#working-with-polars-dataframes","title":"Working with Polars DataFrames","text":"<p>SignalFlow is Polars-first for performance:</p> <pre><code>import polars as pl\n\n# Filter signals\nactive_signals = signals.value.filter(\n    pl.col(\"signal_type\") != \"none\"\n)\n\n# Group by pair\nsignals_by_pair = signals.value.group_by(\"pair\").agg([\n    pl.count().alias(\"signal_count\"),\n    pl.col(\"signal_type\").mode().alias(\"most_common_signal\")\n])\n\n# Join with features\nenriched = signals.value.join(\n    features,\n    on=[\"pair\", \"timestamp\"],\n    how=\"left\"\n)\n</code></pre>"},{"location":"quickstart/#pandas-compatibility","title":"Pandas Compatibility","text":"<p>Convert to Pandas when needed:</p> <pre><code># Convert Polars to Pandas\npandas_df = signals.value.to_pandas()\n\n# Convert Pandas to Polars\npolars_df = pl.from_pandas(pandas_df)\n</code></pre>"},{"location":"quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"quickstart/#no-signals-detected","title":"No signals detected?","text":"<p>Check your detector parameters and data quality:</p> <pre><code># Verify data\nprint(raw_data['spot'].describe())\n\n# Check for NaN in price columns\nprint(raw_data['spot'].select([\n    pl.col(\"close\").is_null().sum()\n]))\n\n# Lower detector thresholds\ndetector = SmaCrossSignalDetector(\n    fast_period=10,  # Lower values = more signals\n    slow_period=20\n)\n</code></pre>"},{"location":"quickstart/#slow-performance","title":"Slow performance?","text":"<p>SignalFlow uses Polars for speed, but check:</p> <pre><code># Reduce data size\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    pairs=[\"BTCUSDT\"],  # Single pair\n    start=datetime(2024, 12, 1),\n    end=datetime(2024, 12, 7)  # One week\n)\n\n# Use numba-accelerated labelers\nlabeler = TripleBarrierLabeler(\n    use_numba=True  # Enable JIT compilation\n)\n</code></pre>"},{"location":"quickstart/#import-errors","title":"Import errors?","text":"<p>Ensure all dependencies are installed:</p> <pre><code># Full installation\npip install signalflow-trading[dev,nn]\n\n# Or install missing packages individually\npip install lightgbm xgboost plotly\n</code></pre>"},{"location":"quickstart/#getting-help","title":"Getting Help","text":"<ul> <li>Email: pathway2nothing@gmail.com</li> <li>GitHub Issues: Report bugs or request features</li> <li>Documentation: Explore the User Guide and API Reference</li> </ul> <p>You're Ready!</p> <p>You now have a working algorithmic trading strategy. Experiment with different detectors, validators, and parameters to improve performance. Remember: backtest thoroughly before live trading!</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for SignalFlow components.</p>"},{"location":"api/#modules","title":"Modules","text":"<ul> <li>Core - Core containers and registries</li> <li>Data - Data loading and storage</li> <li>Detector - Signal detection</li> <li>Feature - Feature extraction</li> <li>Labeler - Signal labeling</li> <li>Validator - Signal validation</li> <li>Strategy - Strategy execution</li> </ul>"},{"location":"api/core/","title":"Core Module","text":""},{"location":"api/core/#signalflow.core","title":"signalflow.core","text":""},{"location":"api/core/#signalflow.core.RawData","title":"RawData  <code>dataclass</code>","text":"<pre><code>RawData(datetime_start: datetime, datetime_end: datetime, pairs: list[str] = list(), data: dict[str, DataFrame] = dict())\n</code></pre> <p>Immutable container for raw market data.</p> <p>Acts as a unified in-memory bundle for multiple raw datasets (e.g. spot prices, funding, trades, orderbook, signals).</p> Design principles <ul> <li>Canonical storage is dataset-based (dictionary by name)</li> <li>Datasets accessed via string keys (e.g. raw_data[\"spot\"])</li> <li>No business logic or transformations</li> <li>Immutability ensures reproducibility in pipelines</li> </ul> <p>Attributes:</p> Name Type Description <code>datetime_start</code> <code>datetime</code> <p>Start datetime of the data snapshot.</p> <code>datetime_end</code> <code>datetime</code> <p>End datetime of the data snapshot.</p> <code>pairs</code> <code>list[str]</code> <p>List of trading pairs in the snapshot.</p> <code>data</code> <code>dict[str, DataFrame]</code> <p>Dictionary of datasets keyed by name.</p> Example <pre><code>from signalflow.core import RawData\nimport polars as pl\nfrom datetime import datetime\n\n# Create RawData with spot data\nraw_data = RawData(\n    datetime_start=datetime(2024, 1, 1),\n    datetime_end=datetime(2024, 12, 31),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    data={\n        \"spot\": spot_dataframe,\n        \"signals\": signals_dataframe,\n    }\n)\n\n# Access datasets\nspot_df = raw_data[\"spot\"]\nsignals_df = raw_data.get(\"signals\")\n\n# Check if dataset exists\nif \"spot\" in raw_data:\n    print(\"Spot data available\")\n</code></pre> Note <p>Dataset schemas are defined by convention, not enforced. Views (pandas/polars) should be handled by RawDataView wrapper.</p>"},{"location":"api/core/#signalflow.core.RawData.__contains__","title":"__contains__","text":"<pre><code>__contains__(key: str) -&gt; bool\n</code></pre> <p>Check if dataset exists.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Dataset name to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if dataset exists, False otherwise.</p> Example <pre><code>if \"spot\" in raw_data:\n    process_spot_data(raw_data[\"spot\"])\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def __contains__(self, key: str) -&gt; bool:\n    \"\"\"Check if dataset exists.\n\n    Args:\n        key (str): Dataset name to check.\n\n    Returns:\n        bool: True if dataset exists, False otherwise.\n\n    Example:\n        ```python\n        if \"spot\" in raw_data:\n            process_spot_data(raw_data[\"spot\"])\n        ```\n    \"\"\"\n    return key in self.data\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.__getitem__","title":"__getitem__","text":"<pre><code>__getitem__(key: str) -&gt; pl.DataFrame\n</code></pre> <p>Dictionary-style access to datasets.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Dataset name.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Dataset as Polars DataFrame.</p> Example <pre><code>spot_df = raw_data[\"spot\"]\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def __getitem__(self, key: str) -&gt; pl.DataFrame:\n    \"\"\"Dictionary-style access to datasets.\n\n    Args:\n        key (str): Dataset name.\n\n    Returns:\n        pl.DataFrame: Dataset as Polars DataFrame.\n\n    Example:\n        ```python\n        spot_df = raw_data[\"spot\"]\n        ```\n    \"\"\"\n    return self.get(key)\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.get","title":"get","text":"<pre><code>get(key: str) -&gt; pl.DataFrame\n</code></pre> <p>Get dataset by key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Dataset name (e.g. \"spot\", \"signals\").</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Polars DataFrame if exists, empty DataFrame otherwise.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If dataset exists but is not a Polars DataFrame.</p> Example <pre><code>spot_df = raw_data.get(\"spot\")\n\n# Returns empty DataFrame if key doesn't exist\nmissing_df = raw_data.get(\"nonexistent\")\nassert missing_df.is_empty()\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def get(self, key: str) -&gt; pl.DataFrame:\n    \"\"\"Get dataset by key.\n\n    Args:\n        key (str): Dataset name (e.g. \"spot\", \"signals\").\n\n    Returns:\n        pl.DataFrame: Polars DataFrame if exists, empty DataFrame otherwise.\n\n    Raises:\n        TypeError: If dataset exists but is not a Polars DataFrame.\n\n    Example:\n        ```python\n        spot_df = raw_data.get(\"spot\")\n\n        # Returns empty DataFrame if key doesn't exist\n        missing_df = raw_data.get(\"nonexistent\")\n        assert missing_df.is_empty()\n        ```\n    \"\"\"\n    obj = self.data.get(key)\n    if obj is None:\n        return pl.DataFrame()\n    if not isinstance(obj, pl.DataFrame):\n        raise TypeError(\n            f\"Dataset '{key}' is not a polars.DataFrame: {type(obj)}\"\n        )\n    return obj\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.items","title":"items","text":"<pre><code>items()\n</code></pre> <p>Return (key, dataset) pairs.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <p>Iterator over (key, DataFrame) tuples.</p> Example <pre><code>for name, df in raw_data.items():\n    print(f\"{name}: {df.shape}\")\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def items(self):\n    \"\"\"Return (key, dataset) pairs.\n\n    Returns:\n        Iterator: Iterator over (key, DataFrame) tuples.\n\n    Example:\n        ```python\n        for name, df in raw_data.items():\n            print(f\"{name}: {df.shape}\")\n        ```\n    \"\"\"\n    return self.data.items()\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.keys","title":"keys","text":"<pre><code>keys() -&gt; Iterator[str]\n</code></pre> <p>Return available dataset keys.</p> <p>Returns:</p> Type Description <code>Iterator[str]</code> <p>Iterator[str]: Iterator over dataset names.</p> Example <pre><code>for key in raw_data.keys():\n    print(f\"Dataset: {key}\")\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def keys(self) -&gt; Iterator[str]:\n    \"\"\"Return available dataset keys.\n\n    Returns:\n        Iterator[str]: Iterator over dataset names.\n\n    Example:\n        ```python\n        for key in raw_data.keys():\n            print(f\"Dataset: {key}\")\n        ```\n    \"\"\"\n    return self.data.keys()\n</code></pre>"},{"location":"api/core/#signalflow.core.RawData.values","title":"values","text":"<pre><code>values()\n</code></pre> <p>Return dataset values.</p> <p>Returns:</p> Name Type Description <code>Iterator</code> <p>Iterator over DataFrames.</p> Example <pre><code>for df in raw_data.values():\n    print(df.columns)\n</code></pre> Source code in <code>src/signalflow/core/containers/raw_data.py</code> <pre><code>def values(self):\n    \"\"\"Return dataset values.\n\n    Returns:\n        Iterator: Iterator over DataFrames.\n\n    Example:\n        ```python\n        for df in raw_data.values():\n            print(df.columns)\n        ```\n    \"\"\"\n    return self.data.values()\n</code></pre>"},{"location":"api/core/#signalflow.core.Signals","title":"Signals  <code>dataclass</code>","text":"<pre><code>Signals(value: DataFrame)\n</code></pre> <p>Immutable container for trading signals.</p> <p>Canonical in-memory format is a Polars DataFrame with long schema.</p> Required columns <ul> <li>pair (str): Trading pair identifier</li> <li>timestamp (datetime): Signal timestamp</li> <li>signal_type (SignalType | int): Signal type (RISE, FALL, NONE)</li> <li>signal (int | float): Signal value</li> </ul> Optional columns <ul> <li>probability (float): Signal probability (required for merge logic)</li> </ul> <p>Attributes:</p> Name Type Description <code>value</code> <code>DataFrame</code> <p>Polars DataFrame containing signal data.</p> Example <pre><code>from signalflow.core import Signals, SignalType\nimport polars as pl\nfrom datetime import datetime\n\n# Create signals\nsignals_df = pl.DataFrame({\n    \"pair\": [\"BTCUSDT\", \"ETHUSDT\"],\n    \"timestamp\": [datetime.now(), datetime.now()],\n    \"signal_type\": [SignalType.RISE.value, SignalType.FALL.value],\n    \"signal\": [1, -1],\n    \"probability\": [0.8, 0.7]\n})\n\nsignals = Signals(signals_df)\n\n# Apply transformation\nfiltered = signals.apply(filter_transform)\n\n# Chain transformations\nprocessed = signals.pipe(\n    transform1,\n    transform2,\n    transform3\n)\n\n# Merge signals\ncombined = signals1 + signals2\n</code></pre> Note <p>All transformations return new Signals instance. No in-place mutation is allowed.</p>"},{"location":"api/core/#signalflow.core.Signals.__add__","title":"__add__","text":"<pre><code>__add__(other: 'Signals') -&gt; 'Signals'\n</code></pre> <p>Merge two Signals objects.</p> Merge rules <ol> <li>Key: (pair, timestamp)</li> <li>Signal type priority:</li> <li>SignalType.NONE has lowest priority</li> <li>Non-NONE always overrides NONE</li> <li>If both non-NONE, <code>other</code> wins</li> <li>SignalType.NONE normalized to probability = 0</li> <li>Merge is deterministic</li> </ol> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Signals</code> <p>Another Signals object to merge.</p> required <p>Returns:</p> Name Type Description <code>Signals</code> <code>'Signals'</code> <p>New merged Signals instance.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If other is not a Signals instance.</p> Example <pre><code># Detector 1 signals\nsignals1 = detector1.run(data)\n\n# Detector 2 signals\nsignals2 = detector2.run(data)\n\n# Merge with priority to signals2\nmerged = signals1 + signals2\n\n# NONE signals overridden by non-NONE\n# Non-NONE conflicts resolved by taking signals2\n</code></pre> Source code in <code>src/signalflow/core/containers/signals.py</code> <pre><code>def __add__(self, other: \"Signals\") -&gt; \"Signals\":\n    \"\"\"Merge two Signals objects.\n\n    Merge rules:\n        1. Key: (pair, timestamp)\n        2. Signal type priority:\n           - SignalType.NONE has lowest priority\n           - Non-NONE always overrides NONE\n           - If both non-NONE, `other` wins\n        3. SignalType.NONE normalized to probability = 0\n        4. Merge is deterministic\n\n    Args:\n        other (Signals): Another Signals object to merge.\n\n    Returns:\n        Signals: New merged Signals instance.\n\n    Raises:\n        TypeError: If other is not a Signals instance.\n\n    Example:\n        ```python\n        # Detector 1 signals\n        signals1 = detector1.run(data)\n\n        # Detector 2 signals\n        signals2 = detector2.run(data)\n\n        # Merge with priority to signals2\n        merged = signals1 + signals2\n\n        # NONE signals overridden by non-NONE\n        # Non-NONE conflicts resolved by taking signals2\n        ```\n    \"\"\"\n    if not isinstance(other, Signals):\n        return NotImplemented\n\n    a = self.value\n    b = other.value\n\n    all_cols = list(dict.fromkeys([*a.columns, *b.columns]))\n\n    def align(df: pl.DataFrame) -&gt; pl.DataFrame:\n        return (\n            df.with_columns(\n                [pl.lit(None).alias(c) for c in all_cols if c not in df.columns]\n            )\n            .select(all_cols)\n        )\n\n    a = align(a).with_columns(pl.lit(0).alias(\"_src\"))\n    b = align(b).with_columns(pl.lit(1).alias(\"_src\"))\n\n    merged = pl.concat([a, b], how=\"vertical\")\n\n    merged = merged.with_columns(\n        pl.when(pl.col(\"signal_type\") == SignalType.NONE.value)\n        .then(pl.lit(0))\n        .otherwise(pl.col(\"probability\"))\n        .alias(\"probability\")\n    )\n\n    merged = merged.with_columns(\n        pl.when(pl.col(\"signal_type\") == SignalType.NONE.value)\n        .then(pl.lit(0))\n        .otherwise(pl.lit(1))\n        .alias(\"_priority\")\n    )\n\n    merged = (\n        merged\n        .sort(\n            [\"pair\", \"timestamp\", \"_priority\", \"_src\"],\n            descending=[False, False, True, True],\n        )\n        .unique(\n            subset=[\"pair\", \"timestamp\"],\n            keep=\"first\",\n        )\n        .drop([\"_priority\", \"_src\"])\n        .sort([\"pair\", \"timestamp\"])\n    )\n\n    return Signals(merged)\n</code></pre>"},{"location":"api/core/#signalflow.core.Signals.apply","title":"apply","text":"<pre><code>apply(transform: SignalsTransform) -&gt; 'Signals'\n</code></pre> <p>Apply a single transformation to signals.</p> <p>Parameters:</p> Name Type Description Default <code>transform</code> <code>SignalsTransform</code> <p>Callable transformation implementing SignalsTransform protocol.</p> required <p>Returns:</p> Name Type Description <code>Signals</code> <code>'Signals'</code> <p>New Signals instance with transformed data.</p> Example <pre><code>from signalflow.core import Signals\nimport polars as pl\n\ndef filter_high_probability(df: pl.DataFrame) -&gt; pl.DataFrame:\n    return df.filter(pl.col(\"probability\") &gt; 0.7)\n\nfiltered = signals.apply(filter_high_probability)\n</code></pre> Source code in <code>src/signalflow/core/containers/signals.py</code> <pre><code>def apply(self, transform: SignalsTransform) -&gt; \"Signals\":\n    \"\"\"Apply a single transformation to signals.\n\n    Args:\n        transform (SignalsTransform): Callable transformation implementing\n            SignalsTransform protocol.\n\n    Returns:\n        Signals: New Signals instance with transformed data.\n\n    Example:\n        ```python\n        from signalflow.core import Signals\n        import polars as pl\n\n        def filter_high_probability(df: pl.DataFrame) -&gt; pl.DataFrame:\n            return df.filter(pl.col(\"probability\") &gt; 0.7)\n\n        filtered = signals.apply(filter_high_probability)\n        ```\n    \"\"\"\n    out = transform(self.value)\n    return Signals(out)\n</code></pre>"},{"location":"api/core/#signalflow.core.Signals.pipe","title":"pipe","text":"<pre><code>pipe(*transforms: SignalsTransform) -&gt; 'Signals'\n</code></pre> <p>Apply multiple transformations sequentially.</p> <p>Parameters:</p> Name Type Description Default <code>*transforms</code> <code>SignalsTransform</code> <p>Sequence of transformations to apply in order.</p> <code>()</code> <p>Returns:</p> Name Type Description <code>Signals</code> <code>'Signals'</code> <p>New Signals instance after applying all transformations.</p> Example <pre><code>result = signals.pipe(\n    filter_none_signals,\n    normalize_probabilities,\n    add_metadata\n)\n</code></pre> Source code in <code>src/signalflow/core/containers/signals.py</code> <pre><code>def pipe(self, *transforms: SignalsTransform) -&gt; \"Signals\":\n    \"\"\"Apply multiple transformations sequentially.\n\n    Args:\n        *transforms (SignalsTransform): Sequence of transformations to apply in order.\n\n    Returns:\n        Signals: New Signals instance after applying all transformations.\n\n    Example:\n        ```python\n        result = signals.pipe(\n            filter_none_signals,\n            normalize_probabilities,\n            add_metadata\n        )\n        ```\n    \"\"\"\n    s = self\n    for t in transforms:\n        s = s.apply(t)\n    return s\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalType","title":"SignalType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of signal types.</p> <p>Represents the direction of a trading signal detected by signal detectors.</p> Values <p>NONE: No signal detected or neutral state. RISE: Bullish signal indicating potential price increase. FALL: Bearish signal indicating potential price decrease.</p> Example <pre><code>from signalflow.core.enums import SignalType\n\n# Check signal type\nif signal_type == SignalType.RISE:\n    print(\"Bullish signal detected\")\nelif signal_type == SignalType.FALL:\n    print(\"Bearish signal detected\")\nelse:\n    print(\"No signal\")\n\n# Use in DataFrame\nimport polars as pl\nsignals_df = pl.DataFrame({\n    \"pair\": [\"BTCUSDT\"],\n    \"timestamp\": [datetime.now()],\n    \"signal_type\": [SignalType.RISE.value]\n})\n\n# Compare with enum\nis_rise = signals_df.filter(\n    pl.col(\"signal_type\") == SignalType.RISE.value\n)\n</code></pre> Note <p>Stored as string values in DataFrames for serialization. Use .value to get string representation.</p>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry","title":"SignalFlowRegistry  <code>dataclass</code>","text":"<pre><code>SignalFlowRegistry(_items: Dict[SfComponentType, Dict[str, Type[Any]]] = dict())\n</code></pre> <p>Component registry for dynamic component discovery and instantiation.</p> <p>Provides centralized registration and lookup for SignalFlow components. Components are organized by type (DETECTOR, EXTRACTOR, etc.) and  accessed by case-insensitive names.</p> Registry structure <p>component_type -&gt; name -&gt; class</p> Supported component types <ul> <li>DETECTOR: Signal detection classes</li> <li>EXTRACTOR: Feature extraction classes</li> <li>LABELER: Signal labeling classes</li> <li>ENTRY_RULE: Position entry rules</li> <li>EXIT_RULE: Position exit rules</li> <li>METRIC: Strategy metrics</li> <li>EXECUTOR: Order execution engines</li> </ul> <p>Attributes:</p> Name Type Description <code>_items</code> <code>dict[SfComponentType, dict[str, Type[Any]]]</code> <p>Internal storage mapping component types to name-class pairs.</p> Example <pre><code>from signalflow.core.registry import SignalFlowRegistry\nfrom signalflow.core.enums import SfComponentType\n\n# Create registry\nregistry = SignalFlowRegistry()\n\n# Register component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"sma_cross\",\n    cls=SmaCrossDetector\n)\n\n# Get component class\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n# Instantiate component\ndetector = registry.create(\n    SfComponentType.DETECTOR,\n    \"sma_cross\",\n    fast_window=10,\n    slow_window=20\n)\n\n# List available components\ndetectors = registry.list(SfComponentType.DETECTOR)\nprint(f\"Available detectors: {detectors}\")\n\n# Full snapshot\nsnapshot = registry.snapshot()\nprint(snapshot)\n</code></pre> Note <p>Component names are stored and looked up in lowercase. Use default_registry singleton for application-wide registration.</p> See Also <p>sf_component: Decorator for automatic component registration.</p>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.create","title":"create","text":"<pre><code>create(component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>Instantiate a component by registry key.</p> <p>Convenient method that combines get() and instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to create.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to component constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Instantiated component.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found.</p> <code>TypeError</code> <p>If kwargs don't match component constructor.</p> Example <pre><code># Create detector with params\ndetector = registry.create(\n    SfComponentType.DETECTOR,\n    \"sma_cross\",\n    fast_window=10,\n    slow_window=20\n)\n\n# Create extractor\nextractor = registry.create(\n    SfComponentType.EXTRACTOR,\n    \"rsi\",\n    window=14\n)\n\n# Create with config dict\nconfig = {\"window\": 20, \"threshold\": 0.7}\nlabeler = registry.create(\n    SfComponentType.LABELER,\n    \"fixed\",\n    **config\n)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def create(self, component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Instantiate a component by registry key.\n\n    Convenient method that combines get() and instantiation.\n\n    Args:\n        component_type (SfComponentType): Type of component to create.\n        name (str): Component name (case-insensitive).\n        **kwargs: Arguments to pass to component constructor.\n\n    Returns:\n        Any: Instantiated component.\n\n    Raises:\n        KeyError: If component not found.\n        TypeError: If kwargs don't match component constructor.\n\n    Example:\n        ```python\n        # Create detector with params\n        detector = registry.create(\n            SfComponentType.DETECTOR,\n            \"sma_cross\",\n            fast_window=10,\n            slow_window=20\n        )\n\n        # Create extractor\n        extractor = registry.create(\n            SfComponentType.EXTRACTOR,\n            \"rsi\",\n            window=14\n        )\n\n        # Create with config dict\n        config = {\"window\": 20, \"threshold\": 0.7}\n        labeler = registry.create(\n            SfComponentType.LABELER,\n            \"fixed\",\n            **config\n        )\n        ```\n    \"\"\"\n    cls = self.get(component_type, name)\n    return cls(**kwargs)\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.get","title":"get","text":"<pre><code>get(component_type: SfComponentType, name: str) -&gt; Type[Any]\n</code></pre> <p>Get a registered class by key.</p> <p>Lookup is case-insensitive. Raises helpful error with available components if key not found.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to lookup.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <p>Returns:</p> Type Description <code>Type[Any]</code> <p>Type[Any]: Registered class.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found. Error message includes available components.</p> Example <pre><code># Get component class\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n# Case-insensitive\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n# Instantiate manually\ndetector = detector_cls(fast_window=10, slow_window=20)\n\n# Handle missing component\ntry:\n    cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\nexcept KeyError as e:\n    print(f\"Component not found: {e}\")\n    # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def get(self, component_type: SfComponentType, name: str) -&gt; Type[Any]:\n    \"\"\"Get a registered class by key.\n\n    Lookup is case-insensitive. Raises helpful error with available\n    components if key not found.\n\n    Args:\n        component_type (SfComponentType): Type of component to lookup.\n        name (str): Component name (case-insensitive).\n\n    Returns:\n        Type[Any]: Registered class.\n\n    Raises:\n        KeyError: If component not found. Error message includes available components.\n\n    Example:\n        ```python\n        # Get component class\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # Case-insensitive\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n        # Instantiate manually\n        detector = detector_cls(fast_window=10, slow_window=20)\n\n        # Handle missing component\n        try:\n            cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\n        except KeyError as e:\n            print(f\"Component not found: {e}\")\n            # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n        ```\n    \"\"\"\n    self._ensure(component_type)\n    key = name.lower()\n    try:\n        return self._items[component_type][key]\n    except KeyError as e:\n        available = \", \".join(sorted(self._items[component_type]))\n        raise KeyError(\n            f\"Component not found: {component_type.value}:{key}. Available: [{available}]\"\n        ) from e\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.list","title":"list","text":"<pre><code>list(component_type: SfComponentType) -&gt; list[str]\n</code></pre> <p>List registered components for a type.</p> <p>Returns sorted list of component names for given type.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of components to list.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Sorted list of registered component names.</p> Example <pre><code># List all detectors\ndetectors = registry.list(SfComponentType.DETECTOR)\nprint(f\"Available detectors: {detectors}\")\n# Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n# Check if component exists\nif \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n    detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n# List all component types\nfrom signalflow.core.enums import SfComponentType\nfor component_type in SfComponentType:\n    components = registry.list(component_type)\n    print(f\"{component_type.value}: {components}\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def list(self, component_type: SfComponentType) -&gt; list[str]:\n    \"\"\"List registered components for a type.\n\n    Returns sorted list of component names for given type.\n\n    Args:\n        component_type (SfComponentType): Type of components to list.\n\n    Returns:\n        list[str]: Sorted list of registered component names.\n\n    Example:\n        ```python\n        # List all detectors\n        detectors = registry.list(SfComponentType.DETECTOR)\n        print(f\"Available detectors: {detectors}\")\n        # Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n        # Check if component exists\n        if \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n            detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # List all component types\n        from signalflow.core.enums import SfComponentType\n        for component_type in SfComponentType:\n            components = registry.list(component_type)\n            print(f\"{component_type.value}: {components}\")\n        ```\n    \"\"\"\n    self._ensure(component_type)\n    return sorted(self._items[component_type])\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.register","title":"register","text":"<pre><code>register(component_type: SfComponentType, name: str, cls: Type[Any], *, override: bool = False) -&gt; None\n</code></pre> <p>Register a class under (component_type, name).</p> <p>Stores class in registry for later lookup and instantiation. Names are normalized to lowercase for case-insensitive lookup.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component (DETECTOR, EXTRACTOR, etc.).</p> required <code>name</code> <code>str</code> <p>Registry name (case-insensitive, will be lowercased).</p> required <code>cls</code> <code>Type[Any]</code> <p>Class to register.</p> required <code>override</code> <code>bool</code> <p>Allow overriding existing registration. Default: False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name is empty or already registered (when override=False).</p> Example <pre><code># Register new component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=MyDetector\n)\n\n# Override existing component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=ImprovedDetector,\n    override=True  # Logs warning\n)\n\n# Register multiple types\nregistry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\nregistry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def register(self, component_type: SfComponentType, name: str, cls: Type[Any], *, override: bool = False) -&gt; None:\n    \"\"\"Register a class under (component_type, name).\n\n    Stores class in registry for later lookup and instantiation.\n    Names are normalized to lowercase for case-insensitive lookup.\n\n    Args:\n        component_type (SfComponentType): Type of component (DETECTOR, EXTRACTOR, etc.).\n        name (str): Registry name (case-insensitive, will be lowercased).\n        cls (Type[Any]): Class to register.\n        override (bool): Allow overriding existing registration. Default: False.\n\n    Raises:\n        ValueError: If name is empty or already registered (when override=False).\n\n    Example:\n        ```python\n        # Register new component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=MyDetector\n        )\n\n        # Override existing component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=ImprovedDetector,\n            override=True  # Logs warning\n        )\n\n        # Register multiple types\n        registry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\n        registry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n        ```\n    \"\"\"\n    if not isinstance(name, str) or not name.strip():\n        raise ValueError(\"name must be a non-empty string\")\n\n    key = name.strip().lower()\n    self._ensure(component_type)\n\n    if key in self._items[component_type] and not override:\n        raise ValueError(f\"{component_type.value}:{key} already registered\")\n\n    if key in self._items[component_type] and override:\n        logger.warning(f\"Overriding {component_type.value}:{key} with {cls.__name__}\")\n\n    self._items[component_type][key] = cls\n</code></pre>"},{"location":"api/core/#signalflow.core.SignalFlowRegistry.snapshot","title":"snapshot","text":"<pre><code>snapshot() -&gt; dict[str, list[str]]\n</code></pre> <p>Snapshot of registry for debugging.</p> <p>Returns complete registry state organized by component type.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: Dictionary mapping component type names  to sorted lists of registered component names.</p> Example <pre><code># Get full registry snapshot\nsnapshot = registry.snapshot()\nprint(snapshot)\n# Output:\n# {\n#     'DETECTOR': ['ema_cross', 'sma_cross'],\n#     'EXTRACTOR': ['rsi', 'sma'],\n#     'LABELER': ['fixed', 'triple_barrier'],\n#     'ENTRY_RULE': ['fixed_size'],\n#     'EXIT_RULE': ['take_profit', 'time_based']\n# }\n\n# Use for debugging\nimport json\nprint(json.dumps(registry.snapshot(), indent=2))\n\n# Check registration status\nsnapshot = registry.snapshot()\nif 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n    print(\"SMA detector is registered\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def snapshot(self) -&gt; dict[str, list[str]]:\n    \"\"\"Snapshot of registry for debugging.\n\n    Returns complete registry state organized by component type.\n\n    Returns:\n        dict[str, list[str]]: Dictionary mapping component type names \n            to sorted lists of registered component names.\n\n    Example:\n        ```python\n        # Get full registry snapshot\n        snapshot = registry.snapshot()\n        print(snapshot)\n        # Output:\n        # {\n        #     'DETECTOR': ['ema_cross', 'sma_cross'],\n        #     'EXTRACTOR': ['rsi', 'sma'],\n        #     'LABELER': ['fixed', 'triple_barrier'],\n        #     'ENTRY_RULE': ['fixed_size'],\n        #     'EXIT_RULE': ['take_profit', 'time_based']\n        # }\n\n        # Use for debugging\n        import json\n        print(json.dumps(registry.snapshot(), indent=2))\n\n        # Check registration status\n        snapshot = registry.snapshot()\n        if 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n            print(\"SMA detector is registered\")\n        ```\n    \"\"\"\n    return {t.value: sorted(v.keys()) for t, v in self._items.items()}\n</code></pre>"},{"location":"api/core/#signalflow.core.sf_component","title":"sf_component","text":"<pre><code>sf_component(*, name: str, override: bool = False)\n</code></pre> <p>Register class as SignalFlow component.</p> <p>Decorator that registers a class in the global component registry, making it discoverable by name for dynamic instantiation.</p> <p>The decorated class must have a <code>component_type</code> class attribute of type <code>SfComponentType</code> to indicate what kind of component it is (e.g., DETECTOR, EXTRACTOR, LABELER, ENTRY_RULE, EXIT_RULE).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Registry name for the component (case-insensitive).</p> required <code>override</code> <code>bool</code> <p>Allow overriding existing registration. Default: False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Callable</code> <p>Decorator function that registers and returns the class unchanged.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If class doesn't define component_type attribute.</p> <code>ValueError</code> <p>If name already registered and override=False.</p> Example <pre><code>from signalflow.core import sf_component\nfrom signalflow.core.enums import SfComponentType\nfrom signalflow.detector import SignalDetector\n\n@sf_component(name=\"my_detector\")\nclass MyDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n\n    def detect(self, df):\n        # Detection logic\n        return signals\n\n# Later, instantiate by name\nfrom signalflow.core.registry import default_registry\n\ndetector_cls = default_registry.get(\n    SfComponentType.DETECTOR,\n    \"my_detector\"\n)\ndetector = detector_cls(params={\"window\": 20})\n\n# Override existing registration\n@sf_component(name=\"my_detector\", override=True)\nclass ImprovedDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n    # ... improved implementation\n</code></pre> Example <pre><code># Register multiple component types\n\n@sf_component(name=\"sma_cross\")\nclass SmaCrossDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n    # ...\n\n@sf_component(name=\"rsi\")\nclass RsiExtractor(FeatureExtractor):\n    component_type = SfComponentType.EXTRACTOR\n    # ...\n\n@sf_component(name=\"fixed_size\")\nclass FixedSizeEntry(SignalEntryRule):\n    component_type = SfComponentType.ENTRY_RULE\n    # ...\n\n@sf_component(name=\"take_profit\")\nclass TakeProfitExit(ExitRule):\n    component_type = SfComponentType.EXIT_RULE\n    # ...\n</code></pre> Note <p>Component names are case-insensitive for lookup. The class itself is not modified - only registered. Use override=True carefully to avoid accidental overrides.</p> Source code in <code>src/signalflow/core/decorators.py</code> <pre><code>def sf_component(*, name: str, override: bool = False):\n    \"\"\"Register class as SignalFlow component.\n\n    Decorator that registers a class in the global component registry,\n    making it discoverable by name for dynamic instantiation.\n\n    The decorated class must have a `component_type` class attribute\n    of type `SfComponentType` to indicate what kind of component it is\n    (e.g., DETECTOR, EXTRACTOR, LABELER, ENTRY_RULE, EXIT_RULE).\n\n    Args:\n        name (str): Registry name for the component (case-insensitive).\n        override (bool): Allow overriding existing registration. Default: False.\n\n    Returns:\n        Callable: Decorator function that registers and returns the class unchanged.\n\n    Raises:\n        ValueError: If class doesn't define component_type attribute.\n        ValueError: If name already registered and override=False.\n\n    Example:\n        ```python\n        from signalflow.core import sf_component\n        from signalflow.core.enums import SfComponentType\n        from signalflow.detector import SignalDetector\n\n        @sf_component(name=\"my_detector\")\n        class MyDetector(SignalDetector):\n            component_type = SfComponentType.DETECTOR\n\n            def detect(self, df):\n                # Detection logic\n                return signals\n\n        # Later, instantiate by name\n        from signalflow.core.registry import default_registry\n\n        detector_cls = default_registry.get(\n            SfComponentType.DETECTOR,\n            \"my_detector\"\n        )\n        detector = detector_cls(params={\"window\": 20})\n\n        # Override existing registration\n        @sf_component(name=\"my_detector\", override=True)\n        class ImprovedDetector(SignalDetector):\n            component_type = SfComponentType.DETECTOR\n            # ... improved implementation\n        ```\n\n    Example:\n        ```python\n        # Register multiple component types\n\n        @sf_component(name=\"sma_cross\")\n        class SmaCrossDetector(SignalDetector):\n            component_type = SfComponentType.DETECTOR\n            # ...\n\n        @sf_component(name=\"rsi\")\n        class RsiExtractor(FeatureExtractor):\n            component_type = SfComponentType.EXTRACTOR\n            # ...\n\n        @sf_component(name=\"fixed_size\")\n        class FixedSizeEntry(SignalEntryRule):\n            component_type = SfComponentType.ENTRY_RULE\n            # ...\n\n        @sf_component(name=\"take_profit\")\n        class TakeProfitExit(ExitRule):\n            component_type = SfComponentType.EXIT_RULE\n            # ...\n        ```\n\n    Note:\n        Component names are case-insensitive for lookup.\n        The class itself is not modified - only registered.\n        Use override=True carefully to avoid accidental overrides.\n    \"\"\"\n    def decorator(cls: Type[Any]) -&gt; Type[Any]:\n        component_type = getattr(cls, \"component_type\", None)\n        if not isinstance(component_type, SfComponentType):\n            raise ValueError(\n                f\"{cls.__name__} must define class attribute \"\n                f\"'component_type: SfComponentType'\"\n            )\n\n        default_registry.register(\n            component_type,\n            name=name,\n            cls=cls,\n            override=override,\n        )\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api/core/#signalflow.core.enums","title":"signalflow.core.enums","text":""},{"location":"api/core/#signalflow.core.enums.SfComponentType","title":"SfComponentType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of SignalFlow component types.</p> <p>Defines all component types that can be registered in the component registry. Used by sf_component decorator and SignalFlowRegistry for type-safe registration.</p> Component categories <ul> <li>Data: Raw data loading and storage</li> <li>Feature: Feature extraction</li> <li>Signals: Signal detection, transformation, labeling, validation</li> <li>Strategy: Execution, rules, metrics</li> </ul> Values <p>RAW_DATA_STORE: Raw data storage backends (e.g., DuckDB, Parquet). RAW_DATA_SOURCE: Raw data sources (e.g., Binance API). RAW_DATA_LOADER: Raw data loaders combining source + store. FEATURE_EXTRACTOR: Feature extraction classes (e.g., RSI, SMA). SIGNALS_TRANSFORM: Signal transformation functions. LABELER: Signal labeling strategies (e.g., triple barrier). DETECTOR: Signal detection algorithms (e.g., SMA cross). VALIDATOR: Signal validation models. TORCH_MODULE: PyTorch neural network modules. VALIDATOR_MODEL: Pre-trained validator models. STRATEGY_STORE: Strategy state persistence backends. STRATEGY_RUNNER: Backtest/live runner implementations. STRATEGY_BROKER: Order management and position tracking. STRATEGY_EXECUTOR: Order execution engines (backtest/live). STRATEGY_EXIT_RULE: Position exit rules (e.g., take profit, stop loss). STRATEGY_ENTRY_RULE: Position entry rules (e.g., fixed size). STRATEGY_METRIC: Strategy performance metrics.</p> Example <pre><code>from signalflow.core import sf_component\nfrom signalflow.core.enums import SfComponentType\nfrom signalflow.detector import SignalDetector\n\n# Register detector\n@sf_component(name=\"my_detector\")\nclass MyDetector(SignalDetector):\n    component_type = SfComponentType.DETECTOR\n    # ... implementation\n\n# Register extractor\n@sf_component(name=\"my_feature\")\nclass MyExtractor(FeatureExtractor):\n    component_type = SfComponentType.FEATURE_EXTRACTOR\n    # ... implementation\n\n# Register exit rule\n@sf_component(name=\"my_exit\")\nclass MyExit(ExitRule):\n    component_type = SfComponentType.STRATEGY_EXIT_RULE\n    # ... implementation\n\n# Use in registry\nfrom signalflow.core.registry import default_registry\n\ndetector = default_registry.create(\n    SfComponentType.DETECTOR,\n    \"my_detector\"\n)\n</code></pre> Note <p>All registered components must have component_type class attribute. Component types are organized hierarchically (category/subcategory).</p>"},{"location":"api/core/#signalflow.core.enums.DataFrameType","title":"DataFrameType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported DataFrame backends.</p> <p>Specifies which DataFrame library to use for data processing. Used by FeatureExtractor and other components to determine input/output format.</p> Values <p>POLARS: Polars DataFrame (faster, modern). PANDAS: Pandas DataFrame (legacy compatibility).</p> Example <pre><code>from signalflow.core.enums import DataFrameType\nfrom signalflow.feature import FeatureExtractor\n\n# Polars-based extractor\nclass MyExtractor(FeatureExtractor):\n    df_type = DataFrameType.POLARS\n\n    def extract(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n        return df.with_columns(\n            pl.col(\"close\").rolling_mean(20).alias(\"sma_20\")\n        )\n\n# Pandas-based extractor\nclass LegacyExtractor(FeatureExtractor):\n    df_type = DataFrameType.PANDAS\n\n    def extract(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        df[\"sma_20\"] = df[\"close\"].rolling(20).mean()\n        return df\n\n# Use in RawDataView\nfrom signalflow.core import RawDataView\n\nview = RawDataView(raw=raw_data)\n\n# Get data in required format\ndf_polars = view.get_data(\"spot\", DataFrameType.POLARS)\ndf_pandas = view.get_data(\"spot\", DataFrameType.PANDAS)\n</code></pre> Note <p>New code should prefer POLARS for better performance. PANDAS supported for backward compatibility and legacy libraries.</p>"},{"location":"api/core/#signalflow.core.enums.RawDataType","title":"RawDataType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported raw data types.</p> <p>Defines types of market data that can be loaded and processed.</p> Values <p>SPOT: Spot trading data (OHLCV).</p> Example <pre><code>from signalflow.core.enums import RawDataType\n\n# Load spot data\nloader = BinanceLoader(\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    data_type=RawDataType.SPOT\n)\n\nraw_data = loader.load(\n    datetime_start=datetime(2024, 1, 1),\n    datetime_end=datetime(2024, 12, 31)\n)\n\n# Access spot data\nspot_df = raw_data[RawDataType.SPOT.value]\n\n# Check data type\nif raw_data_type == RawDataType.SPOT:\n    print(\"Processing spot data\")\n</code></pre> Note <p>Future versions will add: - FUTURES: Futures trading data - PERPETUAL: Perpetual swaps data - LOB: Limit order book data</p>"},{"location":"api/core/#signalflow.core.registry","title":"signalflow.core.registry","text":""},{"location":"api/core/#signalflow.core.registry.default_registry","title":"default_registry  <code>module-attribute</code>","text":"<pre><code>default_registry = SignalFlowRegistry()\n</code></pre> <p>Global default registry instance.</p> <p>Use this singleton for application-wide component registration.</p> Example <pre><code>from signalflow.core.registry import default_registry\nfrom signalflow.core.enums import SfComponentType\n\n# Register to default registry\ndefault_registry.register(\n    SfComponentType.DETECTOR,\n    \"my_detector\",\n    MyDetector\n)\n\n# Access from anywhere\ndetector = default_registry.create(\n    SfComponentType.DETECTOR,\n    \"my_detector\"\n)\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry","title":"SignalFlowRegistry  <code>dataclass</code>","text":"<pre><code>SignalFlowRegistry(_items: Dict[SfComponentType, Dict[str, Type[Any]]] = dict())\n</code></pre> <p>Component registry for dynamic component discovery and instantiation.</p> <p>Provides centralized registration and lookup for SignalFlow components. Components are organized by type (DETECTOR, EXTRACTOR, etc.) and  accessed by case-insensitive names.</p> Registry structure <p>component_type -&gt; name -&gt; class</p> Supported component types <ul> <li>DETECTOR: Signal detection classes</li> <li>EXTRACTOR: Feature extraction classes</li> <li>LABELER: Signal labeling classes</li> <li>ENTRY_RULE: Position entry rules</li> <li>EXIT_RULE: Position exit rules</li> <li>METRIC: Strategy metrics</li> <li>EXECUTOR: Order execution engines</li> </ul> <p>Attributes:</p> Name Type Description <code>_items</code> <code>dict[SfComponentType, dict[str, Type[Any]]]</code> <p>Internal storage mapping component types to name-class pairs.</p> Example <pre><code>from signalflow.core.registry import SignalFlowRegistry\nfrom signalflow.core.enums import SfComponentType\n\n# Create registry\nregistry = SignalFlowRegistry()\n\n# Register component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"sma_cross\",\n    cls=SmaCrossDetector\n)\n\n# Get component class\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n# Instantiate component\ndetector = registry.create(\n    SfComponentType.DETECTOR,\n    \"sma_cross\",\n    fast_window=10,\n    slow_window=20\n)\n\n# List available components\ndetectors = registry.list(SfComponentType.DETECTOR)\nprint(f\"Available detectors: {detectors}\")\n\n# Full snapshot\nsnapshot = registry.snapshot()\nprint(snapshot)\n</code></pre> Note <p>Component names are stored and looked up in lowercase. Use default_registry singleton for application-wide registration.</p> See Also <p>sf_component: Decorator for automatic component registration.</p>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.create","title":"create","text":"<pre><code>create(component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any\n</code></pre> <p>Instantiate a component by registry key.</p> <p>Convenient method that combines get() and instantiation.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to create.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <code>**kwargs</code> <code>Any</code> <p>Arguments to pass to component constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>Instantiated component.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found.</p> <code>TypeError</code> <p>If kwargs don't match component constructor.</p> Example <pre><code># Create detector with params\ndetector = registry.create(\n    SfComponentType.DETECTOR,\n    \"sma_cross\",\n    fast_window=10,\n    slow_window=20\n)\n\n# Create extractor\nextractor = registry.create(\n    SfComponentType.EXTRACTOR,\n    \"rsi\",\n    window=14\n)\n\n# Create with config dict\nconfig = {\"window\": 20, \"threshold\": 0.7}\nlabeler = registry.create(\n    SfComponentType.LABELER,\n    \"fixed\",\n    **config\n)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def create(self, component_type: SfComponentType, name: str, **kwargs: Any) -&gt; Any:\n    \"\"\"Instantiate a component by registry key.\n\n    Convenient method that combines get() and instantiation.\n\n    Args:\n        component_type (SfComponentType): Type of component to create.\n        name (str): Component name (case-insensitive).\n        **kwargs: Arguments to pass to component constructor.\n\n    Returns:\n        Any: Instantiated component.\n\n    Raises:\n        KeyError: If component not found.\n        TypeError: If kwargs don't match component constructor.\n\n    Example:\n        ```python\n        # Create detector with params\n        detector = registry.create(\n            SfComponentType.DETECTOR,\n            \"sma_cross\",\n            fast_window=10,\n            slow_window=20\n        )\n\n        # Create extractor\n        extractor = registry.create(\n            SfComponentType.EXTRACTOR,\n            \"rsi\",\n            window=14\n        )\n\n        # Create with config dict\n        config = {\"window\": 20, \"threshold\": 0.7}\n        labeler = registry.create(\n            SfComponentType.LABELER,\n            \"fixed\",\n            **config\n        )\n        ```\n    \"\"\"\n    cls = self.get(component_type, name)\n    return cls(**kwargs)\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.get","title":"get","text":"<pre><code>get(component_type: SfComponentType, name: str) -&gt; Type[Any]\n</code></pre> <p>Get a registered class by key.</p> <p>Lookup is case-insensitive. Raises helpful error with available components if key not found.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component to lookup.</p> required <code>name</code> <code>str</code> <p>Component name (case-insensitive).</p> required <p>Returns:</p> Type Description <code>Type[Any]</code> <p>Type[Any]: Registered class.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If component not found. Error message includes available components.</p> Example <pre><code># Get component class\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n# Case-insensitive\ndetector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n# Instantiate manually\ndetector = detector_cls(fast_window=10, slow_window=20)\n\n# Handle missing component\ntry:\n    cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\nexcept KeyError as e:\n    print(f\"Component not found: {e}\")\n    # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def get(self, component_type: SfComponentType, name: str) -&gt; Type[Any]:\n    \"\"\"Get a registered class by key.\n\n    Lookup is case-insensitive. Raises helpful error with available\n    components if key not found.\n\n    Args:\n        component_type (SfComponentType): Type of component to lookup.\n        name (str): Component name (case-insensitive).\n\n    Returns:\n        Type[Any]: Registered class.\n\n    Raises:\n        KeyError: If component not found. Error message includes available components.\n\n    Example:\n        ```python\n        # Get component class\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # Case-insensitive\n        detector_cls = registry.get(SfComponentType.DETECTOR, \"SMA_Cross\")\n\n        # Instantiate manually\n        detector = detector_cls(fast_window=10, slow_window=20)\n\n        # Handle missing component\n        try:\n            cls = registry.get(SfComponentType.DETECTOR, \"unknown\")\n        except KeyError as e:\n            print(f\"Component not found: {e}\")\n            # Shows: \"Component not found: DETECTOR:unknown. Available: [sma_cross, ...]\"\n        ```\n    \"\"\"\n    self._ensure(component_type)\n    key = name.lower()\n    try:\n        return self._items[component_type][key]\n    except KeyError as e:\n        available = \", \".join(sorted(self._items[component_type]))\n        raise KeyError(\n            f\"Component not found: {component_type.value}:{key}. Available: [{available}]\"\n        ) from e\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.list","title":"list","text":"<pre><code>list(component_type: SfComponentType) -&gt; list[str]\n</code></pre> <p>List registered components for a type.</p> <p>Returns sorted list of component names for given type.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of components to list.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Sorted list of registered component names.</p> Example <pre><code># List all detectors\ndetectors = registry.list(SfComponentType.DETECTOR)\nprint(f\"Available detectors: {detectors}\")\n# Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n# Check if component exists\nif \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n    detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n# List all component types\nfrom signalflow.core.enums import SfComponentType\nfor component_type in SfComponentType:\n    components = registry.list(component_type)\n    print(f\"{component_type.value}: {components}\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def list(self, component_type: SfComponentType) -&gt; list[str]:\n    \"\"\"List registered components for a type.\n\n    Returns sorted list of component names for given type.\n\n    Args:\n        component_type (SfComponentType): Type of components to list.\n\n    Returns:\n        list[str]: Sorted list of registered component names.\n\n    Example:\n        ```python\n        # List all detectors\n        detectors = registry.list(SfComponentType.DETECTOR)\n        print(f\"Available detectors: {detectors}\")\n        # Output: ['ema_cross', 'macd', 'rsi_threshold', 'sma_cross']\n\n        # Check if component exists\n        if \"sma_cross\" in registry.list(SfComponentType.DETECTOR):\n            detector = registry.create(SfComponentType.DETECTOR, \"sma_cross\")\n\n        # List all component types\n        from signalflow.core.enums import SfComponentType\n        for component_type in SfComponentType:\n            components = registry.list(component_type)\n            print(f\"{component_type.value}: {components}\")\n        ```\n    \"\"\"\n    self._ensure(component_type)\n    return sorted(self._items[component_type])\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.register","title":"register","text":"<pre><code>register(component_type: SfComponentType, name: str, cls: Type[Any], *, override: bool = False) -&gt; None\n</code></pre> <p>Register a class under (component_type, name).</p> <p>Stores class in registry for later lookup and instantiation. Names are normalized to lowercase for case-insensitive lookup.</p> <p>Parameters:</p> Name Type Description Default <code>component_type</code> <code>SfComponentType</code> <p>Type of component (DETECTOR, EXTRACTOR, etc.).</p> required <code>name</code> <code>str</code> <p>Registry name (case-insensitive, will be lowercased).</p> required <code>cls</code> <code>Type[Any]</code> <p>Class to register.</p> required <code>override</code> <code>bool</code> <p>Allow overriding existing registration. Default: False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If name is empty or already registered (when override=False).</p> Example <pre><code># Register new component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=MyDetector\n)\n\n# Override existing component\nregistry.register(\n    SfComponentType.DETECTOR,\n    name=\"my_detector\",\n    cls=ImprovedDetector,\n    override=True  # Logs warning\n)\n\n# Register multiple types\nregistry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\nregistry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def register(self, component_type: SfComponentType, name: str, cls: Type[Any], *, override: bool = False) -&gt; None:\n    \"\"\"Register a class under (component_type, name).\n\n    Stores class in registry for later lookup and instantiation.\n    Names are normalized to lowercase for case-insensitive lookup.\n\n    Args:\n        component_type (SfComponentType): Type of component (DETECTOR, EXTRACTOR, etc.).\n        name (str): Registry name (case-insensitive, will be lowercased).\n        cls (Type[Any]): Class to register.\n        override (bool): Allow overriding existing registration. Default: False.\n\n    Raises:\n        ValueError: If name is empty or already registered (when override=False).\n\n    Example:\n        ```python\n        # Register new component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=MyDetector\n        )\n\n        # Override existing component\n        registry.register(\n            SfComponentType.DETECTOR,\n            name=\"my_detector\",\n            cls=ImprovedDetector,\n            override=True  # Logs warning\n        )\n\n        # Register multiple types\n        registry.register(SfComponentType.EXTRACTOR, \"rsi\", RsiExtractor)\n        registry.register(SfComponentType.LABELER, \"fixed\", FixedHorizonLabeler)\n        ```\n    \"\"\"\n    if not isinstance(name, str) or not name.strip():\n        raise ValueError(\"name must be a non-empty string\")\n\n    key = name.strip().lower()\n    self._ensure(component_type)\n\n    if key in self._items[component_type] and not override:\n        raise ValueError(f\"{component_type.value}:{key} already registered\")\n\n    if key in self._items[component_type] and override:\n        logger.warning(f\"Overriding {component_type.value}:{key} with {cls.__name__}\")\n\n    self._items[component_type][key] = cls\n</code></pre>"},{"location":"api/core/#signalflow.core.registry.SignalFlowRegistry.snapshot","title":"snapshot","text":"<pre><code>snapshot() -&gt; dict[str, list[str]]\n</code></pre> <p>Snapshot of registry for debugging.</p> <p>Returns complete registry state organized by component type.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: Dictionary mapping component type names  to sorted lists of registered component names.</p> Example <pre><code># Get full registry snapshot\nsnapshot = registry.snapshot()\nprint(snapshot)\n# Output:\n# {\n#     'DETECTOR': ['ema_cross', 'sma_cross'],\n#     'EXTRACTOR': ['rsi', 'sma'],\n#     'LABELER': ['fixed', 'triple_barrier'],\n#     'ENTRY_RULE': ['fixed_size'],\n#     'EXIT_RULE': ['take_profit', 'time_based']\n# }\n\n# Use for debugging\nimport json\nprint(json.dumps(registry.snapshot(), indent=2))\n\n# Check registration status\nsnapshot = registry.snapshot()\nif 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n    print(\"SMA detector is registered\")\n</code></pre> Source code in <code>src/signalflow/core/registry.py</code> <pre><code>def snapshot(self) -&gt; dict[str, list[str]]:\n    \"\"\"Snapshot of registry for debugging.\n\n    Returns complete registry state organized by component type.\n\n    Returns:\n        dict[str, list[str]]: Dictionary mapping component type names \n            to sorted lists of registered component names.\n\n    Example:\n        ```python\n        # Get full registry snapshot\n        snapshot = registry.snapshot()\n        print(snapshot)\n        # Output:\n        # {\n        #     'DETECTOR': ['ema_cross', 'sma_cross'],\n        #     'EXTRACTOR': ['rsi', 'sma'],\n        #     'LABELER': ['fixed', 'triple_barrier'],\n        #     'ENTRY_RULE': ['fixed_size'],\n        #     'EXIT_RULE': ['take_profit', 'time_based']\n        # }\n\n        # Use for debugging\n        import json\n        print(json.dumps(registry.snapshot(), indent=2))\n\n        # Check registration status\n        snapshot = registry.snapshot()\n        if 'DETECTOR' in snapshot and 'sma_cross' in snapshot['DETECTOR']:\n            print(\"SMA detector is registered\")\n        ```\n    \"\"\"\n    return {t.value: sorted(v.keys()) for t, v in self._items.items()}\n</code></pre>"},{"location":"api/data/","title":"Data Module","text":""},{"location":"api/data/#loaders","title":"Loaders","text":""},{"location":"api/data/#signalflow.data.source.binance.BinanceSpotLoader","title":"signalflow.data.source.binance.BinanceSpotLoader  <code>dataclass</code>","text":"<pre><code>BinanceSpotLoader(store: DuckDbSpotStore = (lambda: DuckDbSpotStore(db_path=(Path('raw_data.duckdb'))))(), timeframe: str = '1m')\n</code></pre> <p>               Bases: <code>RawDataLoader</code></p> <p>Downloads and stores Binance spot OHLCV data for fixed timeframe.</p> <p>Combines BinanceClient (source) and DuckDbSpotStore (storage) to provide complete data pipeline with gap filling and incremental updates.</p> <p>Attributes:</p> Name Type Description <code>store</code> <code>DuckDbSpotStore</code> <p>Storage backend. Default: raw_data.duckdb.</p> <code>timeframe</code> <code>str</code> <p>Fixed timeframe for all data. Default: \"1m\".</p>"},{"location":"api/data/#signalflow.data.source.binance.BinanceSpotLoader.download","title":"download  <code>async</code>","text":"<pre><code>download(pairs: list[str], days: Optional[int] = None, start: Optional[datetime] = None, end: Optional[datetime] = None, fill_gaps: bool = True) -&gt; None\n</code></pre> <p>Download historical data with intelligent range detection.</p> Automatically determines what to download <ul> <li>If no existing data: download full range</li> <li>If data exists: download before/after existing range</li> <li>If fill_gaps=True: detect and fill gaps in existing range</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>list[str]</code> <p>Trading pairs to download.</p> required <code>days</code> <code>int | None</code> <p>Number of days back from end. Default: 7.</p> <code>None</code> <code>start</code> <code>datetime | None</code> <p>Range start (overrides days).</p> <code>None</code> <code>end</code> <code>datetime | None</code> <p>Range end. Default: now.</p> <code>None</code> <code>fill_gaps</code> <code>bool</code> <p>Detect and fill gaps. Default: True.</p> <code>True</code> Note <p>Runs async download for all pairs concurrently. Logs progress for large downloads. Errors logged but don't stop other pairs.</p> Source code in <code>src/signalflow/data/source/binance.py</code> <pre><code>async def download(\n    self,\n    pairs: list[str],\n    days: Optional[int] = None,\n    start: Optional[datetime] = None,\n    end: Optional[datetime] = None,\n    fill_gaps: bool = True,\n) -&gt; None:\n    \"\"\"Download historical data with intelligent range detection.\n\n    Automatically determines what to download:\n        - If no existing data: download full range\n        - If data exists: download before/after existing range\n        - If fill_gaps=True: detect and fill gaps in existing range\n\n    Args:\n        pairs (list[str]): Trading pairs to download.\n        days (int | None): Number of days back from end. Default: 7.\n        start (datetime | None): Range start (overrides days).\n        end (datetime | None): Range end. Default: now.\n        fill_gaps (bool): Detect and fill gaps. Default: True.\n\n    Note:\n        Runs async download for all pairs concurrently.\n        Logs progress for large downloads.\n        Errors logged but don't stop other pairs.\n    \"\"\"\n\n    now = datetime.now(timezone.utc).replace(tzinfo=None)\n    if end is None:\n        end = now\n    else:\n        end = _ensure_utc_naive(end)\n\n    if start is None:\n        start = end - timedelta(days=days if days else 7)\n    else:\n        start = _ensure_utc_naive(start)\n\n    tf_minutes = {\n        \"1m\": 1,\n        \"3m\": 3,\n        \"5m\": 5,\n        \"15m\": 15,\n        \"30m\": 30,\n        \"1h\": 60,\n        \"2h\": 120,\n        \"4h\": 240,\n        \"6h\": 360,\n        \"8h\": 480,\n        \"12h\": 720,\n        \"1d\": 1440,\n    }.get(self.timeframe, 1)\n\n    async def download_pair(client: BinanceClient, pair: str) -&gt; None:\n        logger.info(f\"Processing {pair} from {start} to {end}\")\n\n        db_min, db_max = self.store.get_time_bounds(pair)\n        ranges_to_download: list[tuple[datetime, datetime]] = []\n\n        if db_min is None:\n            ranges_to_download.append((start, end))\n        else:\n            if start &lt; db_min:\n                ranges_to_download.append((start, db_min - timedelta(minutes=tf_minutes)))\n            if end &gt; db_max:\n                ranges_to_download.append((db_max + timedelta(minutes=tf_minutes), end))\n\n            if fill_gaps:\n                overlap_start = max(start, db_min)\n                overlap_end = min(end, db_max)\n                if overlap_start &lt; overlap_end:\n                    gaps = self.store.find_gaps(pair, overlap_start, overlap_end, tf_minutes)\n                    ranges_to_download.extend(gaps)\n\n        for range_start, range_end in ranges_to_download:\n            if range_start &gt;= range_end:\n                continue\n\n            logger.info(f\"{pair}: downloading {range_start} -&gt; {range_end}\")\n\n            try:\n                klines = await client.get_klines_range(\n                    pair=pair,\n                    timeframe=self.timeframe,\n                    start_time=range_start,\n                    end_time=range_end,\n                )\n                self.store.insert_klines(pair, klines)\n            except Exception as e:  \n                logger.error(f\"Error downloading {pair}: {e}\")\n\n    async with BinanceClient() as client:\n        await asyncio.gather(*[download_pair(client, pair) for pair in pairs])\n\n    self.store.close()\n</code></pre>"},{"location":"api/data/#signalflow.data.source.binance.BinanceSpotLoader.sync","title":"sync  <code>async</code>","text":"<pre><code>sync(pairs: list[str], update_interval_sec: int = 60) -&gt; None\n</code></pre> <p>Real-time sync - continuously update with latest data.</p> <p>Runs indefinitely, fetching latest candles at specified interval. Useful for live trading or monitoring.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>list[str]</code> <p>Trading pairs to sync.</p> required <code>update_interval_sec</code> <code>int</code> <p>Update interval in seconds. Default: 60.</p> <code>60</code> Note <p>Runs forever - use Ctrl+C to stop or run in background task. Fetches last 5 candles per update (ensures no gaps). Errors logged but sync continues.</p> Source code in <code>src/signalflow/data/source/binance.py</code> <pre><code>async def sync(\n    self,\n    pairs: list[str],\n    update_interval_sec: int = 60,\n) -&gt; None:\n    \"\"\"Real-time sync - continuously update with latest data.\n\n    Runs indefinitely, fetching latest candles at specified interval.\n    Useful for live trading or monitoring.\n\n    Args:\n        pairs (list[str]): Trading pairs to sync.\n        update_interval_sec (int): Update interval in seconds. Default: 60.\n\n    Note:\n        Runs forever - use Ctrl+C to stop or run in background task.\n        Fetches last 5 candles per update (ensures no gaps).\n        Errors logged but sync continues.\n    \"\"\"\n\n    logger.info(f\"Starting real-time sync for {pairs}\")\n    logger.info(f\"Update interval: {update_interval_sec}s (timeframe={self.timeframe})\")\n\n    async def fetch_and_store(client: BinanceClient, pair: str) -&gt; None:\n        try:\n            klines = await client.get_klines(pair=pair, timeframe=self.timeframe, limit=5)\n            self.store.insert_klines(pair, klines)\n        except Exception as e:\n            logger.error(f\"Error syncing {pair}: {e}\")\n\n    async with BinanceClient() as client:\n        while True:\n            await asyncio.gather(*[fetch_and_store(client, pair) for pair in pairs])\n            logger.debug(f\"Synced {len(pairs)} pairs\")\n            await asyncio.sleep(update_interval_sec)\n</code></pre>"},{"location":"api/data/#storage","title":"Storage","text":""},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore","title":"signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore  <code>dataclass</code>","text":"<pre><code>DuckDbSpotStore(db_path: Path, timeframe: str = '1m')\n</code></pre> <p>               Bases: <code>RawDataStore</code></p> <p>DuckDB storage backend for OHLCV spot data.</p> <p>Provides efficient storage and retrieval of candlestick (OHLCV) data using DuckDB as the backend. Designed for fixed-timeframe storage (timeframe not stored per-row, configured at database level).</p> Key features <ul> <li>Automatic schema migration from legacy formats</li> <li>Efficient batch inserts with upsert (INSERT OR REPLACE)</li> <li>Gap detection for data continuity checks</li> <li>Multi-pair batch loading</li> <li>Indexed queries for fast retrieval</li> </ul> Schema <ul> <li>pair (VARCHAR): Trading pair</li> <li>timestamp (TIMESTAMP): Bar open time (timezone-naive)</li> <li>open, high, low, close (DOUBLE): OHLC prices</li> <li>volume (DOUBLE): Trading volume</li> <li>trades (INTEGER): Number of trades</li> </ul> <p>Attributes:</p> Name Type Description <code>db_path</code> <code>Path</code> <p>Path to DuckDB file.</p> <code>timeframe</code> <code>str</code> <p>Fixed timeframe for all data (e.g., \"1m\", \"5m\"). Default: \"1m\".</p> <code>_con</code> <code>DuckDBPyConnection</code> <p>Database connection (initialized in post_init).</p> Example <pre><code>from signalflow.data.raw_store import DuckDbSpotStore\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Create store\nstore = DuckDbSpotStore(\n    db_path=Path(\"data/binance_spot.duckdb\"),\n    timeframe=\"1m\"\n)\n\ntry:\n    # Insert data\n    klines = [\n        {\n            \"timestamp\": datetime(2024, 1, 1, 10, 0),\n            \"open\": 45000.0,\n            \"high\": 45100.0,\n            \"low\": 44900.0,\n            \"close\": 45050.0,\n            \"volume\": 100.5,\n            \"trades\": 150\n        }\n    ]\n    store.insert_klines(\"BTCUSDT\", klines)\n\n    # Load data\n    df = store.load(\"BTCUSDT\", hours=24)\n\n    # Check data bounds\n    min_ts, max_ts = store.get_time_bounds(\"BTCUSDT\")\n    print(f\"Data range: {min_ts} to {max_ts}\")\n\n    # Get statistics\n    stats = store.get_stats()\n    print(stats)\n\nfinally:\n    store.close()\n</code></pre> Note <p>Timeframe is fixed per database, not per row. Automatically migrates from legacy schema (open_time, timeframe columns). Always call close() to cleanup database connection.</p> See Also <p>RawDataStore: Base class with interface definition. RawDataFactory: Factory for creating RawData from stores.</p>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Initialize database connection and ensure schema.</p> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Initialize database connection and ensure schema.\"\"\"\n    self._con = duckdb.connect(str(self.db_path))\n    self._ensure_tables()\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.close","title":"close","text":"<pre><code>close() -&gt; None\n</code></pre> <p>Close database connection and cleanup resources.</p> <p>Always call in finally block or use context manager to ensure cleanup.</p> Example <pre><code>store = DuckDbSpotStore(Path(\"data/binance.duckdb\"))\ntry:\n    df = store.load(\"BTCUSDT\", hours=24)\nfinally:\n    store.close()\n</code></pre> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def close(self) -&gt; None:\n    \"\"\"Close database connection and cleanup resources.\n\n    Always call in finally block or use context manager to ensure cleanup.\n\n    Example:\n        ```python\n        store = DuckDbSpotStore(Path(\"data/binance.duckdb\"))\n        try:\n            df = store.load(\"BTCUSDT\", hours=24)\n        finally:\n            store.close()\n        ```\n    \"\"\"\n    self._con.close()\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.find_gaps","title":"find_gaps","text":"<pre><code>find_gaps(pair: str, start: datetime, end: datetime, tf_minutes: int) -&gt; list[tuple[datetime, datetime]]\n</code></pre> <p>Find gaps in data coverage for a pair.</p> <p>Detects missing bars in expected continuous sequence based on timeframe. Useful for data quality checks and incremental backfilling.</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>str</code> <p>Trading pair (e.g., \"BTCUSDT\").</p> required <code>start</code> <code>datetime</code> <p>Start of expected range.</p> required <code>end</code> <code>datetime</code> <p>End of expected range.</p> required <code>tf_minutes</code> <code>int</code> <p>Timeframe in minutes (e.g., 1 for 1m, 5 for 5m).</p> required <p>Returns:</p> Type Description <code>list[tuple[datetime, datetime]]</code> <p>list[tuple[datetime, datetime]]: List of (gap_start, gap_end) tuples. Empty list if no gaps found.</p> Example <pre><code>from datetime import datetime\n\n# Check for gaps in January 2024\ngaps = store.find_gaps(\n    pair=\"BTCUSDT\",\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31),\n    tf_minutes=1\n)\n\nif gaps:\n    print(f\"Found {len(gaps)} gaps:\")\n    for gap_start, gap_end in gaps:\n        duration = gap_end - gap_start\n        print(f\"  {gap_start} to {gap_end} ({duration})\")\n\n        # Backfill gaps\n        backfill_data(pair=\"BTCUSDT\", start=gap_start, end=gap_end)\nelse:\n    print(\"No gaps found - data is continuous\")\n\n# Data quality report\ngaps = store.find_gaps(\"BTCUSDT\", start, end, tf_minutes=1)\ntotal_expected = int((end - start).total_seconds() / 60)\ntotal_missing = sum((g[1] - g[0]).total_seconds() / 60 for g in gaps)\ncoverage = (1 - total_missing / total_expected) * 100\nprint(f\"Data coverage: {coverage:.2f}%\")\n</code></pre> Note <p>Returns full range [(start, end)] if no data exists. Computationally expensive for large date ranges - use sparingly.</p> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def find_gaps(\n    self,\n    pair: str,\n    start: datetime,\n    end: datetime,\n    tf_minutes: int,\n) -&gt; list[tuple[datetime, datetime]]:\n    \"\"\"Find gaps in data coverage for a pair.\n\n    Detects missing bars in expected continuous sequence based on timeframe.\n    Useful for data quality checks and incremental backfilling.\n\n    Args:\n        pair (str): Trading pair (e.g., \"BTCUSDT\").\n        start (datetime): Start of expected range.\n        end (datetime): End of expected range.\n        tf_minutes (int): Timeframe in minutes (e.g., 1 for 1m, 5 for 5m).\n\n    Returns:\n        list[tuple[datetime, datetime]]: List of (gap_start, gap_end) tuples.\n            Empty list if no gaps found.\n\n    Example:\n        ```python\n        from datetime import datetime\n\n        # Check for gaps in January 2024\n        gaps = store.find_gaps(\n            pair=\"BTCUSDT\",\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 1, 31),\n            tf_minutes=1\n        )\n\n        if gaps:\n            print(f\"Found {len(gaps)} gaps:\")\n            for gap_start, gap_end in gaps:\n                duration = gap_end - gap_start\n                print(f\"  {gap_start} to {gap_end} ({duration})\")\n\n                # Backfill gaps\n                backfill_data(pair=\"BTCUSDT\", start=gap_start, end=gap_end)\n        else:\n            print(\"No gaps found - data is continuous\")\n\n        # Data quality report\n        gaps = store.find_gaps(\"BTCUSDT\", start, end, tf_minutes=1)\n        total_expected = int((end - start).total_seconds() / 60)\n        total_missing = sum((g[1] - g[0]).total_seconds() / 60 for g in gaps)\n        coverage = (1 - total_missing / total_expected) * 100\n        print(f\"Data coverage: {coverage:.2f}%\")\n        ```\n\n    Note:\n        Returns full range [(start, end)] if no data exists.\n        Computationally expensive for large date ranges - use sparingly.\n    \"\"\"\n    existing = self._con.execute(\"\"\"\n        SELECT timestamp\n        FROM ohlcv\n        WHERE pair = ? AND timestamp BETWEEN ? AND ?\n        ORDER BY timestamp\n    \"\"\", [pair, start, end]).fetchall()\n\n    if not existing:\n        return [(start, end)]\n\n    existing_times = {row[0] for row in existing}\n    gaps: list[tuple[datetime, datetime]] = []\n\n    gap_start: Optional[datetime] = None\n    current = start\n\n    while current &lt;= end:\n        if current not in existing_times:\n            if gap_start is None:\n                gap_start = current\n        else:\n            if gap_start is not None:\n                gaps.append((gap_start, current - timedelta(minutes=tf_minutes)))\n                gap_start = None\n        current += timedelta(minutes=tf_minutes)\n\n    if gap_start is not None:\n        gaps.append((gap_start, end))\n\n    return gaps\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; pl.DataFrame\n</code></pre> <p>Get database statistics per pair.</p> <p>Returns summary statistics for all pairs in database.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Statistics with columns: - pair (str): Trading pair - rows (int): Number of bars - first_candle (datetime): Earliest timestamp - last_candle (datetime): Latest timestamp - total_volume (float): Sum of volume</p> Example <pre><code># Get overview\nstats = store.get_stats()\nprint(stats)\n\n# Check coverage\nfor row in stats.iter_rows(named=True):\n    pair = row[\"pair\"]\n    days = (row[\"last_candle\"] - row[\"first_candle\"]).days\n    print(f\"{pair}: {row['rows']:,} bars over {days} days\")\n\n# Identify incomplete data\nmin_rows = stats[\"rows\"].min()\nincomplete = stats.filter(pl.col(\"rows\") &lt; min_rows * 0.9)\nprint(f\"Pairs with &lt;90% coverage: {incomplete['pair'].to_list()}\")\n</code></pre> Note <p>Timeframe not included in output (stored in meta table). Sorted alphabetically by pair.</p> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def get_stats(self) -&gt; pl.DataFrame:\n    \"\"\"Get database statistics per pair.\n\n    Returns summary statistics for all pairs in database.\n\n    Returns:\n        pl.DataFrame: Statistics with columns:\n            - pair (str): Trading pair\n            - rows (int): Number of bars\n            - first_candle (datetime): Earliest timestamp\n            - last_candle (datetime): Latest timestamp\n            - total_volume (float): Sum of volume\n\n    Example:\n        ```python\n        # Get overview\n        stats = store.get_stats()\n        print(stats)\n\n        # Check coverage\n        for row in stats.iter_rows(named=True):\n            pair = row[\"pair\"]\n            days = (row[\"last_candle\"] - row[\"first_candle\"]).days\n            print(f\"{pair}: {row['rows']:,} bars over {days} days\")\n\n        # Identify incomplete data\n        min_rows = stats[\"rows\"].min()\n        incomplete = stats.filter(pl.col(\"rows\") &lt; min_rows * 0.9)\n        print(f\"Pairs with &lt;90% coverage: {incomplete['pair'].to_list()}\")\n        ```\n\n    Note:\n        Timeframe not included in output (stored in meta table).\n        Sorted alphabetically by pair.\n    \"\"\"\n    return self._con.execute(\"\"\"\n        SELECT\n            pair,\n            COUNT(*) as rows,\n            MIN(timestamp) as first_candle,\n            MAX(timestamp) as last_candle,\n            ROUND(SUM(volume), 2) as total_volume\n        FROM ohlcv\n        GROUP BY pair\n        ORDER BY pair\n    \"\"\").pl()\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.get_time_bounds","title":"get_time_bounds","text":"<pre><code>get_time_bounds(pair: str) -&gt; tuple[Optional[datetime], Optional[datetime]]\n</code></pre> <p>Get earliest and latest timestamps for a pair.</p> Useful for <ul> <li>Checking data availability</li> <li>Planning data updates</li> <li>Validating date ranges</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>str</code> <p>Trading pair (e.g., \"BTCUSDT\").</p> required <p>Returns:</p> Type Description <code>tuple[Optional[datetime], Optional[datetime]]</code> <p>tuple[datetime | None, datetime | None]: (min_timestamp, max_timestamp). Both None if no data exists for pair.</p> Example <pre><code># Check data availability\nmin_ts, max_ts = store.get_time_bounds(\"BTCUSDT\")\n\nif min_ts and max_ts:\n    print(f\"Data available: {min_ts} to {max_ts}\")\n    days = (max_ts - min_ts).days\n    print(f\"Total days: {days}\")\nelse:\n    print(\"No data available\")\n\n# Plan incremental update\n_, max_ts = store.get_time_bounds(\"BTCUSDT\")\nif max_ts:\n    # Fetch data from max_ts to now\n    fetch_data(start=max_ts, end=datetime.now())\n</code></pre> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def get_time_bounds(self, pair: str) -&gt; tuple[Optional[datetime], Optional[datetime]]:\n    \"\"\"Get earliest and latest timestamps for a pair.\n\n    Useful for:\n        - Checking data availability\n        - Planning data updates\n        - Validating date ranges\n\n    Args:\n        pair (str): Trading pair (e.g., \"BTCUSDT\").\n\n    Returns:\n        tuple[datetime | None, datetime | None]: (min_timestamp, max_timestamp).\n            Both None if no data exists for pair.\n\n    Example:\n        ```python\n        # Check data availability\n        min_ts, max_ts = store.get_time_bounds(\"BTCUSDT\")\n\n        if min_ts and max_ts:\n            print(f\"Data available: {min_ts} to {max_ts}\")\n            days = (max_ts - min_ts).days\n            print(f\"Total days: {days}\")\n        else:\n            print(\"No data available\")\n\n        # Plan incremental update\n        _, max_ts = store.get_time_bounds(\"BTCUSDT\")\n        if max_ts:\n            # Fetch data from max_ts to now\n            fetch_data(start=max_ts, end=datetime.now())\n        ```\n    \"\"\"\n    result = self._con.execute(\"\"\"\n        SELECT MIN(timestamp), MAX(timestamp)\n        FROM ohlcv\n        WHERE pair = ?\n    \"\"\", [pair]).fetchone()\n    return (result[0], result[1]) if result and result[0] else (None, None)\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.insert_klines","title":"insert_klines","text":"<pre><code>insert_klines(pair: str, klines: list[dict]) -&gt; None\n</code></pre> <p>Upsert klines (INSERT OR REPLACE).</p> <p>Efficient batch insertion with automatic upsert on (pair, timestamp) conflict. Uses Arrow-based bulk insert for &gt;10 rows for better performance.</p> Timestamp normalization <ul> <li>Removes timezone info</li> <li>Rounds to minute (removes seconds/microseconds)</li> <li>If second != 0, rounds up to next minute</li> </ul> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>str</code> <p>Trading pair (e.g., \"BTCUSDT\").</p> required <code>klines</code> <code>list[dict]</code> <p>List of kline dictionaries. Each must contain: - timestamp (datetime): Bar open time - open (float): Open price - high (float): High price - low (float): Low price - close (float): Close price - volume (float): Trading volume - trades (int, optional): Number of trades</p> required Example <pre><code>from datetime import datetime\n\n# Insert single kline\nstore.insert_klines(\"BTCUSDT\", [\n    {\n        \"timestamp\": datetime(2024, 1, 1, 10, 0),\n        \"open\": 45000.0,\n        \"high\": 45100.0,\n        \"low\": 44900.0,\n        \"close\": 45050.0,\n        \"volume\": 100.5,\n        \"trades\": 150\n    }\n])\n\n# Batch insert (efficient for &gt;10 rows)\nklines = [\n    {\n        \"timestamp\": datetime(2024, 1, 1, 10, i),\n        \"open\": 45000.0 + i,\n        \"high\": 45100.0 + i,\n        \"low\": 44900.0 + i,\n        \"close\": 45050.0 + i,\n        \"volume\": 100.0,\n        \"trades\": 150\n    }\n    for i in range(100)\n]\nstore.insert_klines(\"BTCUSDT\", klines)\n\n# Upsert - updates existing rows\nstore.insert_klines(\"BTCUSDT\", [\n    {\n        \"timestamp\": datetime(2024, 1, 1, 10, 0),\n        \"open\": 45010.0,  # Updated price\n        \"high\": 45110.0,\n        \"low\": 44910.0,\n        \"close\": 45060.0,\n        \"volume\": 101.0,\n        \"trades\": 152\n    }\n])\n</code></pre> Note <p>Empty klines list is silently ignored. Uses executemany for \u226410 rows, Arrow bulk insert for &gt;10 rows. Automatically logs insert count at debug level.</p> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def insert_klines(self, pair: str, klines: list[dict]) -&gt; None:\n    \"\"\"Upsert klines (INSERT OR REPLACE).\n\n    Efficient batch insertion with automatic upsert on (pair, timestamp) conflict.\n    Uses Arrow-based bulk insert for &gt;10 rows for better performance.\n\n    Timestamp normalization:\n        - Removes timezone info\n        - Rounds to minute (removes seconds/microseconds)\n        - If second != 0, rounds up to next minute\n\n    Args:\n        pair (str): Trading pair (e.g., \"BTCUSDT\").\n        klines (list[dict]): List of kline dictionaries. Each must contain:\n            - timestamp (datetime): Bar open time\n            - open (float): Open price\n            - high (float): High price\n            - low (float): Low price\n            - close (float): Close price\n            - volume (float): Trading volume\n            - trades (int, optional): Number of trades\n\n    Example:\n        ```python\n        from datetime import datetime\n\n        # Insert single kline\n        store.insert_klines(\"BTCUSDT\", [\n            {\n                \"timestamp\": datetime(2024, 1, 1, 10, 0),\n                \"open\": 45000.0,\n                \"high\": 45100.0,\n                \"low\": 44900.0,\n                \"close\": 45050.0,\n                \"volume\": 100.5,\n                \"trades\": 150\n            }\n        ])\n\n        # Batch insert (efficient for &gt;10 rows)\n        klines = [\n            {\n                \"timestamp\": datetime(2024, 1, 1, 10, i),\n                \"open\": 45000.0 + i,\n                \"high\": 45100.0 + i,\n                \"low\": 44900.0 + i,\n                \"close\": 45050.0 + i,\n                \"volume\": 100.0,\n                \"trades\": 150\n            }\n            for i in range(100)\n        ]\n        store.insert_klines(\"BTCUSDT\", klines)\n\n        # Upsert - updates existing rows\n        store.insert_klines(\"BTCUSDT\", [\n            {\n                \"timestamp\": datetime(2024, 1, 1, 10, 0),\n                \"open\": 45010.0,  # Updated price\n                \"high\": 45110.0,\n                \"low\": 44910.0,\n                \"close\": 45060.0,\n                \"volume\": 101.0,\n                \"trades\": 152\n            }\n        ])\n        ```\n\n    Note:\n        Empty klines list is silently ignored.\n        Uses executemany for \u226410 rows, Arrow bulk insert for &gt;10 rows.\n        Automatically logs insert count at debug level.\n    \"\"\"\n    if not klines:\n        return\n\n    if len(klines) &lt;= 10:\n        self._con.executemany(\n            \"INSERT OR REPLACE INTO ohlcv VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n            [\n                (\n                    pair,\n                    k[\"timestamp\"],\n                    k[\"open\"],\n                    k[\"high\"],\n                    k[\"low\"],\n                    k[\"close\"],\n                    k[\"volume\"],\n                    k.get(\"trades\"),\n                )\n                for k in klines\n            ],\n        )\n    else:\n        df = pl.DataFrame(\n            {\n                \"pair\": [pair] * len(klines),\n                \"timestamp\": [\n                    k[\"timestamp\"]\n                    .replace(tzinfo=None)\n                    .replace(second=0, microsecond=0)\n                    + timedelta(minutes=1)\n                    if k[\"timestamp\"].second != 0 or k[\"timestamp\"].microsecond != 0\n                    else k[\"timestamp\"].replace(tzinfo=None)\n                    for k in klines\n                ],\n                \"open\": [k[\"open\"] for k in klines],\n                \"high\": [k[\"high\"] for k in klines],\n                \"low\": [k[\"low\"] for k in klines],\n                \"close\": [k[\"close\"] for k in klines],\n                \"volume\": [k[\"volume\"] for k in klines],\n                \"trades\": [k.get(\"trades\") for k in klines],\n            }\n        )\n        self._con.register(\"temp_klines\", df.to_arrow())\n        self._con.execute(\"INSERT OR REPLACE INTO ohlcv SELECT * FROM temp_klines\")\n        self._con.unregister(\"temp_klines\")\n\n    logger.debug(f\"Inserted {len(klines):,} rows for {pair}\")\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.load","title":"load","text":"<pre><code>load(pair: str, hours: Optional[int] = None, start: Optional[datetime] = None, end: Optional[datetime] = None) -&gt; pl.DataFrame\n</code></pre> <p>Load data for a single trading pair.</p> <p>Output columns: pair, timestamp, open, high, low, close, volume, trades</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>str</code> <p>Trading pair (e.g., \"BTCUSDT\").</p> required <code>hours</code> <code>int | None</code> <p>Load last N hours of data. Mutually exclusive with start/end.</p> <code>None</code> <code>start</code> <code>datetime | None</code> <p>Start datetime (inclusive). Requires end parameter.</p> <code>None</code> <code>end</code> <code>datetime | None</code> <p>End datetime (inclusive). Requires start parameter.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: OHLCV data sorted by timestamp. Timezone-naive timestamps.</p> Example <pre><code># Load last 24 hours\ndf = store.load(\"BTCUSDT\", hours=24)\n\n# Load specific range\ndf = store.load(\n    \"BTCUSDT\",\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31)\n)\n\n# Check loaded data\nprint(df.select([\"timestamp\", \"close\"]).head())\n</code></pre> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def load(\n    self,\n    pair: str,\n    hours: Optional[int] = None,\n    start: Optional[datetime] = None,\n    end: Optional[datetime] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Load data for a single trading pair.\n\n    Output columns: pair, timestamp, open, high, low, close, volume, trades\n\n    Args:\n        pair (str): Trading pair (e.g., \"BTCUSDT\").\n        hours (int | None): Load last N hours of data. Mutually exclusive with start/end.\n        start (datetime | None): Start datetime (inclusive). Requires end parameter.\n        end (datetime | None): End datetime (inclusive). Requires start parameter.\n\n    Returns:\n        pl.DataFrame: OHLCV data sorted by timestamp. Timezone-naive timestamps.\n\n    Example:\n        ```python\n        # Load last 24 hours\n        df = store.load(\"BTCUSDT\", hours=24)\n\n        # Load specific range\n        df = store.load(\n            \"BTCUSDT\",\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 1, 31)\n        )\n\n        # Check loaded data\n        print(df.select([\"timestamp\", \"close\"]).head())\n        ```\n    \"\"\"\n    query = \"\"\"\n        SELECT\n            ? AS pair,\n            timestamp, open, high, low, close, volume, trades\n        FROM ohlcv\n        WHERE pair = ?\n    \"\"\"\n    params: list[object] = [pair, pair]\n\n    if hours is not None:\n        query += f\" AND timestamp &gt; NOW() - INTERVAL '{int(hours)}' HOUR\"\n    elif start and end:\n        query += \" AND timestamp BETWEEN ? AND ?\"\n        params.extend([start, end])\n    elif start:\n        query += \" AND timestamp &gt;= ?\"\n        params.append(start)\n    elif end:\n        query += \" AND timestamp &lt;= ?\"\n        params.append(end)\n\n    query += \" ORDER BY timestamp\"\n    df = self._con.execute(query, params).pl()\n\n    if 'timestamp' in df.columns:\n        df = df.with_columns(\n            pl.col('timestamp').dt.replace_time_zone(None)\n        )\n\n    return df\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.load_many","title":"load_many","text":"<pre><code>load_many(pairs: Iterable[str], hours: Optional[int] = None, start: Optional[datetime] = None, end: Optional[datetime] = None) -&gt; pl.DataFrame\n</code></pre> <p>Batch load for multiple pairs.</p> <p>Output columns: pair, timestamp, open, high, low, close, volume, trades</p> <p>More efficient than multiple load() calls due to single query.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>Iterable[str]</code> <p>Trading pairs to load.</p> required <code>hours</code> <code>int | None</code> <p>Load last N hours of data.</p> <code>None</code> <code>start</code> <code>datetime | None</code> <p>Start datetime (inclusive).</p> <code>None</code> <code>end</code> <code>datetime | None</code> <p>End datetime (inclusive).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Combined OHLCV data sorted by (pair, timestamp). Empty DataFrame with correct schema if no pairs provided.</p> Example <pre><code># Load multiple pairs\ndf = store.load_many(\n    pairs=[\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31)\n)\n\n# Analyze by pair\nfor pair in df[\"pair\"].unique():\n    pair_df = df.filter(pl.col(\"pair\") == pair)\n    print(f\"{pair}: {len(pair_df)} bars\")\n</code></pre> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def load_many(\n    self,\n    pairs: Iterable[str],\n    hours: Optional[int] = None,\n    start: Optional[datetime] = None,\n    end: Optional[datetime] = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Batch load for multiple pairs.\n\n    Output columns: pair, timestamp, open, high, low, close, volume, trades\n\n    More efficient than multiple load() calls due to single query.\n\n    Args:\n        pairs (Iterable[str]): Trading pairs to load.\n        hours (int | None): Load last N hours of data.\n        start (datetime | None): Start datetime (inclusive).\n        end (datetime | None): End datetime (inclusive).\n\n    Returns:\n        pl.DataFrame: Combined OHLCV data sorted by (pair, timestamp).\n            Empty DataFrame with correct schema if no pairs provided.\n\n    Example:\n        ```python\n        # Load multiple pairs\n        df = store.load_many(\n            pairs=[\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\"],\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 1, 31)\n        )\n\n        # Analyze by pair\n        for pair in df[\"pair\"].unique():\n            pair_df = df.filter(pl.col(\"pair\") == pair)\n            print(f\"{pair}: {len(pair_df)} bars\")\n        ```\n    \"\"\"\n    pairs = list(pairs)\n    if not pairs:\n        return pl.DataFrame(\n            schema={\n                \"pair\": pl.Utf8,\n                \"timestamp\": pl.Datetime,\n                \"open\": pl.Float64,\n                \"high\": pl.Float64,\n                \"low\": pl.Float64,\n                \"close\": pl.Float64,\n                \"volume\": pl.Float64,\n                \"trades\": pl.Int64,\n            }\n        )\n\n    placeholders = \",\".join([\"?\"] * len(pairs))\n    query = f\"\"\"\n        SELECT\n            pair,\n            timestamp, open, high, low, close, volume, trades\n        FROM ohlcv\n        WHERE pair IN ({placeholders})\n    \"\"\"\n    params: list[object] = [*pairs]\n\n    if hours is not None:\n        query += f\" AND timestamp &gt; NOW() - INTERVAL '{int(hours)}' HOUR\"\n    elif start and end:\n        query += \" AND timestamp BETWEEN ? AND ?\"\n        params.extend([start, end])\n    elif start:\n        query += \" AND timestamp &gt;= ?\"\n        params.append(start)\n    elif end:\n        query += \" AND timestamp &lt;= ?\"\n        params.append(end)\n\n    query += \" ORDER BY pair, timestamp\"\n\n    df = self._con.execute(query, params).pl()\n\n    if 'timestamp' in df.columns:\n        df = df.with_columns(\n            pl.col('timestamp').dt.replace_time_zone(None)\n        )\n\n    return df\n</code></pre>"},{"location":"api/data/#signalflow.data.raw_store.duckdb_stores.DuckDbSpotStore.load_many_pandas","title":"load_many_pandas","text":"<pre><code>load_many_pandas(pairs: list[str], start: datetime | None = None, end: datetime | None = None) -&gt; pd.DataFrame\n</code></pre> <p>Load data for multiple pairs as Pandas DataFrame.</p> <p>Convenience wrapper around load_many() for Pandas compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>pairs</code> <code>list[str]</code> <p>List of trading pairs.</p> required <code>start</code> <code>datetime | None</code> <p>Start datetime (inclusive).</p> <code>None</code> <code>end</code> <code>datetime | None</code> <p>End datetime (inclusive).</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Combined OHLCV data as Pandas DataFrame.</p> Example <pre><code>df = store.load_many_pandas(\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31)\n)\n\n# Use with pandas\ndf[\"returns\"] = df.groupby(\"pair\")[\"close\"].pct_change()\n</code></pre> Source code in <code>src/signalflow/data/raw_store/duckdb_stores.py</code> <pre><code>def load_many_pandas(\n    self,\n    pairs: list[str],\n    start: datetime | None = None,\n    end: datetime | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load data for multiple pairs as Pandas DataFrame.\n\n    Convenience wrapper around load_many() for Pandas compatibility.\n\n    Args:\n        pairs (list[str]): List of trading pairs.\n        start (datetime | None): Start datetime (inclusive).\n        end (datetime | None): End datetime (inclusive).\n\n    Returns:\n        pd.DataFrame: Combined OHLCV data as Pandas DataFrame.\n\n    Example:\n        ```python\n        df = store.load_many_pandas(\n            pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 1, 31)\n        )\n\n        # Use with pandas\n        df[\"returns\"] = df.groupby(\"pair\")[\"close\"].pct_change()\n        ```\n    \"\"\"\n    df_pl = self.load_many(pairs=pairs, start=start, end=end)\n    return df_pl.to_pandas()\n</code></pre>"},{"location":"api/data/#factory","title":"Factory","text":""},{"location":"api/data/#signalflow.data.raw_data_factory.RawDataFactory","title":"signalflow.data.raw_data_factory.RawDataFactory","text":"<p>Factory for creating RawData instances from various sources.</p> <p>Provides static methods to construct RawData objects from different storage backends (DuckDB, Parquet, etc.) with proper validation and schema normalization.</p> Key features <ul> <li>Automatic schema validation</li> <li>Duplicate detection</li> <li>Timezone normalization</li> <li>Column cleanup (remove unnecessary columns)</li> <li>Proper sorting by (pair, timestamp)</li> </ul> Example <pre><code>from signalflow.data import RawDataFactory\nfrom pathlib import Path\nfrom datetime import datetime\n\n# Load spot data from DuckDB\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/binance_spot.duckdb\"),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n    data_types=[\"spot\"]\n)\n\n# Access loaded data\nspot_df = raw_data[\"spot\"]\nprint(f\"Loaded {len(spot_df)} bars\")\nprint(f\"Pairs: {raw_data.pairs}\")\nprint(f\"Date range: {raw_data.datetime_start} to {raw_data.datetime_end}\")\n\n# Use in detector\nfrom signalflow.detector import SmaCrossSignalDetector\n\ndetector = SmaCrossSignalDetector(fast_window=10, slow_window=20)\nsignals = detector.detect(raw_data)\n</code></pre> See Also <p>RawData: Immutable container for raw market data. DuckDbSpotStore: DuckDB storage backend for spot data.</p>"},{"location":"api/data/#signalflow.data.raw_data_factory.RawDataFactory.from_duckdb_spot_store","title":"from_duckdb_spot_store  <code>staticmethod</code>","text":"<pre><code>from_duckdb_spot_store(spot_store_path: Path, pairs: list[str], start: datetime, end: datetime, data_types: list[str] | None = None) -&gt; RawData\n</code></pre> <p>Create RawData from DuckDB spot store.</p> <p>Loads spot trading data from DuckDB storage with validation, deduplication checks, and schema normalization.</p> Processing steps <ol> <li>Load data from DuckDB for specified pairs and date range</li> <li>Validate required columns (pair, timestamp)</li> <li>Remove unnecessary columns (timeframe)</li> <li>Normalize timestamps (microseconds, timezone-naive)</li> <li>Check for duplicates (pair, timestamp)</li> <li>Sort by (pair, timestamp)</li> <li>Package into RawData container</li> </ol> <p>Parameters:</p> Name Type Description Default <code>spot_store_path</code> <code>Path</code> <p>Path to DuckDB file.</p> required <code>pairs</code> <code>list[str]</code> <p>List of trading pairs to load (e.g., [\"BTCUSDT\", \"ETHUSDT\"]).</p> required <code>start</code> <code>datetime</code> <p>Start datetime (inclusive).</p> required <code>end</code> <code>datetime</code> <p>End datetime (inclusive).</p> required <code>data_types</code> <code>list[str] | None</code> <p>Data types to load. Default: None. Currently supports: [\"spot\"].</p> <code>None</code> <p>Returns:</p> Name Type Description <code>RawData</code> <code>RawData</code> <p>Immutable container with loaded and validated data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns missing (pair, timestamp).</p> <code>ValueError</code> <p>If duplicate (pair, timestamp) combinations detected.</p> Example <pre><code>from pathlib import Path\nfrom datetime import datetime\nfrom signalflow.data import RawDataFactory\n\n# Load single pair\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/binance.duckdb\"),\n    pairs=[\"BTCUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 1, 31),\n    data_types=[\"spot\"]\n)\n\n# Load multiple pairs\nraw_data = RawDataFactory.from_duckdb_spot_store(\n    spot_store_path=Path(\"data/binance.duckdb\"),\n    pairs=[\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\"],\n    start=datetime(2024, 1, 1),\n    end=datetime(2024, 12, 31),\n    data_types=[\"spot\"]\n)\n\n# Check loaded data\nspot_df = raw_data[\"spot\"]\nprint(f\"Shape: {spot_df.shape}\")\nprint(f\"Columns: {spot_df.columns}\")\nprint(f\"Pairs: {spot_df['pair'].unique().to_list()}\")\n\n# Verify no duplicates\ndup_check = (\n    spot_df.group_by([\"pair\", \"timestamp\"])\n    .len()\n    .filter(pl.col(\"len\") &gt; 1)\n)\nassert dup_check.is_empty()\n\n# Use in pipeline\nfrom signalflow.core import RawDataView\nview = RawDataView(raw=raw_data)\nspot_pandas = view.to_pandas(\"spot\")\n</code></pre> Example <pre><code># Handle missing data gracefully\ntry:\n    raw_data = RawDataFactory.from_duckdb_spot_store(\n        spot_store_path=Path(\"data/binance.duckdb\"),\n        pairs=[\"BTCUSDT\"],\n        start=datetime(2024, 1, 1),\n        end=datetime(2024, 1, 31),\n        data_types=[\"spot\"]\n    )\nexcept ValueError as e:\n    if \"missing columns\" in str(e):\n        print(\"Data schema invalid\")\n    elif \"Duplicate\" in str(e):\n        print(\"Data contains duplicates\")\n    raise\n\n# Validate date range\nassert raw_data.datetime_start == datetime(2024, 1, 1)\nassert raw_data.datetime_end == datetime(2024, 1, 31)\n\n# Check data quality\nspot_df = raw_data[\"spot\"]\n\n# Verify timestamps are sorted\nassert spot_df[\"timestamp\"].is_sorted()\n\n# Verify timezone-naive\nassert spot_df[\"timestamp\"].dtype == pl.Datetime(\"us\")\n\n# Verify no nulls in key columns\nassert spot_df[\"pair\"].null_count() == 0\nassert spot_df[\"timestamp\"].null_count() == 0\n</code></pre> Note <p>Store connection is automatically closed via finally block. Timestamps are normalized to timezone-naive microseconds. Duplicate detection shows first 10 examples if found. All data sorted by (pair, timestamp) for consistent ordering.</p> Source code in <code>src/signalflow/data/raw_data_factory.py</code> <pre><code>@staticmethod\ndef from_duckdb_spot_store(\n    spot_store_path: Path,\n    pairs: list[str],\n    start: datetime,\n    end: datetime,\n    data_types: list[str] | None = None,\n) -&gt; RawData:\n    \"\"\"Create RawData from DuckDB spot store.\n\n    Loads spot trading data from DuckDB storage with validation,\n    deduplication checks, and schema normalization.\n\n    Processing steps:\n        1. Load data from DuckDB for specified pairs and date range\n        2. Validate required columns (pair, timestamp)\n        3. Remove unnecessary columns (timeframe)\n        4. Normalize timestamps (microseconds, timezone-naive)\n        5. Check for duplicates (pair, timestamp)\n        6. Sort by (pair, timestamp)\n        7. Package into RawData container\n\n    Args:\n        spot_store_path (Path): Path to DuckDB file.\n        pairs (list[str]): List of trading pairs to load (e.g., [\"BTCUSDT\", \"ETHUSDT\"]).\n        start (datetime): Start datetime (inclusive).\n        end (datetime): End datetime (inclusive).\n        data_types (list[str] | None): Data types to load. Default: None.\n            Currently supports: [\"spot\"].\n\n    Returns:\n        RawData: Immutable container with loaded and validated data.\n\n    Raises:\n        ValueError: If required columns missing (pair, timestamp).\n        ValueError: If duplicate (pair, timestamp) combinations detected.\n\n    Example:\n        ```python\n        from pathlib import Path\n        from datetime import datetime\n        from signalflow.data import RawDataFactory\n\n        # Load single pair\n        raw_data = RawDataFactory.from_duckdb_spot_store(\n            spot_store_path=Path(\"data/binance.duckdb\"),\n            pairs=[\"BTCUSDT\"],\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 1, 31),\n            data_types=[\"spot\"]\n        )\n\n        # Load multiple pairs\n        raw_data = RawDataFactory.from_duckdb_spot_store(\n            spot_store_path=Path(\"data/binance.duckdb\"),\n            pairs=[\"BTCUSDT\", \"ETHUSDT\", \"BNBUSDT\"],\n            start=datetime(2024, 1, 1),\n            end=datetime(2024, 12, 31),\n            data_types=[\"spot\"]\n        )\n\n        # Check loaded data\n        spot_df = raw_data[\"spot\"]\n        print(f\"Shape: {spot_df.shape}\")\n        print(f\"Columns: {spot_df.columns}\")\n        print(f\"Pairs: {spot_df['pair'].unique().to_list()}\")\n\n        # Verify no duplicates\n        dup_check = (\n            spot_df.group_by([\"pair\", \"timestamp\"])\n            .len()\n            .filter(pl.col(\"len\") &gt; 1)\n        )\n        assert dup_check.is_empty()\n\n        # Use in pipeline\n        from signalflow.core import RawDataView\n        view = RawDataView(raw=raw_data)\n        spot_pandas = view.to_pandas(\"spot\")\n        ```\n\n    Example:\n        ```python\n        # Handle missing data gracefully\n        try:\n            raw_data = RawDataFactory.from_duckdb_spot_store(\n                spot_store_path=Path(\"data/binance.duckdb\"),\n                pairs=[\"BTCUSDT\"],\n                start=datetime(2024, 1, 1),\n                end=datetime(2024, 1, 31),\n                data_types=[\"spot\"]\n            )\n        except ValueError as e:\n            if \"missing columns\" in str(e):\n                print(\"Data schema invalid\")\n            elif \"Duplicate\" in str(e):\n                print(\"Data contains duplicates\")\n            raise\n\n        # Validate date range\n        assert raw_data.datetime_start == datetime(2024, 1, 1)\n        assert raw_data.datetime_end == datetime(2024, 1, 31)\n\n        # Check data quality\n        spot_df = raw_data[\"spot\"]\n\n        # Verify timestamps are sorted\n        assert spot_df[\"timestamp\"].is_sorted()\n\n        # Verify timezone-naive\n        assert spot_df[\"timestamp\"].dtype == pl.Datetime(\"us\")\n\n        # Verify no nulls in key columns\n        assert spot_df[\"pair\"].null_count() == 0\n        assert spot_df[\"timestamp\"].null_count() == 0\n        ```\n\n    Note:\n        Store connection is automatically closed via finally block.\n        Timestamps are normalized to timezone-naive microseconds.\n        Duplicate detection shows first 10 examples if found.\n        All data sorted by (pair, timestamp) for consistent ordering.\n    \"\"\"\n    data: dict[str, pl.DataFrame] = {}\n    store = DuckDbSpotStore(spot_store_path)\n    try:\n        if \"spot\" in data_types:\n            spot = store.load_many(pairs=pairs, start=start, end=end)\n\n            required = {\"pair\", \"timestamp\"}\n            missing = required - set(spot.columns)\n            if missing:\n                raise ValueError(f\"Spot df missing columns: {sorted(missing)}\")\n\n            if \"timeframe\" in spot.columns:\n                spot = spot.drop(\"timeframe\")\n\n            spot = spot.with_columns(\n                pl.col(\"timestamp\").cast(pl.Datetime(\"us\")).dt.replace_time_zone(None)\n            )\n\n            dup_count = (\n                spot.group_by([\"pair\", \"timestamp\"]).len()\n                .filter(pl.col(\"len\") &gt; 1)\n            )\n            if dup_count.height &gt; 0:\n                dups = (\n                    spot.join(\n                        dup_count.select([\"pair\", \"timestamp\"]),\n                        on=[\"pair\", \"timestamp\"],\n                    )\n                    .select([\"pair\", \"timestamp\"])\n                    .head(10)\n                )\n                raise ValueError(\n                    f\"Duplicate (pair, timestamp) detected. Examples:\\n{dups}\"\n                )\n\n            spot = spot.sort([\"pair\", \"timestamp\"])\n            data[\"spot\"] = spot\n\n        return RawData(\n            datetime_start=start,\n            datetime_end=end,\n            pairs=pairs,\n            data=data,\n        )\n    finally:\n        store.close()\n</code></pre>"},{"location":"api/detector/","title":"Detector Module","text":""},{"location":"api/detector/#signalflow.detector.base.SignalDetector","title":"signalflow.detector.base.SignalDetector  <code>dataclass</code>","text":"<pre><code>SignalDetector(pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType = RawDataType.SPOT, feature_set: FeatureSet | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for Polars-first signal detection.</p> Provides standardized pipeline for detecting trading signals from raw data <ol> <li>preprocess: Extract features from raw data</li> <li>detect: Generate signals from features</li> <li>validate: Ensure data quality</li> </ol> Key features <ul> <li>Polars-native for performance</li> <li>Automatic feature extraction via FeatureSet</li> <li>Built-in validation (schema, duplicates, timezones)</li> <li>Optional probability requirement</li> <li>Keep latest signal per pair option</li> </ul> Public API <ul> <li>run(): Complete pipeline (preprocess \u2192 detect \u2192 validate)</li> <li>preprocess(): Feature extraction (delegates to FeatureSet)</li> <li>detect(): Signal generation (must implement)</li> </ul> <p>Attributes:</p> Name Type Description <code>component_type</code> <code>ClassVar[SfComponentType]</code> <p>Always DETECTOR for registry.</p> <code>pair_col</code> <code>str</code> <p>Trading pair column name. Default: \"pair\".</p> <code>ts_col</code> <code>str</code> <p>Timestamp column name. Default: \"timestamp\".</p> <code>raw_data_type</code> <code>RawDataType</code> <p>Type of raw data to process. Default: SPOT.</p> <code>feature_set</code> <code>FeatureSet | None</code> <p>Feature extractor. Default: None.</p> <code>require_probability</code> <code>bool</code> <p>Require probability column in signals. Default: False.</p> <code>keep_only_latest_per_pair</code> <code>bool</code> <p>Keep only latest signal per pair. Default: False.</p> Example <pre><code>from signalflow.detector import SignalDetector\nfrom signalflow.core import Signals, SignalType\nimport polars as pl\n\nclass SmaCrossDetector(SignalDetector):\n    '''Simple SMA crossover detector'''\n\n    def __init__(self, fast_window: int = 10, slow_window: int = 20):\n        super().__init__()\n        # Auto-generate features\n        from signalflow.feature import FeatureSet, SmaExtractor\n        self.feature_set = FeatureSet([\n            SmaExtractor(window=fast_window, column=\"close\"),\n            SmaExtractor(window=slow_window, column=\"close\")\n        ])\n\n    def detect(self, features: pl.DataFrame, context=None) -&gt; Signals:\n        signals = features.with_columns([\n            # Detect crossover\n            (pl.col(\"sma_10\") &gt; pl.col(\"sma_20\")).alias(\"is_bull\"),\n            (pl.col(\"sma_10\") &lt; pl.col(\"sma_20\")).alias(\"is_bear\")\n        ]).with_columns([\n            # Assign signal type\n            pl.when(pl.col(\"is_bull\"))\n            .then(pl.lit(SignalType.RISE.value))\n            .when(pl.col(\"is_bear\"))\n            .then(pl.lit(SignalType.FALL.value))\n            .otherwise(pl.lit(SignalType.NONE.value))\n            .alias(\"signal_type\")\n        ]).select([\n            self.pair_col,\n            self.ts_col,\n            \"signal_type\",\n            pl.lit(1).alias(\"signal\")\n        ])\n\n        return Signals(signals)\n\n# Usage\ndetector = SmaCrossDetector(fast_window=10, slow_window=20)\nsignals = detector.run(raw_data_view)\n</code></pre> Note <p>Subclasses must implement detect() method. All DataFrames must use timezone-naive timestamps. Duplicate (pair, timestamp) combinations are rejected.</p> See Also <p>FeatureSet: Orchestrates feature extraction. Signals: Container for signal output.</p>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.__call__","title":"__call__  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>__call__ = run\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = DETECTOR\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.feature_set","title":"feature_set  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_set: FeatureSet | None = None\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.keep_only_latest_per_pair","title":"keep_only_latest_per_pair  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keep_only_latest_per_pair: bool = False\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.raw_data_type","title":"raw_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_type: RawDataType = SPOT\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.require_probability","title":"require_probability  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>require_probability: bool = False\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._keep_only_latest","title":"_keep_only_latest","text":"<pre><code>_keep_only_latest(signals: Signals) -&gt; Signals\n</code></pre> <p>Keep only latest signal per pair.</p> <p>Useful for strategies that only trade most recent signal.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals.</p> required <p>Returns:</p> Name Type Description <code>Signals</code> <code>Signals</code> <p>Filtered signals with one per pair.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _keep_only_latest(self, signals: Signals) -&gt; Signals:\n    \"\"\"Keep only latest signal per pair.\n\n    Useful for strategies that only trade most recent signal.\n\n    Args:\n        signals (Signals): Input signals.\n\n    Returns:\n        Signals: Filtered signals with one per pair.\n    \"\"\"\n    s = signals.value\n    out = (\n        s.sort([self.pair_col, self.ts_col])\n        .group_by(self.pair_col, maintain_order=True)\n        .tail(1)\n        .sort([self.pair_col, self.ts_col])\n    )\n    return Signals(out)\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._normalize_index","title":"_normalize_index","text":"<pre><code>_normalize_index(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Normalize timestamps to timezone-naive.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input DataFrame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame with timezone-naive timestamps.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df is not pl.DataFrame.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _normalize_index(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Normalize timestamps to timezone-naive.\n\n    Args:\n        df (pl.DataFrame): Input DataFrame.\n\n    Returns:\n        pl.DataFrame: DataFrame with timezone-naive timestamps.\n\n    Raises:\n        TypeError: If df is not pl.DataFrame.\n    \"\"\"\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"Expected pl.DataFrame, got {type(df)}\")\n\n    if self.ts_col in df.columns:\n        ts_dtype = df.schema.get(self.ts_col)\n        if isinstance(ts_dtype, pl.Datetime) and ts_dtype.time_zone is not None:\n            df = df.with_columns(pl.col(self.ts_col).dt.replace_time_zone(None))\n    return df\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._validate_features","title":"_validate_features","text":"<pre><code>_validate_features(df: DataFrame) -&gt; None\n</code></pre> <p>Validate feature DataFrame.</p> Checks <ul> <li>Is pl.DataFrame</li> <li>Has required columns (pair, timestamp)</li> <li>Timestamps are timezone-naive</li> <li>No duplicate (pair, timestamp) combinations</li> </ul> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Features to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If not pl.DataFrame.</p> <code>ValueError</code> <p>If validation fails.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _validate_features(self, df: pl.DataFrame) -&gt; None:\n    \"\"\"Validate feature DataFrame.\n\n    Checks:\n        - Is pl.DataFrame\n        - Has required columns (pair, timestamp)\n        - Timestamps are timezone-naive\n        - No duplicate (pair, timestamp) combinations\n\n    Args:\n        df (pl.DataFrame): Features to validate.\n\n    Raises:\n        TypeError: If not pl.DataFrame.\n        ValueError: If validation fails.\n    \"\"\"\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"preprocess must return polars.DataFrame, got {type(df)}\")\n\n    missing = [c for c in (self.pair_col, self.ts_col) if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Features missing required columns: {missing}\")\n\n    ts_dtype = df.schema.get(self.ts_col)\n    if isinstance(ts_dtype, pl.Datetime) and ts_dtype.time_zone is not None:\n        raise ValueError(\n            f\"Features column '{self.ts_col}' must be timezone-naive, got tz={ts_dtype.time_zone}. \"\n            f\"Use .dt.replace_time_zone(None).\"\n        )\n\n    dup = (\n        df.group_by([self.pair_col, self.ts_col])\n        .len()\n        .filter(pl.col(\"len\") &gt; 1)\n    )\n    if dup.height &gt; 0:\n        raise ValueError(\n            \"Features contain duplicate keys (pair,timestamp). \"\n            f\"Examples:\\n{dup.select([self.pair_col, self.ts_col]).head(10)}\"\n        )\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector._validate_signals","title":"_validate_signals","text":"<pre><code>_validate_signals(signals: Signals) -&gt; None\n</code></pre> <p>Validate signal output.</p> Checks <ul> <li>Is Signals instance with pl.DataFrame value</li> <li>Has required columns (pair, timestamp, signal_type)</li> <li>signal_type values are valid SignalType enums</li> <li>Timestamps are timezone-naive</li> <li>No duplicate (pair, timestamp) combinations</li> <li>(optional) Has probability column if required</li> </ul> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Signals to validate.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If not Signals or value not pl.DataFrame.</p> <code>ValueError</code> <p>If validation fails.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def _validate_signals(self, signals: Signals) -&gt; None:\n    \"\"\"Validate signal output.\n\n    Checks:\n        - Is Signals instance with pl.DataFrame value\n        - Has required columns (pair, timestamp, signal_type)\n        - signal_type values are valid SignalType enums\n        - Timestamps are timezone-naive\n        - No duplicate (pair, timestamp) combinations\n        - (optional) Has probability column if required\n\n    Args:\n        signals (Signals): Signals to validate.\n\n    Raises:\n        TypeError: If not Signals or value not pl.DataFrame.\n        ValueError: If validation fails.\n    \"\"\"\n    if not isinstance(signals, Signals):\n        raise TypeError(f\"detect must return Signals, got {type(signals)}\")\n\n    s = signals.value\n    if not isinstance(s, pl.DataFrame):\n        raise TypeError(f\"Signals.value must be polars.DataFrame, got {type(s)}\")\n\n    required = {self.pair_col, self.ts_col, \"signal_type\"}\n    missing = sorted(required - set(s.columns))\n    if missing:\n        raise ValueError(f\"Signals missing required columns: {missing}\")\n\n    allowed = {t.value for t in SignalType}\n    bad = (\n        s.select(pl.col(\"signal_type\"))\n        .unique()\n        .filter(~pl.col(\"signal_type\").is_in(list(allowed)))\n    )\n    if bad.height &gt; 0:\n        raise ValueError(\n            f\"Signals contain unknown signal_type values: {bad.get_column('signal_type').to_list()}\"\n        )\n\n    if self.require_probability and \"probability\" not in s.columns:\n        raise ValueError(\"Signals must contain 'probability' column (require_probability=True)\")\n\n    ts_dtype = s.schema.get(self.ts_col)\n    if isinstance(ts_dtype, pl.Datetime) and ts_dtype.time_zone is not None:\n        raise ValueError(f\"Signals column '{self.ts_col}' must be timezone-naive, got tz={ts_dtype.time_zone}.\")\n\n    # optional: hard guarantee no duplicates in signals\n    dup = (\n        s.group_by([self.pair_col, self.ts_col])\n        .len()\n        .filter(pl.col(\"len\") &gt; 1)\n    )\n    if dup.height &gt; 0:\n        raise ValueError(\n            \"Signals contain duplicate keys (pair,timestamp). \"\n            f\"Examples:\\n{dup.select([self.pair_col, self.ts_col]).head(10)}\"\n        )\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.detect","title":"detect  <code>abstractmethod</code>","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Generate signals from features.</p> <p>Core detection logic - must be implemented by subclasses.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>DataFrame</code> <p>Preprocessed features.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Signals</code> <code>Signals</code> <p>Detected signals with columns: - pair (str): Trading pair - timestamp (datetime): Signal timestamp (timezone-naive) - signal_type (int): SignalType enum value - signal (int | float): Signal value - probability (float, optional): Signal probability</p> Example <pre><code>def detect(self, features, context=None):\n    # Simple threshold detector\n    signals = features.filter(\n        pl.col(\"rsi\") &gt; 70  # Overbought\n    ).with_columns([\n        pl.lit(SignalType.FALL.value).alias(\"signal_type\"),\n        pl.lit(-1).alias(\"signal\"),\n        pl.lit(0.8).alias(\"probability\")\n    ]).select([\n        self.pair_col,\n        self.ts_col,\n        \"signal_type\",\n        \"signal\",\n        \"probability\"\n    ])\n\n    return Signals(signals)\n</code></pre> Note <p>Must return Signals with at minimum: pair, timestamp, signal_type. Timestamps must be timezone-naive. No duplicate (pair, timestamp) combinations allowed.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>@abstractmethod\ndef detect(self, features: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; Signals:\n    \"\"\"Generate signals from features.\n\n    Core detection logic - must be implemented by subclasses.\n\n    Args:\n        features (pl.DataFrame): Preprocessed features.\n        context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        Signals: Detected signals with columns:\n            - pair (str): Trading pair\n            - timestamp (datetime): Signal timestamp (timezone-naive)\n            - signal_type (int): SignalType enum value\n            - signal (int | float): Signal value\n            - probability (float, optional): Signal probability\n\n    Example:\n        ```python\n        def detect(self, features, context=None):\n            # Simple threshold detector\n            signals = features.filter(\n                pl.col(\"rsi\") &gt; 70  # Overbought\n            ).with_columns([\n                pl.lit(SignalType.FALL.value).alias(\"signal_type\"),\n                pl.lit(-1).alias(\"signal\"),\n                pl.lit(0.8).alias(\"probability\")\n            ]).select([\n                self.pair_col,\n                self.ts_col,\n                \"signal_type\",\n                \"signal\",\n                \"probability\"\n            ])\n\n            return Signals(signals)\n        ```\n\n    Note:\n        Must return Signals with at minimum: pair, timestamp, signal_type.\n        Timestamps must be timezone-naive.\n        No duplicate (pair, timestamp) combinations allowed.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.preprocess","title":"preprocess","text":"<pre><code>preprocess(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Extract features from raw data.</p> <p>Default implementation delegates to FeatureSet. Override for custom feature extraction logic.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Features with at minimum pair and timestamp columns.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If feature_set is None and not overridden.</p> <code>TypeError</code> <p>If FeatureSet doesn't return pl.DataFrame.</p> Example <pre><code># Default: uses FeatureSet\nfeatures = detector.preprocess(raw_data_view)\n\n# Custom override\nclass CustomDetector(SignalDetector):\n    def preprocess(self, raw_data_view, context=None):\n        df = raw_data_view.to_polars(\"spot\")\n        return df.with_columns([\n            pl.col(\"close\").rolling_mean(10).alias(\"sma_10\")\n        ])\n</code></pre> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def preprocess(self, raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Extract features from raw data.\n\n    Default implementation delegates to FeatureSet. Override for custom\n    feature extraction logic.\n\n    Args:\n        raw_data_view (RawDataView): View to raw market data.\n        context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Features with at minimum pair and timestamp columns.\n\n    Raises:\n        NotImplementedError: If feature_set is None and not overridden.\n        TypeError: If FeatureSet doesn't return pl.DataFrame.\n\n    Example:\n        ```python\n        # Default: uses FeatureSet\n        features = detector.preprocess(raw_data_view)\n\n        # Custom override\n        class CustomDetector(SignalDetector):\n            def preprocess(self, raw_data_view, context=None):\n                df = raw_data_view.to_polars(\"spot\")\n                return df.with_columns([\n                    pl.col(\"close\").rolling_mean(10).alias(\"sma_10\")\n                ])\n        ```\n    \"\"\"\n    if self.feature_set is None:\n        raise NotImplementedError(\n            f\"{self.__class__.__name__}.preprocess is not implemented and feature_set is None\"\n        )\n    out = self.feature_set.extract(raw_data_view, context=context)\n    if not isinstance(out, pl.DataFrame):\n        raise TypeError(f\"{self.__class__.__name__}.feature_set.extract must return pl.DataFrame, got {type(out)}\")\n    return out\n</code></pre>"},{"location":"api/detector/#signalflow.detector.base.SignalDetector.run","title":"run","text":"<pre><code>run(raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> <p>Execute complete detection pipeline.</p> Pipeline steps <ol> <li>preprocess: Extract features</li> <li>normalize: Ensure timezone-naive timestamps</li> <li>validate features: Check schema and duplicates</li> <li>detect: Generate signals</li> <li>validate signals: Check output quality</li> <li>(optional) keep latest: Filter to latest per pair</li> </ol> <p>Parameters:</p> Name Type Description Default <code>raw_data_view</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context for detection.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Signals</code> <code>Signals</code> <p>Detected signals.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If preprocess doesn't return pl.DataFrame.</p> <code>ValueError</code> <p>If features/signals fail validation.</p> Example <pre><code>from signalflow.core import RawData, RawDataView\n\n# Create view\nview = RawDataView(raw=raw_data)\n\n# Run detection\nsignals = detector.run(view)\n\n# With context\nsignals = detector.run(view, context={\"threshold\": 0.7})\n</code></pre> Note <p>Can also be called directly: detector(raw_data_view). All validation errors include helpful diagnostic information.</p> Source code in <code>src/signalflow/detector/base.py</code> <pre><code>def run(self, raw_data_view: RawDataView, context: dict[str, Any] | None = None) -&gt; Signals:\n    \"\"\"Execute complete detection pipeline.\n\n    Pipeline steps:\n        1. preprocess: Extract features\n        2. normalize: Ensure timezone-naive timestamps\n        3. validate features: Check schema and duplicates\n        4. detect: Generate signals\n        5. validate signals: Check output quality\n        6. (optional) keep latest: Filter to latest per pair\n\n    Args:\n        raw_data_view (RawDataView): View to raw market data.\n        context (dict[str, Any] | None): Additional context for detection.\n\n    Returns:\n        Signals: Detected signals.\n\n    Raises:\n        TypeError: If preprocess doesn't return pl.DataFrame.\n        ValueError: If features/signals fail validation.\n\n    Example:\n        ```python\n        from signalflow.core import RawData, RawDataView\n\n        # Create view\n        view = RawDataView(raw=raw_data)\n\n        # Run detection\n        signals = detector.run(view)\n\n        # With context\n        signals = detector.run(view, context={\"threshold\": 0.7})\n        ```\n\n    Note:\n        Can also be called directly: detector(raw_data_view).\n        All validation errors include helpful diagnostic information.\n    \"\"\"\n    feats = self.preprocess(raw_data_view, context=context)\n    feats = self._normalize_index(feats)\n    self._validate_features(feats)\n\n    signals = self.detect(feats, context=context)\n    self._validate_signals(signals)\n\n    if self.keep_only_latest_per_pair:\n        signals = self._keep_only_latest(signals)\n\n    return signals\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector","title":"signalflow.detector.sma_cross.SmaCrossSignalDetector  <code>dataclass</code>","text":"<pre><code>SmaCrossSignalDetector(pair_col: str = 'pair', ts_col: str = 'timestamp', raw_data_type: RawDataType = RawDataType.SPOT, feature_set: FeatureSet | None = None, require_probability: bool = False, keep_only_latest_per_pair: bool = False, fast_period: int = 20, slow_period: int = 50, price_col: str = 'close', fast_col: str | None = None, slow_col: str | None = None)\n</code></pre> <p>               Bases: <code>SignalDetector</code></p> <p>SMA crossover signal detector.</p> <p>Signal rules (per pair, per timestamp):   - RISE  : fast crosses above slow  (fast_t &gt; slow_t) and (fast_{t-1} &lt;= slow_{t-1})   - FALL  : fast crosses below slow  (fast_t &lt; slow_t) and (fast_{t-1} &gt;= slow_{t-1})   - NONE  : otherwise</p> Output Signals columns <ul> <li>pair, timestamp, signal_type, signal</li> <li>signal: +1 for RISE, -1 for FALL, 0 for NONE</li> </ul>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector.fast_col","title":"fast_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fast_col: str | None = None\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector.fast_period","title":"fast_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fast_period: int = 20\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector.slow_col","title":"slow_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>slow_col: str | None = None\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector.slow_period","title":"slow_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>slow_period: int = 50\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/detector/sma_cross.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.fast_period &lt;= 0 or self.slow_period &lt;= 0:\n        raise ValueError(\"fast_period and slow_period must be &gt; 0\")\n    if self.fast_period &gt;= self.slow_period:\n        raise ValueError(f\"fast_period must be &lt; slow_period, got {self.fast_period} &gt;= {self.slow_period}\")\n\n    self.fast_col = self.fast_col or f\"sma_{self.fast_period}\"\n    self.slow_col = self.slow_col or f\"sma_{self.slow_period}\"\n\n    self.feature_set = FeatureSet(\n        extractors=[\n            SmaExtractor(\n                offset_window=1,\n                sma_period=self.fast_period,\n                price_col=self.price_col,\n                out_col=self.fast_col,\n                use_resample=True,\n                raw_data_type=RawDataType.SPOT,\n            ),\n            SmaExtractor(\n                offset_window=1,\n                sma_period=self.slow_period,\n                price_col=self.price_col,\n                out_col=self.slow_col,\n                use_resample=True,\n                raw_data_type=RawDataType.SPOT,\n            ),\n        ]\n    )\n</code></pre>"},{"location":"api/detector/#signalflow.detector.sma_cross.SmaCrossSignalDetector.detect","title":"detect","text":"<pre><code>detect(features: DataFrame, context: dict[str, Any] | None = None) -&gt; Signals\n</code></pre> Source code in <code>src/signalflow/detector/sma_cross.py</code> <pre><code>def detect(self, features: pl.DataFrame, context: dict[str, Any] | None = None) -&gt; Signals:\n    df = features.sort([self.pair_col, self.ts_col])\n\n    if self.fast_col not in df.columns or self.slow_col not in df.columns:\n        raise ValueError(\n            f\"Expected columns '{self.fast_col}' and '{self.slow_col}' in features. \"\n            f\"Got: {df.columns}\"\n        )\n\n    df = df.filter(pl.col(self.fast_col).is_not_null() &amp; pl.col(self.slow_col).is_not_null())\n\n    fast = pl.col(self.fast_col)\n    slow = pl.col(self.slow_col)\n\n    fast_prev = fast.shift(1).over(self.pair_col)\n    slow_prev = slow.shift(1).over(self.pair_col)\n\n    cross_up = (fast &gt; slow) &amp; (fast_prev &lt;= slow_prev)\n    cross_down = (fast &lt; slow) &amp; (fast_prev &gt;= slow_prev)\n\n    out = (\n        df.select([self.pair_col, self.ts_col, self.fast_col, self.slow_col])\n        .with_columns(\n            pl.when(cross_up)\n            .then(pl.lit(SignalType.RISE.value))\n            .when(cross_down)\n            .then(pl.lit(SignalType.FALL.value))\n            .otherwise(pl.lit(SignalType.NONE.value))\n            .alias(\"signal_type\")\n        )\n        .with_columns(\n            pl.when(pl.col(\"signal_type\") == SignalType.RISE.value).then(pl.lit(1))\n            .when(pl.col(\"signal_type\") == SignalType.FALL.value).then(pl.lit(-1))\n            .otherwise(pl.lit(0))\n            .alias(\"signal\")\n        )\n    )\n\n    return Signals(out.select([self.pair_col, self.ts_col, \"signal_type\", \"signal\"]))\n</code></pre>"},{"location":"api/feature/","title":"Feature Module","text":"<p>Feature extraction for technical indicators and derived metrics.</p>"},{"location":"api/feature/#base-classes","title":"Base Classes","text":""},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor","title":"signalflow.feature.base.FeatureExtractor  <code>dataclass</code>","text":"<pre><code>FeatureExtractor(offset_window: int = 1, compute_last_offset: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp', offset_col: str = 'resample_offset', use_resample: bool = False, resample_mode: Literal['add', 'replace'] = 'add', resample_prefix: str | None = None, raw_data_type: RawDataType = RawDataType.SPOT, keep_input_columns: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for Polars-first feature extraction.</p> <p>Extracts technical indicators and derived features from raw OHLCV data with optional sliding window resampling for multi-timeframe features.</p> Key features <ul> <li>Polars-native for performance</li> <li>Optional sliding window resampling (e.g., 5m bars from 1m bars)</li> <li>Per-pair, per-offset-window processing</li> <li>Length-preserving operations</li> <li>Automatic projection (keep only new features)</li> </ul> Processing pipeline <ol> <li>Sort by (pair, timestamp)</li> <li>Add resample_offset column</li> <li>(optional) Apply sliding resample</li> <li>(optional) Filter to last offset</li> <li>Group by (pair, resample_offset) and compute features</li> <li>Sort output</li> <li>Project columns (keep input or features only)</li> </ol> <p>Attributes:</p> Name Type Description <code>offset_window</code> <code>int</code> <p>Sliding window size in bars. Default: 1.</p> <code>compute_last_offset</code> <code>bool</code> <p>Keep only last offset. Default: False.</p> <code>pair_col</code> <code>str</code> <p>Trading pair column. Default: \"pair\".</p> <code>ts_col</code> <code>str</code> <p>Timestamp column. Default: \"timestamp\".</p> <code>offset_col</code> <code>str</code> <p>Offset tracking column. Default: \"resample_offset\".</p> <code>use_resample</code> <code>bool</code> <p>Apply sliding resample. Default: False.</p> <code>resample_mode</code> <code>Literal['add', 'replace']</code> <p>Resample mode. Default: \"add\".</p> <code>resample_prefix</code> <code>str | None</code> <p>Prefix for resampled columns. Default: None.</p> <code>raw_data_type</code> <code>RawDataType</code> <p>Type of raw data. Default: SPOT.</p> <code>component_type</code> <code>ClassVar[SfComponentType]</code> <p>Always FEATURE_EXTRACTOR.</p> <code>keep_input_columns</code> <code>bool</code> <p>Keep all input columns. Default: False.</p> Example <pre><code>from signalflow.feature import FeatureExtractor\nimport polars as pl\n\nclass RsiExtractor(FeatureExtractor):\n    '''RSI indicator extractor'''\n\n    def __init__(self, window: int = 14, column: str = \"close\"):\n        super().__init__()\n        self.window = window\n        self.column = column\n\n    def compute_group(self, group_df, data_context=None):\n        # Compute RSI per group\n        delta = group_df.select(pl.col(self.column).diff().alias(\"delta\"))\n\n        gain = delta.select(\n            pl.when(pl.col(\"delta\") &gt; 0)\n            .then(pl.col(\"delta\"))\n            .otherwise(0)\n            .alias(\"gain\")\n        )\n\n        loss = delta.select(\n            pl.when(pl.col(\"delta\") &lt; 0)\n            .then(-pl.col(\"delta\"))\n            .otherwise(0)\n            .alias(\"loss\")\n        )\n\n        avg_gain = gain.select(\n            pl.col(\"gain\").rolling_mean(self.window).alias(\"avg_gain\")\n        )\n\n        avg_loss = loss.select(\n            pl.col(\"loss\").rolling_mean(self.window).alias(\"avg_loss\")\n        )\n\n        rs = avg_gain.select(\n            (pl.col(\"avg_gain\") / pl.col(\"avg_loss\")).alias(\"rs\")\n        )\n\n        rsi = group_df.with_columns([\n            (100 - (100 / (1 + rs.get_column(\"rs\")))).alias(f\"rsi_{self.window}\")\n        ])\n\n        return rsi\n\n# Usage\nextractor = RsiExtractor(window=14)\nfeatures = extractor.extract(ohlcv_df)\n</code></pre> Note <p>compute_group() must preserve row count (length-preserving). All timestamps must be timezone-naive. For multi-timeframe features, use use_resample=True.</p> See Also <p>RollingAggregator: Sliding window resampler. FeatureSet: Orchestrates multiple extractors.</p>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor._resampler","title":"_resampler  <code>property</code>","text":"<pre><code>_resampler: RollingAggregator\n</code></pre> <p>Get configured RollingAggregator instance.</p> <p>Returns:</p> Name Type Description <code>RollingAggregator</code> <code>RollingAggregator</code> <p>Resampler with current configuration.</p>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = FEATURE_EXTRACTOR\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.compute_last_offset","title":"compute_last_offset  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>compute_last_offset: bool = False\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.keep_input_columns","title":"keep_input_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keep_input_columns: bool = False\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.offset_col","title":"offset_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset_col: str = 'resample_offset'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.offset_window","title":"offset_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset_window: int = 1\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.raw_data_type","title":"raw_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_type: RawDataType = SPOT\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.resample_mode","title":"resample_mode  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resample_mode: Literal['add', 'replace'] = 'add'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.resample_prefix","title":"resample_prefix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>resample_prefix: str | None = None\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.use_resample","title":"use_resample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_resample: bool = False\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate configuration after initialization.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If offset_window &lt;= 0, invalid resample_mode, or wrong offset_col.</p> <code>TypeError</code> <p>If column names not strings.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate configuration after initialization.\n\n    Raises:\n        ValueError: If offset_window &lt;= 0, invalid resample_mode, or wrong offset_col.\n        TypeError: If column names not strings.\n    \"\"\"\n    if self.offset_window &lt;= 0:\n        raise ValueError(f\"offset_window must be &gt; 0, got {self.offset_window}\")\n\n    if self.resample_mode not in (\"add\", \"replace\"):\n        raise ValueError(f\"Invalid resample_mode: {self.resample_mode}\")\n\n    if self.offset_col != RollingAggregator.OFFSET_COL:\n        raise ValueError(\n            f\"offset_col must be '{RollingAggregator.OFFSET_COL}', got '{self.offset_col}'\"\n        )\n\n    if not isinstance(self.pair_col, str) or not isinstance(self.ts_col, str) or not isinstance(self.offset_col, str):\n        raise TypeError(\"pair_col/ts_col/offset_col must be str\")\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor._validate_input","title":"_validate_input","text":"<pre><code>_validate_input(df: DataFrame) -&gt; None\n</code></pre> <p>Validate input DataFrame has required columns.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns missing.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def _validate_input(self, df: pl.DataFrame) -&gt; None:\n    \"\"\"Validate input DataFrame has required columns.\n\n    Args:\n        df (pl.DataFrame): Input to validate.\n\n    Raises:\n        ValueError: If required columns missing.\n    \"\"\"\n    missing = [c for c in (self.pair_col, self.ts_col) if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing required columns: {missing}\")\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> <p>Compute features for single (pair, resample_offset) group.</p> <p>Core feature extraction logic - must be implemented by subclasses.</p> <p>CRITICAL: Must preserve row count (len(output) == len(input)). Should preserve ordering within group.</p> <p>Parameters:</p> Name Type Description Default <code>group_df</code> <code>DataFrame</code> <p>Single group's data, sorted by timestamp.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Same length as input with added feature columns.</p> Example <pre><code>def compute_group(self, group_df, data_context=None):\n    # Simple moving average\n    return group_df.with_columns([\n        pl.col(\"close\")\n        .rolling_mean(self.window)\n        .alias(f\"sma_{self.window}\")\n    ])\n\n# Multiple features\ndef compute_group(self, group_df, data_context=None):\n    return group_df.with_columns([\n        pl.col(\"close\").rolling_mean(10).alias(\"sma_10\"),\n        pl.col(\"close\").rolling_mean(20).alias(\"sma_20\"),\n        pl.col(\"high\").rolling_max(14).alias(\"high_14\"),\n        pl.col(\"low\").rolling_min(14).alias(\"low_14\")\n    ])\n</code></pre> Note <p>Output must have same height as input (length-preserving). Use rolling operations for windowed features. First N-1 bars may have null values for N-period indicators.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def compute_group(\n    self,\n    group_df: pl.DataFrame,\n    data_context: dict[str, Any] | None,\n) -&gt; pl.DataFrame:\n    \"\"\"Compute features for single (pair, resample_offset) group.\n\n    Core feature extraction logic - must be implemented by subclasses.\n\n    CRITICAL: Must preserve row count (len(output) == len(input)).\n    Should preserve ordering within group.\n\n    Args:\n        group_df (pl.DataFrame): Single group's data, sorted by timestamp.\n        data_context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Same length as input with added feature columns.\n\n    Example:\n        ```python\n        def compute_group(self, group_df, data_context=None):\n            # Simple moving average\n            return group_df.with_columns([\n                pl.col(\"close\")\n                .rolling_mean(self.window)\n                .alias(f\"sma_{self.window}\")\n            ])\n\n        # Multiple features\n        def compute_group(self, group_df, data_context=None):\n            return group_df.with_columns([\n                pl.col(\"close\").rolling_mean(10).alias(\"sma_10\"),\n                pl.col(\"close\").rolling_mean(20).alias(\"sma_20\"),\n                pl.col(\"high\").rolling_max(14).alias(\"high_14\"),\n                pl.col(\"low\").rolling_min(14).alias(\"low_14\")\n            ])\n        ```\n\n    Note:\n        Output must have same height as input (length-preserving).\n        Use rolling operations for windowed features.\n        First N-1 bars may have null values for N-period indicators.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/feature/#signalflow.feature.base.FeatureExtractor.extract","title":"extract","text":"<pre><code>extract(df: DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Extract features from input DataFrame.</p> <p>Main entry point - handles sorting, resampling, grouping, and projection.</p> Processing pipeline <ol> <li>Validate input (required columns)</li> <li>Sort by (pair, timestamp)</li> <li>Add resample_offset column if missing</li> <li>(optional) Apply sliding resample</li> <li>(optional) Filter to last offset</li> <li>Group by (pair, resample_offset) and compute features</li> <li>Sort output</li> <li>Project to output columns</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input OHLCV data with pair and timestamp columns.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Additional context for computation.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Features DataFrame with columns: - pair, timestamp (always included) - feature columns (from compute_group)</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df not pl.DataFrame or compute_group returns wrong type.</p> <code>ValueError</code> <p>If compute_group changes row count or columns missing.</p> Example <pre><code># Basic extraction\nfeatures = extractor.extract(ohlcv_df)\n\n# With resampling (5m from 1m)\nextractor = RsiExtractor(\n    window=14,\n    offset_window=5,\n    use_resample=True\n)\nfeatures = extractor.extract(ohlcv_df)\n\n# Keep input columns\nextractor.keep_input_columns = True\nfeatures_with_ohlcv = extractor.extract(ohlcv_df)\n</code></pre> Note <p>Only accepts pl.DataFrame (Polars-first design). Use PandasFeatureExtractor adapter for Pandas data.</p> Source code in <code>src/signalflow/feature/base.py</code> <pre><code>def extract(self, df: pl.DataFrame, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Extract features from input DataFrame.\n\n    Main entry point - handles sorting, resampling, grouping, and projection.\n\n    Processing pipeline:\n        1. Validate input (required columns)\n        2. Sort by (pair, timestamp)\n        3. Add resample_offset column if missing\n        4. (optional) Apply sliding resample\n        5. (optional) Filter to last offset\n        6. Group by (pair, resample_offset) and compute features\n        7. Sort output\n        8. Project to output columns\n\n    Args:\n        df (pl.DataFrame): Input OHLCV data with pair and timestamp columns.\n        data_context (dict[str, Any] | None): Additional context for computation.\n\n    Returns:\n        pl.DataFrame: Features DataFrame with columns:\n            - pair, timestamp (always included)\n            - feature columns (from compute_group)\n\n    Raises:\n        TypeError: If df not pl.DataFrame or compute_group returns wrong type.\n        ValueError: If compute_group changes row count or columns missing.\n\n    Example:\n        ```python\n        # Basic extraction\n        features = extractor.extract(ohlcv_df)\n\n        # With resampling (5m from 1m)\n        extractor = RsiExtractor(\n            window=14,\n            offset_window=5,\n            use_resample=True\n        )\n        features = extractor.extract(ohlcv_df)\n\n        # Keep input columns\n        extractor.keep_input_columns = True\n        features_with_ohlcv = extractor.extract(ohlcv_df)\n        ```\n\n    Note:\n        Only accepts pl.DataFrame (Polars-first design).\n        Use PandasFeatureExtractor adapter for Pandas data.\n    \"\"\"\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(\n            f\"{self.__class__.__name__} is polars-first and accepts only pl.DataFrame. \"\n            f\"Got: {type(df)}. Use an adapter for other dataframe types.\"\n        )\n    self._validate_input(df)\n\n    df0 = df.sort([self.pair_col, self.ts_col])\n\n    if self.offset_col not in df0.columns:\n        df0 = self._resampler.add_offset_column(df0)\n\n    if self.use_resample:\n        df0 = self._resampler.resample(df0)\n\n    if self.compute_last_offset:\n        last_off = self._resampler.get_last_offset(df0)\n        df0 = df0.filter(pl.col(self.offset_col) == last_off)\n\n    prepared_cols = set(df0.columns)\n    inferred_features: set[str] = set()\n\n    def _wrapped(g: pl.DataFrame) -&gt; pl.DataFrame:\n        nonlocal inferred_features\n\n        in_cols = set(g.columns)\n        out = self.compute_group(g, data_context=data_context)\n\n        if not isinstance(out, pl.DataFrame):\n            raise TypeError(f\"{self.__class__.__name__}.compute_pl_group must return pl.DataFrame\")\n\n        if out.height != g.height:\n            raise ValueError(\n                f\"{self.__class__.__name__}: len(output_group)={out.height} != len(input_group)={g.height}\"\n            )\n\n        if not inferred_features:\n            inferred_features = set(out.columns) - in_cols\n\n        return out\n\n    out = (\n        df0.group_by(self.pair_col, self.offset_col, maintain_order=True)\n        .map_groups(_wrapped)\n        .sort([self.pair_col, self.ts_col])\n    )\n\n    if self.keep_input_columns:\n        return out\n\n    feature_cols = sorted(set(out.columns) - prepared_cols)\n    keep_cols = [self.pair_col, self.ts_col] + feature_cols\n\n    missing = [c for c in keep_cols if c not in out.columns]\n    if missing:\n        raise ValueError(f\"Projection error, missing columns: {missing}\")\n\n    return out.select(keep_cols)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet","title":"signalflow.feature.feature_set.FeatureSet  <code>dataclass</code>","text":"<pre><code>FeatureSet(extractors: list[FeatureExtractor], parallel: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp')\n</code></pre> <p>Polars-first orchestrator for multiple feature extractors.</p> <p>Combines independent feature extractors via outer join on (pair, timestamp). Each extractor fetches its required data, computes features, and results are merged into single DataFrame.</p> Key features <ul> <li>Automatic data fetching per extractor</li> <li>Timezone normalization (all \u2192 naive)</li> <li>Outer join on (pair, timestamp) for alignment</li> <li>Duplicate feature column detection</li> <li>Consistent index columns across extractors</li> </ul> Processing flow <p>For each extractor:     1. Fetch appropriate raw data as Polars     2. Run extractor.extract()     3. Normalize timestamps to timezone-naive     4. Validate index columns present Then:     5. Outer join all results on (pair, timestamp)</p> <p>Attributes:</p> Name Type Description <code>extractors</code> <code>list[FeatureExtractor]</code> <p>Feature extractors to orchestrate.</p> <code>parallel</code> <code>bool</code> <p>Parallel execution flag (not yet implemented). Default: False.</p> <code>pair_col</code> <code>str</code> <p>Trading pair column name. Default: \"pair\".</p> <code>ts_col</code> <code>str</code> <p>Timestamp column name. Default: \"timestamp\".</p> Example <pre><code>from signalflow.feature import FeatureSet, SmaExtractor, RsiExtractor\n\n# Create feature set\nfeature_set = FeatureSet([\n    SmaExtractor(window=10, column=\"close\"),\n    SmaExtractor(window=20, column=\"close\"),\n    RsiExtractor(window=14, column=\"close\")\n])\n\n# Extract all features at once\nfrom signalflow.core import RawDataView\nview = RawDataView(raw=raw_data)\nfeatures = feature_set.extract(view)\n\n# Result has: pair, timestamp, sma_10, sma_20, rsi_14\nprint(features.columns)\n# ['pair', 'timestamp', 'sma_10', 'sma_20', 'rsi_14']\n</code></pre> Example <pre><code># With multi-timeframe features\nfeature_set = FeatureSet([\n    # 1-minute features\n    SmaExtractor(window=10, offset_window=1),\n    # 5-minute features\n    SmaExtractor(\n        window=10,\n        offset_window=5,\n        use_resample=True,\n        resample_prefix=\"5m_\"\n    )\n])\n\nfeatures = feature_set.extract(view)\n# Has both 1m and 5m features aligned\n</code></pre> Note <p>All extractors must use same pair_col and ts_col. Feature column names must be unique across extractors. Timestamps automatically normalized to timezone-naive.</p> See Also <p>FeatureExtractor: Base class for individual extractors. RawDataView: Provides data in required format.</p>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet.extractors","title":"extractors  <code>instance-attribute</code>","text":"<pre><code>extractors: list[FeatureExtractor]\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet.parallel","title":"parallel  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>parallel: bool = False\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> <p>Validate extractors configuration.</p> Checks <ul> <li>At least one extractor provided</li> <li>All extractors use same pair_col</li> <li>All extractors use same ts_col</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If validation fails.</p> Source code in <code>src/signalflow/feature/feature_set.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validate extractors configuration.\n\n    Checks:\n        - At least one extractor provided\n        - All extractors use same pair_col\n        - All extractors use same ts_col\n\n    Raises:\n        ValueError: If validation fails.\n    \"\"\"\n    if not self.extractors:\n        raise ValueError(\"At least one extractor must be provided\")\n\n    for ex in self.extractors:\n        if getattr(ex, \"pair_col\", self.pair_col) != self.pair_col:\n            raise ValueError(\n                f\"All extractors must use pair_col='{self.pair_col}'. \"\n                f\"{ex.__class__.__name__} uses '{getattr(ex, 'pair_col', None)}'\"\n            )\n        if getattr(ex, \"ts_col\", self.ts_col) != self.ts_col:\n            raise ValueError(\n                f\"All extractors must use ts_col='{self.ts_col}'. \"\n                f\"{ex.__class__.__name__} uses '{getattr(ex, 'ts_col', None)}'\"\n            )\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet._combine_features","title":"_combine_features","text":"<pre><code>_combine_features(feature_dfs: list[DataFrame]) -&gt; pl.DataFrame\n</code></pre> <p>Combine feature DataFrames via outer join.</p> <p>Merges all feature DataFrames on (pair, timestamp) index. Detects and rejects duplicate feature column names.</p> <p>Parameters:</p> Name Type Description Default <code>feature_dfs</code> <code>list[DataFrame]</code> <p>Feature DataFrames to combine.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Combined features with outer join semantics.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no DataFrames or duplicate feature columns found.</p> Example <pre><code># Internal usage\ndf1 = pl.DataFrame({\"pair\": [\"BTC\"], \"timestamp\": [t1], \"sma_10\": [45000]})\ndf2 = pl.DataFrame({\"pair\": [\"BTC\"], \"timestamp\": [t1], \"rsi_14\": [50]})\ncombined = self._combine_features([df1, df2])\n# Result: pair, timestamp, sma_10, rsi_14\n</code></pre> Note <p>Outer join preserves all (pair, timestamp) from all extractors. Duplicate columns trigger error - use unique prefixes.</p> Source code in <code>src/signalflow/feature/feature_set.py</code> <pre><code>def _combine_features(self, feature_dfs: list[pl.DataFrame]) -&gt; pl.DataFrame:\n    \"\"\"Combine feature DataFrames via outer join.\n\n    Merges all feature DataFrames on (pair, timestamp) index.\n    Detects and rejects duplicate feature column names.\n\n    Args:\n        feature_dfs (list[pl.DataFrame]): Feature DataFrames to combine.\n\n    Returns:\n        pl.DataFrame: Combined features with outer join semantics.\n\n    Raises:\n        ValueError: If no DataFrames or duplicate feature columns found.\n\n    Example:\n        ```python\n        # Internal usage\n        df1 = pl.DataFrame({\"pair\": [\"BTC\"], \"timestamp\": [t1], \"sma_10\": [45000]})\n        df2 = pl.DataFrame({\"pair\": [\"BTC\"], \"timestamp\": [t1], \"rsi_14\": [50]})\n        combined = self._combine_features([df1, df2])\n        # Result: pair, timestamp, sma_10, rsi_14\n        ```\n\n    Note:\n        Outer join preserves all (pair, timestamp) from all extractors.\n        Duplicate columns trigger error - use unique prefixes.\n    \"\"\"\n    if not feature_dfs:\n        raise ValueError(\"No feature DataFrames to combine\")\n\n    combined = feature_dfs[0]\n\n    for right in feature_dfs[1:]:\n        right_feature_cols = [c for c in right.columns if c not in (self.pair_col, self.ts_col)]\n        dup = set(right_feature_cols).intersection(set(combined.columns))\n        if dup:\n            raise ValueError(\n                f\"Duplicate feature columns during FeatureSet combine: {sorted(dup)}. \"\n                f\"Rename features or set unique prefixes.\"\n            )\n\n        combined = combined.join(right, on=[self.pair_col, self.ts_col], how=\"outer\", coalesce=True)\n\n    return combined\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet._get_input_df","title":"_get_input_df","text":"<pre><code>_get_input_df(raw_data: RawDataView, extractor: FeatureExtractor) -&gt; pl.DataFrame\n</code></pre> <p>Fetch input data for extractor in Polars format.</p> <p>Determines required data type from extractor.raw_data_type and fetches as Polars DataFrame (canonical format).</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>RawDataView</code> <p>Data view.</p> required <code>extractor</code> <code>FeatureExtractor</code> <p>Extractor needing data.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Raw data in Polars format.</p> Note <p>Always returns Polars (Polars-first design). Falls back to string \"polars\" for backward compatibility.</p> Source code in <code>src/signalflow/feature/feature_set.py</code> <pre><code>def _get_input_df(self, raw_data: RawDataView, extractor: FeatureExtractor) -&gt; pl.DataFrame:\n    \"\"\"Fetch input data for extractor in Polars format.\n\n    Determines required data type from extractor.raw_data_type and\n    fetches as Polars DataFrame (canonical format).\n\n    Args:\n        raw_data (RawDataView): Data view.\n        extractor (FeatureExtractor): Extractor needing data.\n\n    Returns:\n        pl.DataFrame: Raw data in Polars format.\n\n    Note:\n        Always returns Polars (Polars-first design).\n        Falls back to string \"polars\" for backward compatibility.\n    \"\"\"\n    raw_data_type = getattr(extractor, \"raw_data_type\", RawDataType.SPOT)\n\n    try:\n        return raw_data.get_data(raw_data_type, DataFrameType.POLARS)\n    except TypeError:\n        return raw_data.get_data(raw_data_type, \"polars\")\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet._normalize_index","title":"_normalize_index","text":"<pre><code>_normalize_index(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Normalize timestamp to timezone-naive.</p> <p>Ensures consistent timezone handling across all extractors.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame to normalize.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame with timezone-naive timestamps.</p> Source code in <code>src/signalflow/feature/feature_set.py</code> <pre><code>def _normalize_index(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Normalize timestamp to timezone-naive.\n\n    Ensures consistent timezone handling across all extractors.\n\n    Args:\n        df (pl.DataFrame): DataFrame to normalize.\n\n    Returns:\n        pl.DataFrame: DataFrame with timezone-naive timestamps.\n    \"\"\"\n    if self.ts_col in df.columns:\n        ts_dtype = df.schema.get(self.ts_col)\n        if isinstance(ts_dtype, pl.Datetime) and ts_dtype.time_zone is not None:\n            df = df.with_columns(pl.col(self.ts_col).dt.replace_time_zone(None))\n    return df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.feature_set.FeatureSet.extract","title":"extract","text":"<pre><code>extract(raw_data: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Extract and combine features from all extractors.</p> <p>Main entry point - orchestrates extraction and merging.</p> Processing <ol> <li>For each extractor:<ul> <li>Fetch appropriate data format</li> <li>Run extraction</li> <li>Normalize timestamps</li> <li>Validate output</li> </ul> </li> <li>Outer join all results on (pair, timestamp)</li> <li>Detect duplicate feature columns</li> </ol> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>RawDataView</code> <p>View to raw market data.</p> required <code>context</code> <code>dict[str, Any] | None</code> <p>Additional context passed to extractors.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Combined features with columns: - pair, timestamp (index) - feature columns from all extractors</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no extractors or duplicate feature columns.</p> <code>TypeError</code> <p>If extractor doesn't return pl.DataFrame.</p> Example <pre><code>from signalflow.core import RawData, RawDataView\n\n# Create view\nview = RawDataView(raw=raw_data)\n\n# Extract features\nfeatures = feature_set.extract(view)\n\n# Check result\nprint(f\"Features: {features.columns}\")\nprint(f\"Shape: {features.shape}\")\n\n# With context\nfeatures = feature_set.extract(\n    view,\n    context={\"lookback_bars\": 100}\n)\n</code></pre> Note <p>Outer join means all (pair, timestamp) combinations preserved. Missing features filled with null for non-matching timestamps.</p> Source code in <code>src/signalflow/feature/feature_set.py</code> <pre><code>def extract(self, raw_data: RawDataView, context: dict[str, Any] | None = None) -&gt; pl.DataFrame:\n    \"\"\"Extract and combine features from all extractors.\n\n    Main entry point - orchestrates extraction and merging.\n\n    Processing:\n        1. For each extractor:\n            - Fetch appropriate data format\n            - Run extraction\n            - Normalize timestamps\n            - Validate output\n        2. Outer join all results on (pair, timestamp)\n        3. Detect duplicate feature columns\n\n    Args:\n        raw_data (RawDataView): View to raw market data.\n        context (dict[str, Any] | None): Additional context passed to extractors.\n\n    Returns:\n        pl.DataFrame: Combined features with columns:\n            - pair, timestamp (index)\n            - feature columns from all extractors\n\n    Raises:\n        ValueError: If no extractors or duplicate feature columns.\n        TypeError: If extractor doesn't return pl.DataFrame.\n\n    Example:\n        ```python\n        from signalflow.core import RawData, RawDataView\n\n        # Create view\n        view = RawDataView(raw=raw_data)\n\n        # Extract features\n        features = feature_set.extract(view)\n\n        # Check result\n        print(f\"Features: {features.columns}\")\n        print(f\"Shape: {features.shape}\")\n\n        # With context\n        features = feature_set.extract(\n            view,\n            context={\"lookback_bars\": 100}\n        )\n        ```\n\n    Note:\n        Outer join means all (pair, timestamp) combinations preserved.\n        Missing features filled with null for non-matching timestamps.\n    \"\"\"\n    feature_dfs: list[pl.DataFrame] = []\n\n    for extractor in self.extractors:\n        input_df = self._get_input_df(raw_data, extractor)\n\n        result_df = extractor.extract(input_df, data_context=context)\n        if not isinstance(result_df, pl.DataFrame):\n            raise TypeError(\n                f\"{extractor.__class__.__name__}.extract must return pl.DataFrame, got {type(result_df)}\"\n            )\n\n        result_df = self._normalize_index(result_df)\n\n        if self.pair_col not in result_df.columns or self.ts_col not in result_df.columns:\n            raise ValueError(\n                f\"{extractor.__class__.__name__} returned no index columns \"\n                f\"('{self.pair_col}', '{self.ts_col}'). \"\n                f\"FeatureSet requires index columns to combine features.\"\n            )\n\n        feature_dfs.append(result_df)\n\n    return self._combine_features(feature_dfs)\n</code></pre>"},{"location":"api/feature/#smoothing-extractors","title":"Smoothing Extractors","text":""},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor","title":"signalflow.feature.smoother.sma_extractor.SmaExtractor  <code>dataclass</code>","text":"<pre><code>SmaExtractor(offset_window: int = 1, compute_last_offset: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp', offset_col: str = 'resample_offset', use_resample: bool = True, resample_mode: Literal['add', 'replace'] = 'add', resample_prefix: str | None = None, raw_data_type: RawDataType = RawDataType.SPOT, keep_input_columns: bool = False, sma_period: int = 20, price_col: str = 'close', out_col: str = 'sma')\n</code></pre> <p>               Bases: <code>FeatureExtractor</code></p> <p>SMA per (pair, resample_offset) group.</p> <p>Notes: - offset_window here is for RollingAggregator (your framework requirement).   SMA window is <code>sma_period</code>. - In v1 you said only spot -&gt; keep data_type=\"spot\" by default.</p>"},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor.offset_window","title":"offset_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>offset_window: int = 1\n</code></pre>"},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor.out_col","title":"out_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>out_col: str = 'sma'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor.sma_period","title":"sma_period  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>sma_period: int = 20\n</code></pre>"},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor.use_resample","title":"use_resample  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>use_resample: bool = True\n</code></pre>"},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/smoother/sma_extractor.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    super().__post_init__()\n    if self.sma_period &lt;= 0:\n        raise ValueError(f\"sma_period must be &gt; 0, got {self.sma_period}\")\n    if not self.out_col:\n        self.out_col = \"sma\"\n</code></pre>"},{"location":"api/feature/#signalflow.feature.smoother.sma_extractor.SmaExtractor.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict | None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/feature/smoother/sma_extractor.py</code> <pre><code>def compute_group(self, group_df: pl.DataFrame, data_context: dict | None) -&gt; pl.DataFrame:\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column: {self.price_col}\")\n\n    sma = (\n        pl.col(self.price_col)\n        .rolling_mean(window_size=self.sma_period, min_samples=self.sma_period)\n        .alias(self.out_col)\n    )\n    return group_df.with_columns(sma)\n</code></pre>"},{"location":"api/feature/#pandas-ta-extractors","title":"Pandas-TA Extractors","text":""},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaRsiExtractor","title":"signalflow.feature.pandasta.top_pandasta_extractors.PandasTaRsiExtractor  <code>dataclass</code>","text":"<pre><code>PandasTaRsiExtractor(offset_window: int = 1, compute_last_offset: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp', offset_col: str = 'resample_offset', use_resample: bool = False, resample_mode: Literal['add', 'replace'] = 'add', resample_prefix: str | None = None, raw_data_type: RawDataType = RawDataType.SPOT, keep_input_columns: bool = False, out_cols: list[str] | None = None, series_name: str = 'feature', rename_outputs: dict[str, str] = dict(), indicator: str = 'rsi', params: dict[str, Any] = dict(), input_column: str = 'close', additional_inputs: dict[str, str] = dict(), feature_prefix: str | None = None, length: int = 14, *, pandas_group_fn: PandasGroupFn | None = None)\n</code></pre> <p>               Bases: <code>PandasTaExtractor</code></p>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaRsiExtractor.length","title":"length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>length: int = 14\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaRsiExtractor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/pandasta/top_pandasta_extractors.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.indicator = \"rsi\"\n    self.params = {\"length\": int(self.length)}\n    self.input_column = \"close\"\n    self.additional_inputs = {}\n    self.feature_prefix = \"rsi\"\n    super().__post_init__()\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaBbandsExtractor","title":"signalflow.feature.pandasta.top_pandasta_extractors.PandasTaBbandsExtractor  <code>dataclass</code>","text":"<pre><code>PandasTaBbandsExtractor(offset_window: int = 1, compute_last_offset: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp', offset_col: str = 'resample_offset', use_resample: bool = False, resample_mode: Literal['add', 'replace'] = 'add', resample_prefix: str | None = None, raw_data_type: RawDataType = RawDataType.SPOT, keep_input_columns: bool = False, out_cols: list[str] | None = None, series_name: str = 'feature', rename_outputs: dict[str, str] = dict(), indicator: str = 'rsi', params: dict[str, Any] = dict(), input_column: str = 'close', additional_inputs: dict[str, str] = dict(), feature_prefix: str | None = None, length: int = 20, std: float = 2.0, *, pandas_group_fn: PandasGroupFn | None = None)\n</code></pre> <p>               Bases: <code>PandasTaExtractor</code></p>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaBbandsExtractor.length","title":"length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>length: int = 20\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaBbandsExtractor.std","title":"std  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>std: float = 2.0\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaBbandsExtractor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/pandasta/top_pandasta_extractors.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.indicator = \"bbands\"\n    self.params = {\"length\": int(self.length), \"std\": float(self.std)}\n    self.input_column = \"close\"\n    self.additional_inputs = {}\n    self.feature_prefix = f\"bbands_{int(self.length)}_{float(self.std)}\"\n    super().__post_init__()\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaAtrExtractor","title":"signalflow.feature.pandasta.top_pandasta_extractors.PandasTaAtrExtractor  <code>dataclass</code>","text":"<pre><code>PandasTaAtrExtractor(offset_window: int = 1, compute_last_offset: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp', offset_col: str = 'resample_offset', use_resample: bool = False, resample_mode: Literal['add', 'replace'] = 'add', resample_prefix: str | None = None, raw_data_type: RawDataType = RawDataType.SPOT, keep_input_columns: bool = False, out_cols: list[str] | None = None, series_name: str = 'feature', rename_outputs: dict[str, str] = dict(), indicator: str = 'rsi', params: dict[str, Any] = dict(), input_column: str = 'close', additional_inputs: dict[str, str] = dict(), feature_prefix: str | None = None, length: int = 14, *, pandas_group_fn: PandasGroupFn | None = None)\n</code></pre> <p>               Bases: <code>PandasTaExtractor</code></p>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaAtrExtractor.length","title":"length  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>length: int = 14\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaAtrExtractor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/pandasta/top_pandasta_extractors.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.indicator = \"atr\"\n    self.params = {\"length\": int(self.length)}\n    self.input_column = \"high\"\n    self.additional_inputs = {\"low\": \"low\", \"close\": \"close\"}\n    self.feature_prefix = f\"atr_{int(self.length)}\"\n    super().__post_init__()\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaMacdExtractor","title":"signalflow.feature.pandasta.top_pandasta_extractors.PandasTaMacdExtractor  <code>dataclass</code>","text":"<pre><code>PandasTaMacdExtractor(offset_window: int = 1, compute_last_offset: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp', offset_col: str = 'resample_offset', use_resample: bool = False, resample_mode: Literal['add', 'replace'] = 'add', resample_prefix: str | None = None, raw_data_type: RawDataType = RawDataType.SPOT, keep_input_columns: bool = False, out_cols: list[str] | None = None, series_name: str = 'feature', rename_outputs: dict[str, str] = dict(), indicator: str = 'rsi', params: dict[str, Any] = dict(), input_column: str = 'close', additional_inputs: dict[str, str] = dict(), feature_prefix: str | None = None, fast: int = 12, slow: int = 26, signal: int = 9, *, pandas_group_fn: PandasGroupFn | None = None)\n</code></pre> <p>               Bases: <code>PandasTaExtractor</code></p>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaMacdExtractor.fast","title":"fast  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>fast: int = 12\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaMacdExtractor.signal","title":"signal  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>signal: int = 9\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaMacdExtractor.slow","title":"slow  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>slow: int = 26\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.top_pandasta_extractors.PandasTaMacdExtractor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/pandasta/top_pandasta_extractors.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    self.indicator = \"macd\"\n    self.params = {\"fast\": int(self.fast), \"slow\": int(self.slow), \"signal\": int(self.signal)}\n    self.input_column = \"close\"\n    self.additional_inputs = {}\n    self.feature_prefix = f\"macd_{int(self.fast)}_{int(self.slow)}_{int(self.signal)}\"\n    super().__post_init__()\n</code></pre>"},{"location":"api/feature/#pandas-ta-base","title":"Pandas-TA Base","text":""},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor","title":"signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor  <code>dataclass</code>","text":"<pre><code>PandasTaExtractor(offset_window: int = 1, compute_last_offset: bool = False, pair_col: str = 'pair', ts_col: str = 'timestamp', offset_col: str = 'resample_offset', use_resample: bool = False, resample_mode: Literal['add', 'replace'] = 'add', resample_prefix: str | None = None, raw_data_type: RawDataType = RawDataType.SPOT, keep_input_columns: bool = False, out_cols: list[str] | None = None, series_name: str = 'feature', rename_outputs: dict[str, str] = dict(), indicator: str = 'rsi', params: dict[str, Any] = dict(), input_column: str = 'close', additional_inputs: dict[str, str] = dict(), feature_prefix: str | None = None, *, pandas_group_fn: PandasGroupFn | None = None)\n</code></pre> <p>               Bases: <code>PandasFeatureExtractor</code></p> <p>Polars-first Pandas-TA adapter.</p> <p>This extractor runs pandas-ta inside <code>pandas_group_fn</code> per (pair, resample_offset) group, then merges produced feature columns back into the Polars pipeline.</p> Key guarantees <ul> <li>pandas-ta output is normalized to pd.DataFrame</li> <li>output length matches input group length</li> <li>output columns are namespaced to avoid collisions across extractors</li> </ul>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor.additional_inputs","title":"additional_inputs  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>additional_inputs: dict[str, str] = field(default_factory=dict)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor.feature_prefix","title":"feature_prefix  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>feature_prefix: str | None = None\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor.indicator","title":"indicator  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>indicator: str = 'rsi'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor.input_column","title":"input_column  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>input_column: str = 'close'\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor.params","title":"params  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>params: dict[str, Any] = field(default_factory=dict)\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/pandasta/pandas_ta_extractor.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    try:\n        import pandas_ta as _  \n    except ImportError as e:\n        raise ImportError(\"pandas-ta is required. Install with: pip install pandas-ta\") from e\n\n    if not isinstance(self.indicator, str) or not self.indicator.strip():\n        raise ValueError(\"indicator name must be a non-empty string\")\n\n    if not isinstance(self.input_column, str) or not self.input_column.strip():\n        raise ValueError(\"input_column must be a non-empty string\")\n\n    if not isinstance(self.params, dict):\n        raise TypeError(f\"params must be dict[str, Any], got {type(self.params)}\")\n\n    if not isinstance(self.additional_inputs, dict):\n        raise TypeError(f\"additional_inputs must be dict[str, str], got {type(self.additional_inputs)}\")\n\n    for k, v in self.additional_inputs.items():\n        if not isinstance(k, str) or not k.strip():\n            raise TypeError(f\"additional_inputs keys must be non-empty str, got {k!r}\")\n        if not isinstance(v, str) or not v.strip():\n            raise TypeError(f\"additional_inputs values must be non-empty str column names, got {v!r}\")\n\n    self.pandas_group_fn = self._pandas_ta_group_fn\n\n    super().__post_init__()\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor._namespace_columns","title":"_namespace_columns","text":"<pre><code>_namespace_columns(df: DataFrame) -&gt; pd.DataFrame\n</code></pre> <p>Prefix output columns to avoid collisions across different indicators/extractors.</p> Source code in <code>src/signalflow/feature/pandasta/pandas_ta_extractor.py</code> <pre><code>def _namespace_columns(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Prefix output columns to avoid collisions across different indicators/extractors.\n    \"\"\"\n    prefix = self.feature_prefix or self.indicator\n    prefix = str(prefix).strip()\n\n    df = df.copy()\n    new_cols: list[str] = []\n    for i, c in enumerate(df.columns):\n        name = str(c) if c is not None else f\"{self.indicator}_{i}\"\n        name = name.strip() or f\"{self.indicator}_{i}\"\n\n        if name == prefix or name.startswith(prefix + \"_\"):\n            new_cols.append(name)\n        else:\n            new_cols.append(f\"{prefix}_{name}\")\n\n    df.columns = new_cols\n    return df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor._normalize_output","title":"_normalize_output","text":"<pre><code>_normalize_output(out: Any, group_len: int) -&gt; pd.DataFrame\n</code></pre> <p>Normalize pandas-ta output to pd.DataFrame and ensure length matches group.</p> Source code in <code>src/signalflow/feature/pandasta/pandas_ta_extractor.py</code> <pre><code>def _normalize_output(self, out: Any, group_len: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Normalize pandas-ta output to pd.DataFrame and ensure length matches group.\n    \"\"\"\n    if isinstance(out, pd.Series):\n        out_df = out.to_frame()\n        col = out_df.columns[0]\n        if col is None or (isinstance(col, str) and not col.strip()):\n            out_df.columns = [self.indicator]\n    elif isinstance(out, pd.DataFrame):\n        out_df = out\n        if out_df.columns.isnull().any():\n            out_df = out_df.copy()\n            out_df.columns = [\n                c if (c is not None and (not isinstance(c, str) or c.strip())) else f\"{self.indicator}_{i}\"\n                for i, c in enumerate(out_df.columns)\n            ]\n    else:\n        raise TypeError(\n            f\"pandas-ta '{self.indicator}' returned unsupported type: {type(out)}. \"\n            f\"Expected pd.Series or pd.DataFrame.\"\n        )\n\n    if len(out_df) != group_len:\n        raise ValueError(\n            f\"{self.__class__.__name__}: len(output_group)={len(out_df)} != len(input_group)={group_len}\"\n        )\n\n    return out_df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor._pandas_ta_group_fn","title":"_pandas_ta_group_fn","text":"<pre><code>_pandas_ta_group_fn(group: DataFrame, ctx: dict[str, Any] | None) -&gt; pd.DataFrame\n</code></pre> Source code in <code>src/signalflow/feature/pandasta/pandas_ta_extractor.py</code> <pre><code>def _pandas_ta_group_fn(self, group: pd.DataFrame, ctx: dict[str, Any] | None) -&gt; pd.DataFrame:\n    import pandas_ta as ta\n\n    self._validate_required_columns(group)\n\n    try:\n        indicator_func = getattr(ta, self.indicator)\n    except AttributeError as e:\n        raise AttributeError(f\"Indicator '{self.indicator}' not found in pandas-ta.\") from e\n\n    kwargs = dict(self.params)\n\n    primary_input = group[self.input_column]\n    for param_name, column_name in self.additional_inputs.items():\n        kwargs[param_name] = group[column_name]\n\n    out = indicator_func(primary_input, **kwargs)\n\n    out_df = self._normalize_output(out, group_len=len(group))\n    out_df = self._namespace_columns(out_df)\n\n    return out_df\n</code></pre>"},{"location":"api/feature/#signalflow.feature.pandasta.pandas_ta_extractor.PandasTaExtractor._validate_required_columns","title":"_validate_required_columns","text":"<pre><code>_validate_required_columns(df: DataFrame) -&gt; None\n</code></pre> Source code in <code>src/signalflow/feature/pandasta/pandas_ta_extractor.py</code> <pre><code>def _validate_required_columns(self, df: pd.DataFrame) -&gt; None:\n    required = [self.input_column, *self.additional_inputs.values()]\n    missing = sorted(set(required) - set(df.columns))\n    if missing:\n        raise ValueError(f\"Missing required columns for pandas-ta: {missing}\")\n</code></pre>"},{"location":"api/labeler/","title":"Target Module","text":"<p>Signal labeling strategies for machine learning training.</p> <p>Module Name</p> <p>The target functionality is implemented in the <code>signalflow.target</code> module.</p>"},{"location":"api/labeler/#base-class","title":"Base Class","text":""},{"location":"api/labeler/#signalflow.target.base.Labeler","title":"signalflow.target.base.Labeler  <code>dataclass</code>","text":"<pre><code>Labeler(raw_data_type: RawDataType = RawDataType.SPOT, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t_hit', 'ret'))\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for Polars-only signal labeling.</p> <p>Assigns forward-looking labels to historical data based on future price movement. Labels are computed per-pair with length-preserving operations.</p> Key concepts <ul> <li>Forward-looking: Labels depend on future data (not available in live trading)</li> <li>Per-pair processing: Each pair labeled independently</li> <li>Length-preserving: Output has same row count as input</li> <li>Signal masking: Optionally label only at signal timestamps</li> </ul> Public API <ul> <li>compute(): Main entry point (handles grouping, filtering, projection)</li> <li>compute_group(): Per-pair labeling logic (must implement)</li> </ul> Common labeling strategies <ul> <li>Fixed horizon: Label based on return over N bars</li> <li>Triple barrier: Label based on first hit of profit/loss/time barrier</li> <li>Quantile-based: Label based on return quantiles</li> </ul> <p>Attributes:</p> Name Type Description <code>component_type</code> <code>ClassVar[SfComponentType]</code> <p>Always LABELER for registry.</p> <code>raw_data_type</code> <code>RawDataType</code> <p>Type of raw data. Default: SPOT.</p> <code>pair_col</code> <code>str</code> <p>Trading pair column. Default: \"pair\".</p> <code>ts_col</code> <code>str</code> <p>Timestamp column. Default: \"timestamp\".</p> <code>keep_input_columns</code> <code>bool</code> <p>Keep all input columns. Default: False.</p> <code>output_columns</code> <code>list[str] | None</code> <p>Specific columns to output. Default: None.</p> <code>filter_signal_type</code> <code>SignalType | None</code> <p>Filter to specific signal type. Default: None.</p> <code>mask_to_signals</code> <code>bool</code> <p>Mask labels to signal timestamps only. Default: True.</p> <code>out_col</code> <code>str</code> <p>Output label column name. Default: \"label\".</p> <code>include_meta</code> <code>bool</code> <p>Include metadata columns. Default: False.</p> <code>meta_columns</code> <code>tuple[str, ...]</code> <p>Metadata column names. Default: (\"t_hit\", \"ret\").</p> Example <pre><code>from signalflow.target import Labeler\nfrom signalflow.core import SignalType\nimport polars as pl\n\nclass FixedHorizonLabeler(Labeler):\n    '''Label based on fixed-horizon return'''\n\n    def __init__(self, horizon: int = 10, threshold: float = 0.01):\n        super().__init__()\n        self.horizon = horizon\n        self.threshold = threshold\n\n    def compute_group(self, group_df, data_context=None):\n        # Compute forward return\n        labels = group_df.with_columns([\n            pl.col(\"close\").shift(-self.horizon).alias(\"future_close\")\n        ]).with_columns([\n            ((pl.col(\"future_close\") / pl.col(\"close\")) - 1).alias(\"return\")\n        ]).with_columns([\n            pl.when(pl.col(\"return\") &gt; self.threshold)\n            .then(pl.lit(SignalType.RISE.value))\n            .when(pl.col(\"return\") &lt; -self.threshold)\n            .then(pl.lit(SignalType.FALL.value))\n            .otherwise(pl.lit(SignalType.NONE.value))\n            .alias(\"label\")\n        ])\n\n        return labels\n\n# Usage\nlabeler = FixedHorizonLabeler(horizon=10, threshold=0.01)\nlabeled = labeler.compute(ohlcv_df, signals=signals)\n</code></pre> Note <p>compute_group() must preserve row count (no filtering). All timestamps must be timezone-naive. Signal masking requires mask_to_signals=True and signal_keys in context.</p> See Also <p>FixedHorizonLabeler: Simple fixed-horizon implementation. TripleBarrierLabeler: Three-barrier labeling strategy.</p>"},{"location":"api/labeler/#signalflow.target.base.Labeler.component_type","title":"component_type  <code>class-attribute</code>","text":"<pre><code>component_type: SfComponentType = LABELER\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.filter_signal_type","title":"filter_signal_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filter_signal_type: SignalType | None = None\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.include_meta","title":"include_meta  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>include_meta: bool = False\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.keep_input_columns","title":"keep_input_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>keep_input_columns: bool = False\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.mask_to_signals","title":"mask_to_signals  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>mask_to_signals: bool = True\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('t_hit', 'ret')\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.out_col","title":"out_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>out_col: str = 'label'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.output_columns","title":"output_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>output_columns: list[str] | None = None\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.pair_col","title":"pair_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>pair_col: str = 'pair'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.raw_data_type","title":"raw_data_type  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>raw_data_type: RawDataType = SPOT\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.ts_col","title":"ts_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>ts_col: str = 'timestamp'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._apply_signal_mask","title":"_apply_signal_mask","text":"<pre><code>_apply_signal_mask(df: DataFrame, data_context: dict[str, Any], group_df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Mask labels to signal timestamps only.</p> <p>Labels are computed for all rows, but only signal timestamps get actual labels; others are set to SignalType.NONE.</p> <p>Used for meta-labeling: only label at detected signal points, not every bar.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with computed labels.</p> required <code>data_context</code> <code>dict[str, Any]</code> <p>Must contain \"signal_keys\" DataFrame.</p> required <code>group_df</code> <code>DataFrame</code> <p>Original group data for extracting pair value.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: DataFrame with masked labels.</p> Example <pre><code># In compute_group with masking\ndef compute_group(self, group_df, data_context=None):\n    # Compute labels for all rows\n    labeled = group_df.with_columns([...])\n\n    # Mask to signal timestamps only\n    if self.mask_to_signals and data_context:\n        labeled = self._apply_signal_mask(\n            labeled, data_context, group_df\n        )\n\n    return labeled\n</code></pre> Note <p>Requires signal_keys in data_context with (pair, timestamp) columns. Non-signal rows get label=SignalType.NONE. Metadata columns also masked if include_meta=True.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _apply_signal_mask(\n    self,\n    df: pl.DataFrame,\n    data_context: dict[str, Any],\n    group_df: pl.DataFrame,\n) -&gt; pl.DataFrame:\n    \"\"\"Mask labels to signal timestamps only.\n\n    Labels are computed for all rows, but only signal timestamps\n    get actual labels; others are set to SignalType.NONE.\n\n    Used for meta-labeling: only label at detected signal points,\n    not every bar.\n\n    Args:\n        df (pl.DataFrame): DataFrame with computed labels.\n        data_context (dict[str, Any]): Must contain \"signal_keys\" DataFrame.\n        group_df (pl.DataFrame): Original group data for extracting pair value.\n\n    Returns:\n        pl.DataFrame: DataFrame with masked labels.\n\n    Example:\n        ```python\n        # In compute_group with masking\n        def compute_group(self, group_df, data_context=None):\n            # Compute labels for all rows\n            labeled = group_df.with_columns([...])\n\n            # Mask to signal timestamps only\n            if self.mask_to_signals and data_context:\n                labeled = self._apply_signal_mask(\n                    labeled, data_context, group_df\n                )\n\n            return labeled\n        ```\n\n    Note:\n        Requires signal_keys in data_context with (pair, timestamp) columns.\n        Non-signal rows get label=SignalType.NONE.\n        Metadata columns also masked if include_meta=True.\n    \"\"\"\n    signal_keys: pl.DataFrame = data_context[\"signal_keys\"]\n    pair_value = group_df.get_column(self.pair_col)[0]\n\n    signal_ts = (\n        signal_keys.filter(pl.col(self.pair_col) == pair_value)\n        .select(self.ts_col)\n        .unique()\n    )\n\n    if signal_ts.height == 0:\n        df = df.with_columns(pl.lit(SignalType.NONE.value).alias(self.out_col))\n        if self.include_meta:\n            df = df.with_columns(\n                [pl.lit(None).alias(col) for col in self.meta_columns]\n            )\n    else:\n        is_signal = pl.col(\"_is_signal\").fill_null(False)\n        mask_exprs = [\n            pl.when(is_signal)\n            .then(pl.col(self.out_col))\n            .otherwise(pl.lit(SignalType.NONE.value))\n            .alias(self.out_col),\n        ]\n        if self.include_meta:\n            mask_exprs += [\n                pl.when(is_signal)\n                .then(pl.col(col))\n                .otherwise(pl.lit(None))\n                .alias(col)\n                for col in self.meta_columns\n            ]\n\n        df = (\n            df.join(\n                signal_ts.with_columns(pl.lit(True).alias(\"_is_signal\")),\n                on=self.ts_col,\n                how=\"left\",\n            )\n            .with_columns(mask_exprs)\n            .drop(\"_is_signal\")\n        )\n\n    return df\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._compute_pl","title":"_compute_pl","text":"<pre><code>_compute_pl(df: DataFrame, signals: Signals | None, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> <p>Internal Polars-based computation.</p> <p>Orchestrates validation, filtering, grouping, and projection.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data.</p> required <code>signals</code> <code>Signals | None</code> <p>Optional signals.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Optional context.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Labeled data.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _compute_pl(\n    self,\n    df: pl.DataFrame,\n    signals: Signals | None,\n    data_context: dict[str, Any] | None,\n) -&gt; pl.DataFrame:\n    \"\"\"Internal Polars-based computation.\n\n    Orchestrates validation, filtering, grouping, and projection.\n\n    Args:\n        df (pl.DataFrame): Input data.\n        signals (Signals | None): Optional signals.\n        data_context (dict[str, Any] | None): Optional context.\n\n    Returns:\n        pl.DataFrame: Labeled data.\n    \"\"\"\n    self._validate_input_pl(df)\n    df0 = df.sort([self.pair_col, self.ts_col])\n\n    if signals is not None and self.filter_signal_type is not None:\n        s_pl = self._signals_to_pl(signals)\n        df0 = self._filter_by_signals_pl(df0, s_pl, self.filter_signal_type)\n\n    input_cols = set(df0.columns)\n\n    def _wrapped(g: pl.DataFrame) -&gt; pl.DataFrame:\n        out = self.compute_group(g, data_context=data_context)\n        if not isinstance(out, pl.DataFrame):\n            raise TypeError(f\"{self.__class__.__name__}.compute_group must return pl.DataFrame\")\n        if out.height != g.height:\n            raise ValueError(\n                f\"{self.__class__.__name__}: len(output_group)={out.height} != len(input_group)={g.height}\"\n            )\n        return out\n\n    out = (\n        df0.group_by(self.pair_col, maintain_order=True)\n        .map_groups(_wrapped)\n        .sort([self.pair_col, self.ts_col])\n    )\n\n    if self.keep_input_columns:\n        return out\n\n    label_cols = (\n        sorted(set(out.columns) - input_cols)\n        if self.output_columns is None\n        else list(self.output_columns)\n    )\n\n    keep_cols = [self.pair_col, self.ts_col] + label_cols\n    missing = [c for c in keep_cols if c not in out.columns]\n    if missing:\n        raise ValueError(f\"Projection error, missing columns: {missing}\")\n\n    return out.select(keep_cols)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._filter_by_signals_pl","title":"_filter_by_signals_pl","text":"<pre><code>_filter_by_signals_pl(df: DataFrame, s: DataFrame, signal_type: SignalType) -&gt; pl.DataFrame\n</code></pre> <p>Filter input to rows matching signal timestamps.</p> <p>Inner join with signal timestamps of specific type.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data.</p> required <code>s</code> <code>DataFrame</code> <p>Signals DataFrame.</p> required <code>signal_type</code> <code>SignalType</code> <p>Signal type to filter.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Filtered data (only rows at signal timestamps).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If signals missing required columns.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _filter_by_signals_pl(\n    self, df: pl.DataFrame, s: pl.DataFrame, signal_type: SignalType\n) -&gt; pl.DataFrame:\n    \"\"\"Filter input to rows matching signal timestamps.\n\n    Inner join with signal timestamps of specific type.\n\n    Args:\n        df (pl.DataFrame): Input data.\n        s (pl.DataFrame): Signals DataFrame.\n        signal_type (SignalType): Signal type to filter.\n\n    Returns:\n        pl.DataFrame: Filtered data (only rows at signal timestamps).\n\n    Raises:\n        ValueError: If signals missing required columns.\n    \"\"\"\n    required = {self.pair_col, self.ts_col, \"signal_type\"}\n    missing = required - set(s.columns)\n    if missing:\n        raise ValueError(f\"Signals missing columns: {sorted(missing)}\")\n\n    s_f = (\n        s.filter(pl.col(\"signal_type\") == signal_type.value)\n        .select([self.pair_col, self.ts_col])\n        .unique(subset=[self.pair_col, self.ts_col])\n    )\n    return df.join(s_f, on=[self.pair_col, self.ts_col], how=\"inner\")\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._signals_to_pl","title":"_signals_to_pl","text":"<pre><code>_signals_to_pl(signals: Signals) -&gt; pl.DataFrame\n</code></pre> <p>Convert Signals to Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Signals container.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Signals as DataFrame.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If Signals.value is not pl.DataFrame.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _signals_to_pl(self, signals: Signals) -&gt; pl.DataFrame:\n    \"\"\"Convert Signals to Polars DataFrame.\n\n    Args:\n        signals (Signals): Signals container.\n\n    Returns:\n        pl.DataFrame: Signals as DataFrame.\n\n    Raises:\n        TypeError: If Signals.value is not pl.DataFrame.\n    \"\"\"\n    s = signals.value\n    if isinstance(s, pl.DataFrame):\n        return s\n    raise TypeError(f\"Unsupported Signals.value type: {type(s)}\")\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler._validate_input_pl","title":"_validate_input_pl","text":"<pre><code>_validate_input_pl(df: DataFrame) -&gt; None\n</code></pre> <p>Validate input DataFrame schema.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input to validate.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required columns missing.</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def _validate_input_pl(self, df: pl.DataFrame) -&gt; None:\n    \"\"\"Validate input DataFrame schema.\n\n    Args:\n        df (pl.DataFrame): Input to validate.\n\n    Raises:\n        ValueError: If required columns missing.\n    \"\"\"\n    missing = [c for c in (self.pair_col, self.ts_col) if c not in df.columns]\n    if missing:\n        raise ValueError(f\"Missing required columns: {missing}\")\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.compute","title":"compute","text":"<pre><code>compute(df: DataFrame, signals: Signals | None = None, data_context: dict[str, Any] | None = None) -&gt; pl.DataFrame\n</code></pre> <p>Compute labels for input DataFrame.</p> <p>Main entry point - handles validation, filtering, grouping, and projection.</p> Processing steps <ol> <li>Validate input schema</li> <li>Sort by (pair, timestamp)</li> <li>(optional) Filter to specific signal type</li> <li>Group by pair and apply compute_group()</li> <li>Validate output (length-preserving)</li> <li>Project to output columns</li> </ol> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Input data with OHLCV and required columns.</p> required <code>signals</code> <code>Signals | None</code> <p>Signals for filtering/masking.</p> <code>None</code> <code>data_context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Labeled data with columns: - pair, timestamp (always included) - label column(s) (as specified by out_col) - (optional) metadata columns</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If df not pl.DataFrame or compute_group returns wrong type.</p> <code>ValueError</code> <p>If compute_group changes row count or columns missing.</p> Example <pre><code># Basic labeling\nlabeled = labeler.compute(ohlcv_df)\n\n# With signal filtering\nlabeled = labeler.compute(\n    ohlcv_df,\n    signals=signals,\n    filter_signal_type=SignalType.RISE\n)\n\n# With masking context\nlabeled = labeler.compute(\n    ohlcv_df,\n    signals=signals,\n    data_context={\"signal_keys\": signal_timestamps_df}\n)\n</code></pre> Source code in <code>src/signalflow/target/base.py</code> <pre><code>def compute(\n    self,\n    df: pl.DataFrame,\n    signals: Signals | None = None,\n    data_context: dict[str, Any] | None = None,\n) -&gt; pl.DataFrame:\n    \"\"\"Compute labels for input DataFrame.\n\n    Main entry point - handles validation, filtering, grouping, and projection.\n\n    Processing steps:\n        1. Validate input schema\n        2. Sort by (pair, timestamp)\n        3. (optional) Filter to specific signal type\n        4. Group by pair and apply compute_group()\n        5. Validate output (length-preserving)\n        6. Project to output columns\n\n    Args:\n        df (pl.DataFrame): Input data with OHLCV and required columns.\n        signals (Signals | None): Signals for filtering/masking.\n        data_context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Labeled data with columns:\n            - pair, timestamp (always included)\n            - label column(s) (as specified by out_col)\n            - (optional) metadata columns\n\n    Raises:\n        TypeError: If df not pl.DataFrame or compute_group returns wrong type.\n        ValueError: If compute_group changes row count or columns missing.\n\n    Example:\n        ```python\n        # Basic labeling\n        labeled = labeler.compute(ohlcv_df)\n\n        # With signal filtering\n        labeled = labeler.compute(\n            ohlcv_df,\n            signals=signals,\n            filter_signal_type=SignalType.RISE\n        )\n\n        # With masking context\n        labeled = labeler.compute(\n            ohlcv_df,\n            signals=signals,\n            data_context={\"signal_keys\": signal_timestamps_df}\n        )\n        ```\n    \"\"\"\n    if not isinstance(df, pl.DataFrame):\n        raise TypeError(f\"{self.__class__.__name__}.compute expects pl.DataFrame, got {type(df)}\")\n    return self._compute_pl(df=df, signals=signals, data_context=data_context)\n</code></pre>"},{"location":"api/labeler/#signalflow.target.base.Labeler.compute_group","title":"compute_group  <code>abstractmethod</code>","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> <p>Compute labels for single pair group.</p> <p>Core labeling logic - must be implemented by subclasses.</p> <p>CRITICAL: Must preserve row count (len(output) == len(input)). No filtering allowed inside compute_group.</p> <p>Parameters:</p> Name Type Description Default <code>group_df</code> <code>DataFrame</code> <p>Single pair's data, sorted by timestamp.</p> required <code>data_context</code> <code>dict[str, Any] | None</code> <p>Additional context.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pl.DataFrame: Same length as input with added label columns.</p> Example <pre><code>def compute_group(self, group_df, data_context=None):\n    # Compute 10-bar forward return\n    return group_df.with_columns([\n        pl.col(\"close\").shift(-10).alias(\"future_close\")\n    ]).with_columns([\n        ((pl.col(\"future_close\") / pl.col(\"close\")) - 1).alias(\"return\"),\n        pl.when((pl.col(\"future_close\") / pl.col(\"close\") - 1) &gt; 0.01)\n        .then(pl.lit(SignalType.RISE.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n        .alias(\"label\")\n    ])\n</code></pre> Note <p>Output must have same height as input (length-preserving). Use shift(-n) for forward-looking operations. Last N bars will have null labels (no future data).</p> Source code in <code>src/signalflow/target/base.py</code> <pre><code>@abstractmethod\ndef compute_group(\n    self, group_df: pl.DataFrame, data_context: dict[str, Any] | None\n) -&gt; pl.DataFrame:\n    \"\"\"Compute labels for single pair group.\n\n    Core labeling logic - must be implemented by subclasses.\n\n    CRITICAL: Must preserve row count (len(output) == len(input)).\n    No filtering allowed inside compute_group.\n\n    Args:\n        group_df (pl.DataFrame): Single pair's data, sorted by timestamp.\n        data_context (dict[str, Any] | None): Additional context.\n\n    Returns:\n        pl.DataFrame: Same length as input with added label columns.\n\n    Example:\n        ```python\n        def compute_group(self, group_df, data_context=None):\n            # Compute 10-bar forward return\n            return group_df.with_columns([\n                pl.col(\"close\").shift(-10).alias(\"future_close\")\n            ]).with_columns([\n                ((pl.col(\"future_close\") / pl.col(\"close\")) - 1).alias(\"return\"),\n                pl.when((pl.col(\"future_close\") / pl.col(\"close\") - 1) &gt; 0.01)\n                .then(pl.lit(SignalType.RISE.value))\n                .otherwise(pl.lit(SignalType.NONE.value))\n                .alias(\"label\")\n            ])\n        ```\n\n    Note:\n        Output must have same height as input (length-preserving).\n        Use shift(-n) for forward-looking operations.\n        Last N bars will have null labels (no future data).\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api/labeler/#labeling-strategies","title":"Labeling Strategies","text":""},{"location":"api/labeler/#fixed-horizon","title":"Fixed Horizon","text":""},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler","title":"signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler  <code>dataclass</code>","text":"<pre><code>FixedHorizonLabeler(raw_data_type: RawDataType = RawDataType.SPOT, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t1', 'ret'), price_col: str = 'close', horizon: int = 60)\n</code></pre> <p>               Bases: <code>Labeler</code></p> Fixed-Horizon Labeling <p>label[t0] = sign(close[t0 + horizon] - close[t0])</p> <p>If signals provided, labels are written only on signal rows, while horizon is computed on full series (per pair).</p>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.horizon","title":"horizon  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>horizon: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.meta_columns","title":"meta_columns  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>meta_columns: tuple[str, ...] = ('t1', 'ret')\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/fixed_horizon_labeler.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.horizon &lt;= 0:\n        raise ValueError(\"horizon must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.fixed_horizon_labeler.FixedHorizonLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/fixed_horizon_labeler.py</code> <pre><code>def compute_group(\n    self, group_df: pl.DataFrame, data_context: dict[str, Any] | None\n) -&gt; pl.DataFrame:\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    if group_df.height == 0:\n        return group_df\n\n    h = int(self.horizon)\n    price = pl.col(self.price_col)\n    future_price = price.shift(-h)\n\n    df = group_df.with_columns(future_price.alias(\"_future_price\"))\n\n    label_expr = (\n        pl.when(\n            pl.col(\"_future_price\").is_null()\n            | pl.col(self.price_col).is_null()\n            | (pl.col(self.price_col) &lt;= 0)\n            | (pl.col(\"_future_price\") &lt;= 0)\n        )\n        .then(pl.lit(SignalType.NONE.value))\n        .when(pl.col(\"_future_price\") &gt; pl.col(self.price_col))\n        .then(pl.lit(SignalType.RISE.value))\n        .when(pl.col(\"_future_price\") &lt; pl.col(self.price_col))\n        .then(pl.lit(SignalType.FALL.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n    )\n\n    df = df.with_columns(label_expr.alias(self.out_col))\n\n    if self.include_meta:\n        df = df.with_columns(\n            [\n                pl.col(self.ts_col).shift(-h).alias(\"t1\"),\n                pl.when(\n                    pl.col(\"_future_price\").is_not_null()\n                    &amp; (pl.col(self.price_col) &gt; 0)\n                    &amp; (pl.col(\"_future_price\") &gt; 0)\n                )\n                .then((pl.col(\"_future_price\") / pl.col(self.price_col)).log())\n                .otherwise(pl.lit(None))\n                .alias(\"ret\"),\n            ]\n        )\n\n    df = df.drop(\"_future_price\")\n\n    if (\n        self.mask_to_signals\n        and data_context is not None\n        and \"signal_keys\" in data_context\n    ):\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    return df\n</code></pre>"},{"location":"api/labeler/#triple-barrier-dynamic","title":"Triple Barrier (Dynamic)","text":""},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler","title":"signalflow.target.triple_barrier.TripleBarrierLabeler  <code>dataclass</code>","text":"<pre><code>TripleBarrierLabeler(raw_data_type: RawDataType = RawDataType.SPOT, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t_hit', 'ret'), price_col: str = 'close', vol_window: int = 60, lookforward_window: int = 1440, profit_multiplier: float = 1.0, stop_loss_multiplier: float = 1.0)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Triple-Barrier Labeling (De Prado), Numba-accelerated.</p> Volatility-based barriers <ul> <li>pt = close * exp(vol * profit_multiplier)</li> <li>sl = close * exp(-vol * stop_loss_multiplier)</li> </ul>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler.lookforward_window","title":"lookforward_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookforward_window: int = 1440\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler.profit_multiplier","title":"profit_multiplier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>profit_multiplier: float = 1.0\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler.stop_loss_multiplier","title":"stop_loss_multiplier  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop_loss_multiplier: float = 1.0\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler.vol_window","title":"vol_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>vol_window: int = 60\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/triple_barrier.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.vol_window &lt;= 1:\n        raise ValueError(\"vol_window must be &gt; 1\")\n    if self.lookforward_window &lt;= 0:\n        raise ValueError(\"lookforward_window must be &gt; 0\")\n    if self.profit_multiplier &lt;= 0 or self.stop_loss_multiplier &lt;= 0:\n        raise ValueError(\"profit_multiplier/stop_loss_multiplier must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler._apply_labels","title":"_apply_labels","text":"<pre><code>_apply_labels(df: DataFrame) -&gt; pl.DataFrame\n</code></pre> <p>Apply RISE/FALL/NONE labels based on barrier hits.</p> Source code in <code>src/signalflow/target/triple_barrier.py</code> <pre><code>def _apply_labels(self, df: pl.DataFrame) -&gt; pl.DataFrame:\n    \"\"\"Apply RISE/FALL/NONE labels based on barrier hits.\"\"\"\n    choose_up = pl.col(\"_up_off\").is_not_null() &amp; (\n        pl.col(\"_dn_off\").is_null() | (pl.col(\"_up_off\") &lt;= pl.col(\"_dn_off\"))\n    )\n    choose_dn = pl.col(\"_dn_off\").is_not_null() &amp; (\n        pl.col(\"_up_off\").is_null() | (pl.col(\"_dn_off\") &lt; pl.col(\"_up_off\"))\n    )\n\n    return df.with_columns(\n        pl.when(choose_up)\n        .then(pl.lit(SignalType.RISE.value))\n        .when(choose_dn)\n        .then(pl.lit(SignalType.FALL.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n        .alias(self.out_col)\n    )\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler._compute_meta","title":"_compute_meta","text":"<pre><code>_compute_meta(df: DataFrame, prices: ndarray, up_off_series: Series, dn_off_series: Series, lf: int) -&gt; pl.DataFrame\n</code></pre> <p>Compute t_hit and ret meta columns.</p> Source code in <code>src/signalflow/target/triple_barrier.py</code> <pre><code>def _compute_meta(\n    self,\n    df: pl.DataFrame,\n    prices: np.ndarray,\n    up_off_series: pl.Series,\n    dn_off_series: pl.Series,\n    lf: int,\n) -&gt; pl.DataFrame:\n    \"\"\"Compute t_hit and ret meta columns.\"\"\"\n    n = df.height\n    ts_arr = df.get_column(self.ts_col).to_numpy()\n\n    idx = np.arange(n)\n    up_np = up_off_series.fill_null(0).to_numpy()\n    dn_np = dn_off_series.fill_null(0).to_numpy()\n\n    hit_off = np.where(\n        (up_np &gt; 0) &amp; ((dn_np == 0) | (up_np &lt;= dn_np)),\n        up_np,\n        np.where(dn_np &gt; 0, dn_np, 0),\n    )\n\n    hit_idx = np.clip(idx + hit_off, 0, n - 1)\n    vert_idx = np.clip(idx + lf, 0, n - 1)\n    final_idx = np.where(hit_off &gt; 0, hit_idx, vert_idx)\n\n    t_hit = ts_arr[final_idx]\n    ret = np.where(prices &gt; 0, np.log(prices[final_idx] / prices), np.nan)\n\n    return df.with_columns(\n        [\n            pl.Series(\"t_hit\", t_hit),\n            pl.Series(\"ret\", ret),\n        ]\n    )\n</code></pre>"},{"location":"api/labeler/#signalflow.target.triple_barrier.TripleBarrierLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/triple_barrier.py</code> <pre><code>def compute_group(\n    self, group_df: pl.DataFrame, data_context: dict[str, Any] | None\n) -&gt; pl.DataFrame:\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    if group_df.height == 0:\n        return group_df\n\n    lf = int(self.lookforward_window)\n    vw = int(self.vol_window)\n\n    df = group_df.with_columns(\n        (pl.col(self.price_col) / pl.col(self.price_col).shift(1))\n        .log()\n        .rolling_std(window_size=vw, ddof=1)\n        .alias(\"_vol\")\n    ).with_columns(\n        [\n            (\n                pl.col(self.price_col)\n                * (pl.col(\"_vol\") * self.profit_multiplier).exp()\n            ).alias(\"_pt\"),\n            (\n                pl.col(self.price_col)\n                * (-pl.col(\"_vol\") * self.stop_loss_multiplier).exp()\n            ).alias(\"_sl\"),\n        ]\n    )\n\n    prices = df.get_column(self.price_col).to_numpy().astype(np.float64)\n    pt = df.get_column(\"_pt\").fill_null(np.nan).to_numpy().astype(np.float64)\n    sl = df.get_column(\"_sl\").fill_null(np.nan).to_numpy().astype(np.float64)\n\n    up_off, dn_off = _find_first_hit(prices, pt, sl, lf)\n\n    up_off_series = pl.Series(\"_up_off\", up_off).replace(0, None).cast(pl.Int32)\n    dn_off_series = pl.Series(\"_dn_off\", dn_off).replace(0, None).cast(pl.Int32)\n\n    df = df.with_columns([up_off_series, dn_off_series])\n\n    df = self._apply_labels(df)\n\n    if self.include_meta:\n        df = self._compute_meta(df, prices, up_off_series, dn_off_series, lf)\n\n    if (\n        self.mask_to_signals\n        and data_context is not None\n        and \"signal_keys\" in data_context\n    ):\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    drop_cols = [\"_vol\", \"_pt\", \"_sl\", \"_up_off\", \"_dn_off\"]\n    df = df.drop([c for c in drop_cols if c in df.columns])\n\n    return df\n</code></pre>"},{"location":"api/labeler/#static-triple-barrier","title":"Static Triple Barrier","text":""},{"location":"api/labeler/#signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler","title":"signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler  <code>dataclass</code>","text":"<pre><code>StaticTripleBarrierLabeler(raw_data_type: RawDataType = RawDataType.SPOT, pair_col: str = 'pair', ts_col: str = 'timestamp', keep_input_columns: bool = False, output_columns: list[str] | None = None, filter_signal_type: SignalType | None = None, mask_to_signals: bool = True, out_col: str = 'label', include_meta: bool = False, meta_columns: tuple[str, ...] = ('t_hit', 'ret'), price_col: str = 'close', lookforward_window: int = 1440, profit_pct: float = 0.01, stop_loss_pct: float = 0.01)\n</code></pre> <p>               Bases: <code>Labeler</code></p> <p>Triple-Barrier (first-touch) labeling with STATIC horizontal barriers. Numba-accelerated version.</p> <p>De Prado's framework:   - Vertical barrier at t1 = t0 + lookforward_window   - Horizontal barriers defined as % from initial price at t0:       pt = close[t0] * (1 + profit_pct)       sl = close[t0] * (1 - stop_loss_pct)   - Label by first touch within (t0, t1]:       RISE if PT touched first (ties -&gt; PT)       FALL if SL touched first       NONE if none touched by t1</p>"},{"location":"api/labeler/#signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler.lookforward_window","title":"lookforward_window  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>lookforward_window: int = 1440\n</code></pre>"},{"location":"api/labeler/#signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler.price_col","title":"price_col  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>price_col: str = 'close'\n</code></pre>"},{"location":"api/labeler/#signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler.profit_pct","title":"profit_pct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>profit_pct: float = 0.01\n</code></pre>"},{"location":"api/labeler/#signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler.stop_loss_pct","title":"stop_loss_pct  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>stop_loss_pct: float = 0.01\n</code></pre>"},{"location":"api/labeler/#signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler.__post_init__","title":"__post_init__","text":"<pre><code>__post_init__() -&gt; None\n</code></pre> Source code in <code>src/signalflow/target/static_triple_barrier.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    if self.lookforward_window &lt;= 0:\n        raise ValueError(\"lookforward_window must be &gt; 0\")\n    if self.profit_pct &lt;= 0 or self.stop_loss_pct &lt;= 0:\n        raise ValueError(\"profit_pct/stop_loss_pct must be &gt; 0\")\n\n    cols = [self.out_col]\n    if self.include_meta:\n        cols += list(self.meta_columns)\n    self.output_columns = cols\n</code></pre>"},{"location":"api/labeler/#signalflow.target.static_triple_barrier.StaticTripleBarrierLabeler.compute_group","title":"compute_group","text":"<pre><code>compute_group(group_df: DataFrame, data_context: dict[str, Any] | None) -&gt; pl.DataFrame\n</code></pre> Source code in <code>src/signalflow/target/static_triple_barrier.py</code> <pre><code>def compute_group(\n    self, group_df: pl.DataFrame, data_context: dict[str, Any] | None\n) -&gt; pl.DataFrame:\n    if self.price_col not in group_df.columns:\n        raise ValueError(f\"Missing required column '{self.price_col}'\")\n\n    if group_df.height == 0:\n        return group_df\n\n    lf = int(self.lookforward_window)\n    n = group_df.height\n\n    prices = group_df.get_column(self.price_col).to_numpy().astype(np.float64)\n    pt = prices * (1.0 + self.profit_pct)\n    sl = prices * (1.0 - self.stop_loss_pct)\n\n    up_off, dn_off = _find_first_hit_static(prices, pt, sl, lf)\n\n    up_off_series = pl.Series(\"_up_off\", up_off).replace(0, None).cast(pl.Int32)\n    dn_off_series = pl.Series(\"_dn_off\", dn_off).replace(0, None).cast(pl.Int32)\n\n    df = group_df.with_columns([up_off_series, dn_off_series])\n\n    choose_up = pl.col(\"_up_off\").is_not_null() &amp; (\n        pl.col(\"_dn_off\").is_null() | (pl.col(\"_up_off\") &lt;= pl.col(\"_dn_off\"))\n    )\n    choose_dn = pl.col(\"_dn_off\").is_not_null() &amp; (\n        pl.col(\"_up_off\").is_null() | (pl.col(\"_dn_off\") &lt; pl.col(\"_up_off\"))\n    )\n\n    df = df.with_columns(\n        pl.when(choose_up)\n        .then(pl.lit(SignalType.RISE.value))\n        .when(choose_dn)\n        .then(pl.lit(SignalType.FALL.value))\n        .otherwise(pl.lit(SignalType.NONE.value))\n        .alias(self.out_col)\n    )\n\n    if self.include_meta:\n        ts_arr = group_df.get_column(self.ts_col).to_numpy()\n\n        up_np = up_off_series.fill_null(0).to_numpy()\n        dn_np = dn_off_series.fill_null(0).to_numpy()\n        idx = np.arange(n)\n\n        hit_off = np.where(\n            (up_np &gt; 0) &amp; ((dn_np == 0) | (up_np &lt;= dn_np)),\n            up_np,\n            np.where(dn_np &gt; 0, dn_np, 0),\n        )\n\n        hit_idx = np.clip(idx + hit_off, 0, n - 1)\n        vert_idx = np.clip(idx + lf, 0, n - 1)\n        final_idx = np.where(hit_off &gt; 0, hit_idx, vert_idx)\n\n        t_hit = ts_arr[final_idx]\n        ret = np.log(prices[final_idx] / prices)\n\n        df = df.with_columns(\n            [\n                pl.Series(\"t_hit\", t_hit),\n                pl.Series(\"ret\", ret),\n            ]\n        )\n\n    if (\n        self.mask_to_signals\n        and data_context is not None\n        and \"signal_keys\" in data_context\n    ):\n        df = self._apply_signal_mask(df, data_context, group_df)\n\n    df = df.drop([\"_up_off\", \"_dn_off\"])\n\n    return df\n</code></pre>"},{"location":"api/strategy/","title":"Strategy Module","text":""},{"location":"api/strategy/#execution","title":"Execution","text":""},{"location":"api/strategy/#signalflow.strategy.runner.backtest_runner.BacktestRunner","title":"signalflow.strategy.runner.backtest_runner.BacktestRunner  <code>dataclass</code>","text":"<pre><code>BacktestRunner(strategy_id: str = 'backtest', broker: Any = None, entry_rules: list[EntryRule] = list(), exit_rules: list[ExitRule] = list(), metrics: list[StrategyMetric] = list(), initial_capital: float = 10000.0, pair_col: str = 'pair', ts_col: str = 'timestamp', price_col: str = 'close', data_key: str = 'spot')\n</code></pre> <p>               Bases: <code>StrategyRunner</code></p> <p>Runs backtests over historical data.</p> Execution flow per bar <ol> <li>Mark prices on all positions</li> <li>Compute metrics</li> <li>Check and execute exits</li> <li>Check and execute entries</li> </ol> This order ensures <ul> <li>Metrics reflect current market state</li> <li>Exits are processed before entries (can close and re-enter same bar)</li> <li>No look-ahead bias</li> </ul>"},{"location":"api/strategy/#signalflow.strategy.runner.backtest_runner.BacktestRunner.metrics_df","title":"metrics_df  <code>property</code>","text":"<pre><code>metrics_df: DataFrame\n</code></pre> <p>Get metrics history as a DataFrame.</p>"},{"location":"api/strategy/#signalflow.strategy.runner.backtest_runner.BacktestRunner.trades","title":"trades  <code>property</code>","text":"<pre><code>trades: list[Trade]\n</code></pre> <p>Get all trades from the backtest.</p>"},{"location":"api/strategy/#signalflow.strategy.runner.backtest_runner.BacktestRunner.trades_df","title":"trades_df  <code>property</code>","text":"<pre><code>trades_df: DataFrame\n</code></pre> <p>Get trades as a DataFrame.</p>"},{"location":"api/strategy/#signalflow.strategy.runner.backtest_runner.BacktestRunner.get_results","title":"get_results","text":"<pre><code>get_results() -&gt; dict[str, Any]\n</code></pre> <p>Get backtest results summary.</p> Source code in <code>src/signalflow/strategy/runner/backtest_runner.py</code> <pre><code>def get_results(self) -&gt; dict[str, Any]:\n    \"\"\"Get backtest results summary.\"\"\"\n    trades_df = self.trades_df\n    metrics_df = self.metrics_df\n\n    results = {\n        'total_trades': len(self._trades),\n        'metrics_df': metrics_df,\n        'trades_df': trades_df,\n    }\n\n    if metrics_df.height &gt; 0 and 'total_return' in metrics_df.columns:\n        results['final_return'] = metrics_df.select('total_return').tail(1).item()\n        results['final_equity'] = metrics_df.select('equity').tail(1).item()\n\n    if trades_df.height &gt; 0:\n        entry_trades = trades_df.filter(pl.col('meta').struct.field('type') == 'entry')\n        exit_trades = trades_df.filter(pl.col('meta').struct.field('type') == 'exit')\n        results['entry_count'] = entry_trades.height\n        results['exit_count'] = exit_trades.height\n\n    return results\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.runner.backtest_runner.BacktestRunner.run","title":"run","text":"<pre><code>run(raw_data: RawData, signals: Signals, state: StrategyState | None = None) -&gt; StrategyState\n</code></pre> <p>Run backtest over the entire dataset.</p> <p>Parameters:</p> Name Type Description Default <code>raw_data</code> <code>RawData</code> <p>Historical OHLCV data</p> required <code>signals</code> <code>Signals</code> <p>Pre-computed signals for the period</p> required <code>state</code> <code>StrategyState | None</code> <p>Optional initial state (for continuing backtests)</p> <code>None</code> <p>Returns:</p> Type Description <code>StrategyState</code> <p>Final strategy state</p> Source code in <code>src/signalflow/strategy/runner/backtest_runner.py</code> <pre><code>def run(\n    self,\n    raw_data: RawData,\n    signals: Signals,\n    state: StrategyState | None = None\n) -&gt; StrategyState:\n    \"\"\"\n    Run backtest over the entire dataset.\n\n    Args:\n        raw_data: Historical OHLCV data\n        signals: Pre-computed signals for the period\n        state: Optional initial state (for continuing backtests)\n\n    Returns:\n        Final strategy state\n    \"\"\"\n    if state is None:\n        state = StrategyState(\n            strategy_id=self.strategy_id,\n        )\n        state.portfolio.cash = self.initial_capital\n\n    self._trades = []\n    self._metrics_history = []\n\n    # Get data\n    df = raw_data.get(self.data_key)\n    if df.height == 0:\n        logger.warning(\"No data to backtest\")\n        return state\n\n    timestamps = df.select(self.ts_col).unique().sort(self.ts_col).get_column(self.ts_col)\n\n    signals_df = signals.value if signals else pl.DataFrame()\n\n    logger.info(f\"Starting backtest: {len(timestamps)} bars, {signals_df.height} signals\")\n\n    for ts in tqdm(timestamps, desc=\"Processing bars\"):\n        state = self._process_bar(\n            ts=ts,\n            raw_df=df,\n            signals_df=signals_df,\n            state=state\n        )\n\n    logger.info(\n        f\"Backtest complete: {len(self._trades)} trades, \"\n        f\"{len(state.portfolio.open_positions())} open positions\"\n    )\n\n    return state\n</code></pre>"},{"location":"api/strategy/#exit-rules","title":"Exit Rules","text":""},{"location":"api/strategy/#signalflow.strategy.component.exit.tp_sl.TakeProfitStopLossExit","title":"signalflow.strategy.component.exit.tp_sl.TakeProfitStopLossExit  <code>dataclass</code>","text":"<pre><code>TakeProfitStopLossExit(take_profit_pct: float = 0.02, stop_loss_pct: float = 0.01, use_position_levels: bool = False)\n</code></pre> <p>               Bases: <code>ExitRule</code></p> <p>Exit rule based on take-profit and stop-loss levels.</p> <p>Can use fixed percentages or dynamic levels from position meta.</p>"},{"location":"api/strategy/#metrics","title":"Metrics","text":""},{"location":"api/strategy/#signalflow.strategy.component.metric","title":"signalflow.strategy.component.metric","text":""},{"location":"api/strategy/#signalflow.strategy.component.metric.__all__","title":"__all__  <code>module-attribute</code>","text":"<pre><code>__all__ = ['TotalReturnMetric', 'BalanceAllocationMetric', 'DrawdownMetric', 'WinRateMetric', 'SharpeRatioMetric']\n</code></pre>"},{"location":"api/strategy/#signalflow.strategy.component.metric.BalanceAllocationMetric","title":"BalanceAllocationMetric  <code>dataclass</code>","text":"<pre><code>BalanceAllocationMetric(initial_capital: float = 10000.0)\n</code></pre> <p>               Bases: <code>StrategyMetric</code></p>"},{"location":"api/strategy/#signalflow.strategy.component.metric.DrawdownMetric","title":"DrawdownMetric  <code>dataclass</code>","text":"<pre><code>DrawdownMetric(_peak_equity: float = 0.0, _max_drawdown: float = 0.0)\n</code></pre> <p>               Bases: <code>StrategyMetric</code></p>"},{"location":"api/strategy/#signalflow.strategy.component.metric.SharpeRatioMetric","title":"SharpeRatioMetric  <code>dataclass</code>","text":"<pre><code>SharpeRatioMetric(initial_capital: float = 10000.0, window_size: int = 100, risk_free_rate: float = 0.0, _returns_history: list[float] = None)\n</code></pre> <p>               Bases: <code>StrategyMetric</code></p>"},{"location":"api/strategy/#signalflow.strategy.component.metric.TotalReturnMetric","title":"TotalReturnMetric  <code>dataclass</code>","text":"<pre><code>TotalReturnMetric(initial_capital: float = 10000.0)\n</code></pre> <p>               Bases: <code>StrategyMetric</code></p> <p>Computes total return metrics for the portfolio.</p>"},{"location":"api/strategy/#signalflow.strategy.component.metric.WinRateMetric","title":"WinRateMetric  <code>dataclass</code>","text":"<pre><code>WinRateMetric()\n</code></pre> <p>               Bases: <code>StrategyMetric</code></p>"},{"location":"api/validator/","title":"Validator Module","text":""},{"location":"api/validator/#signalflow.validator.base.SignalValidator","title":"signalflow.validator.base.SignalValidator  <code>dataclass</code>","text":"<pre><code>SignalValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp')\n</code></pre> <p>Base class for signal validators (meta-labelers).</p> <p>Validates trading signals by predicting their risk/quality. In De Prado's terminology - this is a meta-labeler.</p> <p>Note: Filtering to active signals (RISE/FALL only) should be done BEFORE passing data to fit. This keeps the validator simple and gives users full control over data preparation.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>Any | None</code> <p>The trained model instance</p> <code>model_type</code> <code>str | None</code> <p>String identifier for model type (e.g., \"lightgbm\", \"xgboost\")</p> <code>model_params</code> <code>dict | None</code> <p>Parameters for model initialization</p> <code>train_params</code> <code>dict | None</code> <p>Parameters for training (e.g., early stopping)</p> <code>tune_enabled</code> <code>bool</code> <p>Whether hyperparameter tuning is enabled</p> <code>tune_params</code> <code>dict | None</code> <p>Parameters for tuning (e.g., n_trials, cv_folds)</p> <code>feature_columns</code> <code>list[str] | None</code> <p>List of feature column names (set after fit)</p>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.fit","title":"fit","text":"<pre><code>fit(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; SignalValidator\n</code></pre> <p>Train the validator model.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>Training features (Polars DataFrame)</p> required <code>y_train</code> <code>DataFrame | Series</code> <p>Training labels</p> required <code>X_val</code> <code>DataFrame | None</code> <p>Validation features (optional)</p> <code>None</code> <code>y_val</code> <code>DataFrame | Series | None</code> <p>Validation labels (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>SignalValidator</code> <p>Self for method chaining</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def fit(\n    self, \n    X_train: pl.DataFrame, \n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None, \n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; \"SignalValidator\":\n    \"\"\"Train the validator model.\n\n    Args:\n        X_train: Training features (Polars DataFrame)\n        y_train: Training labels\n        X_val: Validation features (optional)\n        y_val: Validation labels (optional)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement fit()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(path: str | Path) -&gt; SignalValidator\n</code></pre> <p>Load model from file.</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>@classmethod\ndef load(cls, path: str | Path) -&gt; \"SignalValidator\":\n    \"\"\"Load model from file.\"\"\"\n    raise NotImplementedError(\"Subclasses must implement load()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.predict","title":"predict","text":"<pre><code>predict(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class labels and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>X</code> <code>DataFrame</code> <p>Features (Polars DataFrame) with (pair, timestamp) + feature columns</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with prediction column added</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def predict(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class labels and return updated Signals.\n\n    Args:\n        signals: Input signals container\n        X: Features (Polars DataFrame) with (pair, timestamp) + feature columns\n\n    Returns:\n        New Signals with prediction column added\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement predict()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class probabilities and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container  </p> required <code>X</code> <code>DataFrame</code> <p>Features (Polars DataFrame)</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with probability columns added</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def predict_proba(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class probabilities and return updated Signals.\n\n    Args:\n        signals: Input signals container  \n        X: Features (Polars DataFrame)\n\n    Returns:\n        New Signals with probability columns added\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement predict_proba()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.save","title":"save","text":"<pre><code>save(path: str | Path) -&gt; None\n</code></pre> <p>Save model to file.</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save model to file.\"\"\"\n    raise NotImplementedError(\"Subclasses must implement save()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.tune","title":"tune","text":"<pre><code>tune(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Tune hyperparameters.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Best parameters found</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def tune(\n    self, \n    X_train: pl.DataFrame, \n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None, \n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Tune hyperparameters.\n\n    Returns:\n        Best parameters found\n    \"\"\"\n    if not self.tune_enabled:\n        raise ValueError(\"Tuning is not enabled for this validator\")\n    raise NotImplementedError(\"Subclasses must implement tune()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.base.SignalValidator.validate_signals","title":"validate_signals","text":"<pre><code>validate_signals(signals: Signals, features: DataFrame, prefix: str = 'probability_') -&gt; Signals\n</code></pre> <p>Add validation predictions to signals.</p> <p>Convenience method - calls predict_proba internally.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>features</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + feature columns</p> required <code>prefix</code> <code>str</code> <p>Prefix for probability columns</p> <code>'probability_'</code> <p>Returns:</p> Type Description <code>Signals</code> <p>Signals with added validation columns</p> Source code in <code>src/signalflow/validator/base.py</code> <pre><code>def validate_signals(\n    self, \n    signals: Signals, \n    features: pl.DataFrame,\n    prefix: str = \"probability_\",\n) -&gt; Signals:\n    \"\"\"Add validation predictions to signals.\n\n    Convenience method - calls predict_proba internally.\n\n    Args:\n        signals: Input signals container\n        features: Features DataFrame with (pair, timestamp) + feature columns\n        prefix: Prefix for probability columns\n\n    Returns:\n        Signals with added validation columns\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement validate_signals()\")\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator","title":"signalflow.validator.sklearn_validator.SklearnSignalValidator  <code>dataclass</code>","text":"<pre><code>SklearnSignalValidator(model: Any | None = None, model_type: str | None = None, model_params: dict | None = None, train_params: dict | None = None, tune_enabled: bool = False, tune_params: dict | None = None, feature_columns: list[str] | None = None, pair_col: str = 'pair', ts_col: str = 'timestamp', auto_select_metric: str = 'roc_auc', auto_select_cv_folds: int = 5)\n</code></pre> <p>               Bases: <code>SignalValidator</code></p> <p>Sklearn-based signal validator.</p> <p>Supports: - Multiple sklearn-compatible models (LightGBM, XGBoost, RF, etc.) - Automatic model selection via cross-validation - Hyperparameter tuning with Optuna - Early stopping for boosting models</p> <p>Note: Filter data to active signals (not NONE) BEFORE calling fit(). This gives you full control over data preparation.</p> Example"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator--prepare-data-filter-to-active-signals","title":"Prepare data - filter to active signals","text":"<p>df = df.filter(pl.col(\"signal_type\") != \"none\")</p> <p>validator = SklearnSignalValidator(model_type=\"lightgbm\") validator.fit( ...     train_df.select([\"pair\", \"timestamp\"] + feature_cols), ...     train_df.select(\"label\"), ... )</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator--validate_signals-returns-signals-object","title":"validate_signals returns Signals object","text":"<p>validated = validator.validate_signals( ...     Signals(test_df.select(signal_cols)), ...     test_df.select([\"pair\", \"timestamp\"] + feature_cols), ... ) validated.value.filter(pl.col(\"probability_rise\") &gt; 0.7)</p>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator.fit","title":"fit","text":"<pre><code>fit(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; SklearnSignalValidator\n</code></pre> <p>Train the validator.</p> <p>Note: Filter to active signals BEFORE calling this method.</p> <p>For boosting models with validation data, early stopping is applied.</p> <p>Parameters:</p> Name Type Description Default <code>X_train</code> <code>DataFrame</code> <p>Training features (already filtered to active signals)</p> required <code>y_train</code> <code>DataFrame | Series</code> <p>Training labels</p> required <code>X_val</code> <code>DataFrame | None</code> <p>Validation features (optional)</p> <code>None</code> <code>y_val</code> <code>DataFrame | Series | None</code> <p>Validation labels (optional)</p> <code>None</code> <p>Returns:</p> Type Description <code>SklearnSignalValidator</code> <p>Self for method chaining</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def fit(\n    self, \n    X_train: pl.DataFrame, \n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None, \n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; \"SklearnSignalValidator\":\n    \"\"\"Train the validator.\n\n    Note: Filter to active signals BEFORE calling this method.\n\n    For boosting models with validation data, early stopping is applied.\n\n    Args:\n        X_train: Training features (already filtered to active signals)\n        y_train: Training labels\n        X_val: Validation features (optional)\n        y_val: Validation labels (optional)\n\n    Returns:\n        Self for method chaining\n    \"\"\"\n    X_np = self._extract_features(X_train, fit_mode=True)\n    y_np = self._extract_labels(y_train)\n\n    if self.model_type == \"auto\" or self.model_type is None:\n        self.model_type, self.model_params = self._auto_select_model(X_np, y_np)\n\n    self.model = self._create_model(self.model_type, self.model_params)\n\n    fit_kwargs: dict[str, Any] = {}\n\n    if X_val is not None and y_val is not None:\n        X_val_np = self._extract_features(X_val)\n        y_val_np = self._extract_labels(y_val)\n\n        if self.model_type in (\"lightgbm\", \"xgboost\"):\n            early_stopping = self.train_params.get(\"early_stopping_rounds\", 50)\n\n            if self.model_type == \"lightgbm\":\n                fit_kwargs[\"eval_set\"] = [(X_val_np, y_val_np)]\n                fit_kwargs[\"callbacks\"] = [\n                    __import__(\"lightgbm\").early_stopping(early_stopping, verbose=False)\n                ]\n            elif self.model_type == \"xgboost\":\n                fit_kwargs[\"eval_set\"] = [(X_val_np, y_val_np)]\n                fit_kwargs[\"early_stopping_rounds\"] = early_stopping\n                fit_kwargs[\"verbose\"] = False\n\n    self.model.fit(X_np, y_np, **fit_kwargs)\n\n    return self\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator.load","title":"load  <code>classmethod</code>","text":"<pre><code>load(path: str | Path) -&gt; SklearnSignalValidator\n</code></pre> <p>Load validator from file.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>@classmethod\ndef load(cls, path: str | Path) -&gt; \"SklearnSignalValidator\":\n    \"\"\"Load validator from file.\"\"\"\n    path = Path(path)\n\n    with open(path, \"rb\") as f:\n        state = pickle.load(f)\n\n    validator = cls(\n        model=state[\"model\"],\n        model_type=state[\"model_type\"],\n        model_params=state[\"model_params\"],\n        train_params=state[\"train_params\"],\n        tune_params=state[\"tune_params\"],\n        feature_columns=state[\"feature_columns\"],\n        pair_col=state.get(\"pair_col\", \"pair\"),\n        ts_col=state.get(\"ts_col\", \"timestamp\"),\n    )\n\n    return validator\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator.predict","title":"predict","text":"<pre><code>predict(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class labels and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>X</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + feature columns</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with 'validation_pred' column added</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def predict(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class labels and return updated Signals.\n\n    Args:\n        signals: Input signals container\n        X: Features DataFrame with (pair, timestamp) + feature columns\n\n    Returns:\n        New Signals with 'validation_pred' column added\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not fitted. Call fit() first.\")\n\n    signals_df = signals.value\n\n    # Join features to signals by keys\n    X_matched = signals_df.select([self.pair_col, self.ts_col]).join(\n        X,\n        on=[self.pair_col, self.ts_col],\n        how=\"left\",\n    )\n\n    X_np = self._extract_features(X_matched)\n    predictions = self.model.predict(X_np)\n\n    result_df = signals_df.with_columns(\n        pl.Series(name=\"validation_pred\", values=predictions)\n    )\n\n    return Signals(result_df)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator.predict_proba","title":"predict_proba","text":"<pre><code>predict_proba(signals: Signals, X: DataFrame) -&gt; Signals\n</code></pre> <p>Predict class probabilities and return updated Signals.</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input signals container</p> required <code>X</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + feature columns</p> required <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with probability columns (probability_none, probability_rise, probability_fall)</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def predict_proba(self, signals: Signals, X: pl.DataFrame) -&gt; Signals:\n    \"\"\"Predict class probabilities and return updated Signals.\n\n    Args:\n        signals: Input signals container\n        X: Features DataFrame with (pair, timestamp) + feature columns\n\n    Returns:\n        New Signals with probability columns (probability_none, probability_rise, probability_fall)\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not fitted. Call fit() first.\")\n\n    signals_df = signals.value\n    classes = self._get_class_labels()\n\n    # Join features to signals by keys\n    X_matched = signals_df.select([self.pair_col, self.ts_col]).join(\n        X,\n        on=[self.pair_col, self.ts_col],\n        how=\"left\",\n    )\n\n    X_np = self._extract_features(X_matched)\n    probas = self.model.predict_proba(X_np)\n\n    # Add probability columns\n    result_df = signals_df\n    for i, class_label in enumerate(classes):\n        col_name = f\"probability_{class_label}\"\n        result_df = result_df.with_columns(\n            pl.Series(name=col_name, values=probas[:, i])\n        )\n\n    return Signals(result_df)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator.save","title":"save","text":"<pre><code>save(path: str | Path) -&gt; None\n</code></pre> <p>Save validator to file.</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def save(self, path: str | Path) -&gt; None:\n    \"\"\"Save validator to file.\"\"\"\n    path = Path(path)\n\n    state = {\n        \"model\": self.model,\n        \"model_type\": self.model_type,\n        \"model_params\": self.model_params,\n        \"train_params\": self.train_params,\n        \"tune_params\": self.tune_params,\n        \"feature_columns\": self.feature_columns,\n        \"pair_col\": self.pair_col,\n        \"ts_col\": self.ts_col,\n    }\n\n    with open(path, \"wb\") as f:\n        pickle.dump(state, f)\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator.tune","title":"tune","text":"<pre><code>tune(X_train: DataFrame, y_train: DataFrame | Series, X_val: DataFrame | None = None, y_val: DataFrame | Series | None = None) -&gt; dict[str, Any]\n</code></pre> <p>Tune hyperparameters using Optuna.</p> <p>Note: Filter to active signals BEFORE calling this method.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Best parameters found</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def tune(\n    self, \n    X_train: pl.DataFrame, \n    y_train: pl.DataFrame | pl.Series,\n    X_val: pl.DataFrame | None = None, \n    y_val: pl.DataFrame | pl.Series | None = None,\n) -&gt; dict[str, Any]:\n    \"\"\"Tune hyperparameters using Optuna.\n\n    Note: Filter to active signals BEFORE calling this method.\n\n    Returns:\n        Best parameters found\n    \"\"\"\n    import optuna\n    from sklearn.model_selection import cross_val_score\n\n    if self.model_type is None or self.model_type == \"auto\":\n        raise ValueError(\"Set model_type before tuning (not 'auto')\")\n\n    config = self._get_model_config(self.model_type)\n    tune_space = config[\"tune_space\"]\n\n    X_np = self._extract_features(X_train, fit_mode=True)\n    y_np = self._extract_labels(y_train)\n\n    n_trials = self.tune_params.get(\"n_trials\", 50)\n    cv_folds = self.tune_params.get(\"cv_folds\", 5)\n    timeout = self.tune_params.get(\"timeout\", 600)\n\n    def objective(trial: optuna.Trial) -&gt; float:\n        params = build_optuna_params(trial, tune_space)\n        params.update(config[\"default_params\"])  # Base params\n\n        model = self._create_model(self.model_type, params)\n\n        scores = cross_val_score(\n            model, X_np, y_np,\n            cv=cv_folds,\n            scoring=self.auto_select_metric,\n            n_jobs=-1,\n        )\n        return scores.mean()\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(\n        objective, \n        n_trials=n_trials, \n        timeout=timeout,\n        show_progress_bar=True,\n    )\n\n    best_params = {**config[\"default_params\"], **study.best_params}\n    self.model_params = best_params\n\n    return best_params\n</code></pre>"},{"location":"api/validator/#signalflow.validator.sklearn_validator.SklearnSignalValidator.validate_signals","title":"validate_signals","text":"<pre><code>validate_signals(signals: Signals, features: DataFrame, prefix: str = 'probability_') -&gt; Signals\n</code></pre> <p>Add validation probabilities to signals.</p> <p>Adds probability columns for each class: - probability_none: P(signal is noise / not actionable) - probability_rise: P(signal leads to price rise) - probability_fall: P(signal leads to price fall)</p> <p>Parameters:</p> Name Type Description Default <code>signals</code> <code>Signals</code> <p>Input Signals container</p> required <code>features</code> <code>DataFrame</code> <p>Features DataFrame with (pair, timestamp) + features</p> required <code>prefix</code> <code>str</code> <p>Prefix for probability columns (default: \"probability_\")</p> <code>'probability_'</code> <p>Returns:</p> Type Description <code>Signals</code> <p>New Signals with probability columns added.</p> Example <p>validated = validator.validate_signals(signals, features) df = validated.value confident_rise = df.filter( ...     (pl.col(\"signal_type\") == \"rise\") &amp;  ...     (pl.col(\"probability_rise\") &gt; 0.7) ... )</p> Source code in <code>src/signalflow/validator/sklearn_validator.py</code> <pre><code>def validate_signals(\n    self, \n    signals: Signals, \n    features: pl.DataFrame,\n    prefix: str = \"probability_\",\n) -&gt; Signals:\n    \"\"\"Add validation probabilities to signals.\n\n    Adds probability columns for each class:\n    - probability_none: P(signal is noise / not actionable)\n    - probability_rise: P(signal leads to price rise)  \n    - probability_fall: P(signal leads to price fall)\n\n    Args:\n        signals: Input Signals container\n        features: Features DataFrame with (pair, timestamp) + features\n        prefix: Prefix for probability columns (default: \"probability_\")\n\n    Returns:\n        New Signals with probability columns added.\n\n    Example:\n        &gt;&gt;&gt; validated = validator.validate_signals(signals, features)\n        &gt;&gt;&gt; df = validated.value\n        &gt;&gt;&gt; confident_rise = df.filter(\n        ...     (pl.col(\"signal_type\") == \"rise\") &amp; \n        ...     (pl.col(\"probability_rise\") &gt; 0.7)\n        ... )\n    \"\"\"\n    return self.predict_proba(signals, features)\n</code></pre>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get SignalFlow up and running in minutes.</p>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>4GB RAM minimum (16GB recommended for backtesting)</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":""},{"location":"getting-started/installation/#standard-installation","title":"Standard Installation","text":"<pre><code>pip install signalflow-trading\n</code></pre>"},{"location":"getting-started/installation/#with-neural-networks","title":"With Neural Networks","text":"<p>For deep learning validators (LSTM, Transformers):</p> <pre><code>pip install signalflow-trading[nn]\n</code></pre>"},{"location":"getting-started/installation/#virtual-environment-recommended","title":"Virtual Environment (Recommended)","text":"<pre><code># Create environment\npython -m venv signalflow-env\nsource signalflow-env/bin/activate  # Windows: signalflow-env\\Scripts\\activate\n\n# Install\npip install signalflow-trading[nn]\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>import signalflow\nfrom signalflow.core import RawData, Signals\nfrom signalflow.detector import SmaCrossSignalDetector\n\nprint(f\"SignalFlow {signalflow.__version__} installed \u2713\")\n</code></pre>"},{"location":"getting-started/installation/#platform-notes","title":"Platform Notes","text":"<p>=== \"Linux\"     Works out of the box.</p> <p>=== \"macOS\"     Supports both Intel and Apple Silicon (M1/M2/M3).</p> <p>=== \"Windows\"     Works in Command Prompt or PowerShell.</p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors? <pre><code>pip install --force-reinstall signalflow-trading\n</code></pre></p> <p>GPU not detected? <pre><code># Check CUDA version first: nvidia-smi\npip install torch --index-url https://download.pytorch.org/whl/cu121\npip install signalflow-nn\n</code></pre></p> <p>Python version too old? <pre><code>python --version  # Must be 3.12+\n</code></pre></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li> <p> Quick Start</p> <p>Build your first strategy in 10 minutes</p> </li> <li> <p> Configuration</p> <p>Configure components and parameters</p> </li> </ul> <p>Need help? pathway2nothing@gmail.com</p>"}]}