{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# FundingRateDetector - Signal Detection from Funding Rate Transitions\n\nThis notebook demonstrates:\n1. Loading perpetual futures data from **3 exchanges** (Binance, OKX, Bybit) for the last 3 months\n2. Computing **Aggregated Open Interest** across all pairs\n3. Building a `FundingRateDetector` that generates **RISE** signals when funding rate transitions from positive series to negative\n4. Signal metrics visualization"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from __future__ import annotations\n\nimport asyncio\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom typing import Any, ClassVar\n\nimport aiohttp\nimport polars as pl\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nfrom signalflow.core import RawData, Signals, RawDataView\nfrom signalflow.core.enums import RawDataType, SignalCategory, SfComponentType\nfrom signalflow.core import sf_component\nfrom signalflow.detector.base import SignalDetector\nfrom signalflow.data.source import (\n    BinanceFuturesUsdtLoader,\n    OkxFuturesLoader,\n    BybitFuturesLoader,\n)\nfrom signalflow.data.raw_store import DuckDbRawStore, DuckDbSpotStore\nfrom signalflow.feature import AggregatedOpenInterest, AggregatedOpenInterestMultiSource\nfrom signalflow.analytic.signals import (\n    SignalDistributionMetric,\n    SignalProfileMetric,\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Load Perpetual Data from 3 Exchanges\n\nDownload perpetual futures data from Binance, OKX, and Bybit for the last 3 months.\n\n**Data availability:**\n- **Binance**: OHLCV + `funding_rate` + `open_interest`\n- **OKX**: OHLCV only (funding rate via separate API)\n- **Bybit**: OHLCV only (funding rate via separate API)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nDATA_DIR = Path(\"../data\")\nDATA_DIR.mkdir(exist_ok=True)\n\n# Database paths for each exchange\nDB_BINANCE = DATA_DIR / \"perpetual_binance.duckdb\"\nDB_OKX = DATA_DIR / \"perpetual_okx.duckdb\"\nDB_BYBIT = DATA_DIR / \"perpetual_bybit.duckdb\"\n\nTIMEFRAME = \"8h\"  # Funding rate is typically every 8 hours\n\n# Date range: last 3 months\nEND_DATE = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)\nSTART_DATE = END_DATE - timedelta(days=90)\n\nprint(f\"Date range: {START_DATE} to {END_DATE}\")\nprint(f\"Timeframe: {TIMEFRAME}\")\nprint(f\"Databases: {DB_BINANCE.name}, {DB_OKX.name}, {DB_BYBIT.name}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def get_binance_perpetual_pairs() -> list[str]:\n    \"\"\"Fetch all USDT perpetual pairs from Binance.\"\"\"\n    url = \"https://fapi.binance.com/fapi/v1/exchangeInfo\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            data = await resp.json()\n    return sorted([\n        s[\"symbol\"] for s in data[\"symbols\"]\n        if s[\"contractType\"] == \"PERPETUAL\"\n        and s[\"quoteAsset\"] == \"USDT\"\n        and s[\"status\"] == \"TRADING\"\n    ])\n\nasync def get_okx_perpetual_pairs() -> list[str]:\n    \"\"\"Fetch all USDT perpetual pairs from OKX.\"\"\"\n    url = \"https://www.okx.com/api/v5/public/instruments?instType=SWAP\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            data = await resp.json()\n    # Convert OKX format (BTC-USDT-SWAP) to standard (BTCUSDT)\n    pairs = []\n    for inst in data.get(\"data\", []):\n        if inst[\"settleCcy\"] == \"USDT\" and inst[\"state\"] == \"live\":\n            # BTC-USDT-SWAP -> BTCUSDT\n            base = inst[\"instId\"].split(\"-\")[0]\n            pairs.append(f\"{base}USDT\")\n    return sorted(pairs)\n\nasync def get_bybit_perpetual_pairs() -> list[str]:\n    \"\"\"Fetch all USDT perpetual pairs from Bybit.\"\"\"\n    url = \"https://api.bybit.com/v5/market/instruments-info?category=linear\"\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as resp:\n            data = await resp.json()\n    return sorted([\n        s[\"symbol\"] for s in data.get(\"result\", {}).get(\"list\", [])\n        if s[\"quoteCoin\"] == \"USDT\"\n        and s[\"status\"] == \"Trading\"\n        and s[\"contractType\"] == \"LinearPerpetual\"\n    ])\n\n# Get pairs from all exchanges\nbinance_pairs, okx_pairs, bybit_pairs = await asyncio.gather(\n    get_binance_perpetual_pairs(),\n    get_okx_perpetual_pairs(),\n    get_bybit_perpetual_pairs(),\n)\n\nprint(f\"Binance: {len(binance_pairs)} pairs\")\nprint(f\"OKX: {len(okx_pairs)} pairs\")\nprint(f\"Bybit: {len(bybit_pairs)} pairs\")\n\n# Find common pairs across all exchanges\ncommon_pairs = sorted(set(binance_pairs) & set(okx_pairs) & set(bybit_pairs))\nprint(f\"\\nCommon pairs (all 3 exchanges): {len(common_pairs)}\")\nprint(f\"Examples: {common_pairs[:10]}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create stores and loaders for each exchange\n\n# Binance - perpetual data with funding_rate and open_interest\nstore_binance = DuckDbRawStore(\n    db_path=DB_BINANCE,\n    data_type=\"perpetual\",\n    timeframe=TIMEFRAME,\n)\nloader_binance = BinanceFuturesUsdtLoader(store=store_binance, timeframe=TIMEFRAME)\n\n# OKX - OHLCV only (spot store schema)\nstore_okx = DuckDbSpotStore(db_path=DB_OKX, timeframe=TIMEFRAME)\nloader_okx = OkxFuturesLoader(store=store_okx, timeframe=TIMEFRAME)\n\n# Bybit - OHLCV only (spot store schema)\nstore_bybit = DuckDbSpotStore(db_path=DB_BYBIT, timeframe=TIMEFRAME)\nloader_bybit = BybitFuturesLoader(store=store_bybit, timeframe=TIMEFRAME)\n\nprint(\"Loaders initialized:\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download data from all exchanges (skip if already exists)\n# Using common_pairs to ensure we have data for same pairs across all exchanges\n\nasync def download_all():\n    tasks = []\n    \n    if not DB_BINANCE.exists():\n        print(\"Downloading Binance data...\")\n        tasks.append(loader_binance.download(\n            pairs=common_pairs,\n            start=START_DATE,\n            end=END_DATE,\n            fill_gaps=True,\n        ))\n    else:\n        print(f\"Binance: Using existing data from {DB_BINANCE.name}\")\n    \n    if not DB_OKX.exists():\n        print(\"Downloading OKX data...\")\n        tasks.append(loader_okx.download(\n            pairs=common_pairs,\n            start=START_DATE,\n            end=END_DATE,\n            fill_gaps=True,\n        ))\n    else:\n        print(f\"OKX: Using existing data from {DB_OKX.name}\")\n    \n    if not DB_BYBIT.exists():\n        print(\"Downloading Bybit data...\")\n        tasks.append(loader_bybit.download(\n            pairs=common_pairs,\n            start=START_DATE,\n            end=END_DATE,\n            fill_gaps=True,\n        ))\n    else:\n        print(f\"Bybit: Using existing data from {DB_BYBIT.name}\")\n    \n    if tasks:\n        await asyncio.gather(*tasks)\n        print(\"\\nDownload complete!\")\n\nawait download_all()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load data from all exchanges\n# Binance has funding_rate and open_interest\ndf_binance = store_binance.load_many(pairs=common_pairs, start=START_DATE, end=END_DATE)\ndf_binance = df_binance.with_columns(pl.lit(\"binance\").alias(\"source\"))\n\n# OKX and Bybit have OHLCV only\ndf_okx = store_okx.load_many(pairs=common_pairs, start=START_DATE, end=END_DATE)\ndf_okx = df_okx.with_columns(pl.lit(\"okx\").alias(\"source\"))\n\ndf_bybit = store_bybit.load_many(pairs=common_pairs, start=START_DATE, end=END_DATE)\ndf_bybit = df_bybit.with_columns(pl.lit(\"bybit\").alias(\"source\"))\n\nprint(f\"Binance: {len(df_binance):,} rows, columns: {df_binance.columns}\")\nprint(f\"OKX: {len(df_okx):,} rows, columns: {df_okx.columns}\")\nprint(f\"Bybit: {len(df_bybit):,} rows, columns: {df_bybit.columns}\")\n\n# Use Binance data as primary (has funding_rate and open_interest)\nraw_df = df_binance\nprint(f\"\\nUsing Binance as primary source (with funding_rate & open_interest)\")\nprint(f\"Total pairs: {raw_df['pair'].n_unique()}\")\nprint(f\"Date range: {raw_df['timestamp'].min()} to {raw_df['timestamp'].max()}\")\n\nraw_df.head(5)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check funding rate statistics\n",
    "print(\"Funding Rate Statistics:\")\n",
    "raw_df.select(\n",
    "    pl.col(\"funding_rate\").mean().alias(\"mean\"),\n",
    "    pl.col(\"funding_rate\").std().alias(\"std\"),\n",
    "    pl.col(\"funding_rate\").min().alias(\"min\"),\n",
    "    pl.col(\"funding_rate\").max().alias(\"max\"),\n",
    "    pl.col(\"funding_rate\").quantile(0.25).alias(\"q25\"),\n",
    "    pl.col(\"funding_rate\").quantile(0.75).alias(\"q75\"),\n",
    ")\n",
    "\n",
    "print(\"\\nOpen Interest Statistics:\")\n",
    "raw_df.select(\n",
    "    pl.col(\"open_interest\").mean().alias(\"mean\"),\n",
    "    pl.col(\"open_interest\").std().alias(\"std\"),\n",
    "    pl.col(\"open_interest\").min().alias(\"min\"),\n",
    "    pl.col(\"open_interest\").max().alias(\"max\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Aggregated Open Interest Feature\n\nCalculate market-wide aggregated open interest from Binance (the only source with OI in klines).\n\nFor multi-exchange analysis, we also compute **aggregated volume** across all 3 exchanges."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Aggregated Open Interest from Binance\nagg_oi_feature = AggregatedOpenInterest(\n    zscore_window=21,  # ~7 days at 8h timeframe\n    include_pair_count=True,\n)\n\n# Add aggregated OI to Binance data\nraw_df_with_agg_oi = agg_oi_feature.compute(raw_df)\n\nprint(\"Aggregated OI columns added:\")\nprint([c for c in raw_df_with_agg_oi.columns if \"agg_oi\" in c or c == \"n_pairs\"])\n\n# Show sample\nraw_df_with_agg_oi.select([\n    \"pair\", \"timestamp\", \"close\", \"funding_rate\", \"open_interest\",\n    \"agg_oi\", \"agg_oi_change\", \"agg_oi_zscore\"\n]).head(10)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique aggregated OI time series for visualization\n",
    "agg_oi_ts = (\n",
    "    raw_df_with_agg_oi\n",
    "    .select([\"timestamp\", \"agg_oi\", \"agg_oi_change\", \"agg_oi_zscore\"])\n",
    "    .unique(subset=[\"timestamp\"])\n",
    "    .sort(\"timestamp\")\n",
    ")\n",
    "\n",
    "# Visualize aggregated open interest\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=1,\n",
    "    subplot_titles=(\n",
    "        \"Aggregated Open Interest (All Pairs)\",\n",
    "        \"Aggregated OI Change (%)\",\n",
    "        \"Aggregated OI Z-Score\"\n",
    "    ),\n",
    "    shared_xaxes=True,\n",
    "    vertical_spacing=0.08,\n",
    ")\n",
    "\n",
    "# Raw aggregated OI\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=agg_oi_ts[\"timestamp\"].to_list(),\n",
    "        y=agg_oi_ts[\"agg_oi\"].to_list(),\n",
    "        mode=\"lines\",\n",
    "        name=\"Agg OI\",\n",
    "        line=dict(color=\"#2171b5\", width=1.5),\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# OI change\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=agg_oi_ts[\"timestamp\"].to_list(),\n",
    "        y=(agg_oi_ts[\"agg_oi_change\"] * 100).fill_null(0).to_list(),\n",
    "        mode=\"lines\",\n",
    "        name=\"OI Change %\",\n",
    "        line=dict(color=\"#41ab5d\", width=1),\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Z-score with threshold lines\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=agg_oi_ts[\"timestamp\"].to_list(),\n",
    "        y=agg_oi_ts[\"agg_oi_zscore\"].fill_null(0).to_list(),\n",
    "        mode=\"lines\",\n",
    "        name=\"Z-Score\",\n",
    "        line=dict(color=\"#d94801\", width=1),\n",
    "    ),\n",
    "    row=3, col=1\n",
    ")\n",
    "fig.add_hline(y=2, line_dash=\"dash\", line_color=\"red\", row=3, col=1)\n",
    "fig.add_hline(y=-2, line_dash=\"dash\", line_color=\"green\", row=3, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=700,\n",
    "    title=\"Market-Wide Aggregated Open Interest\",\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.update_yaxes(title_text=\"OI (USD)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Change %\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Z-Score\", row=3, col=1)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Multi-Exchange Aggregated Volume\n\nSince OKX and Bybit don't provide OI in klines API, we compute aggregated volume across all 3 exchanges as an alternative market sentiment indicator.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Combine OHLCV from all 3 exchanges for volume analysis\n# Normalize columns to common schema\ncommon_cols = [\"pair\", \"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"source\"]\n\ndf_all_exchanges = pl.concat([\n    df_binance.select([c for c in common_cols if c in df_binance.columns]),\n    df_okx.select([c for c in common_cols if c in df_okx.columns]),\n    df_bybit.select([c for c in common_cols if c in df_bybit.columns]),\n], how=\"diagonal\")\n\nprint(f\"Combined data: {len(df_all_exchanges):,} rows\")\nprint(f\"Sources: {df_all_exchanges['source'].unique().to_list()}\")\n\n# Compute aggregated volume per timestamp across all exchanges\nagg_volume = (\n    df_all_exchanges\n    .group_by(\"timestamp\")\n    .agg([\n        pl.col(\"volume\").sum().alias(\"agg_volume_total\"),\n        # Per-exchange volumes\n        pl.col(\"volume\").filter(pl.col(\"source\") == \"binance\").sum().alias(\"agg_volume_binance\"),\n        pl.col(\"volume\").filter(pl.col(\"source\") == \"okx\").sum().alias(\"agg_volume_okx\"),\n        pl.col(\"volume\").filter(pl.col(\"source\") == \"bybit\").sum().alias(\"agg_volume_bybit\"),\n    ])\n    .sort(\"timestamp\")\n)\n\n# Add change and z-score\nagg_volume = agg_volume.with_columns([\n    (pl.col(\"agg_volume_total\") / pl.col(\"agg_volume_total\").shift(1) - 1).alias(\"agg_volume_change\"),\n    ((pl.col(\"agg_volume_total\") - pl.col(\"agg_volume_total\").rolling_mean(21)) \n     / pl.col(\"agg_volume_total\").rolling_std(21)).alias(\"agg_volume_zscore\"),\n])\n\nprint(f\"\\nAggregated volume time series: {len(agg_volume)} points\")\nagg_volume.head(10)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize multi-exchange volume comparison\nfig = make_subplots(\n    rows=2, cols=1,\n    subplot_titles=(\n        \"Aggregated Volume by Exchange\",\n        \"Volume Market Share (%)\"\n    ),\n    shared_xaxes=True,\n    vertical_spacing=0.12,\n)\n\n# Stacked area chart of volume by exchange\nfor source, color in [(\"binance\", \"#F0B90B\"), (\"okx\", \"#000000\"), (\"bybit\", \"#F7A600\")]:\n    col = f\"agg_volume_{source}\"\n    fig.add_trace(\n        go.Scatter(\n            x=agg_volume[\"timestamp\"].to_list(),\n            y=agg_volume[col].fill_null(0).to_list(),\n            mode=\"lines\",\n            name=source.capitalize(),\n            stackgroup=\"volume\",\n            line=dict(width=0.5),\n            fillcolor=color,\n        ),\n        row=1, col=1\n    )\n\n# Market share over time\nfor source, color in [(\"binance\", \"#F0B90B\"), (\"okx\", \"#121212\"), (\"bybit\", \"#F7A600\")]:\n    col = f\"agg_volume_{source}\"\n    share = (agg_volume[col].fill_null(0) / agg_volume[\"agg_volume_total\"] * 100)\n    fig.add_trace(\n        go.Scatter(\n            x=agg_volume[\"timestamp\"].to_list(),\n            y=share.to_list(),\n            mode=\"lines\",\n            name=f\"{source.capitalize()} %\",\n            line=dict(color=color, width=1.5),\n        ),\n        row=2, col=1\n    )\n\nfig.update_layout(\n    height=600,\n    title=\"Multi-Exchange Volume Analysis (Binance, OKX, Bybit)\",\n    showlegend=True,\n    legend=dict(x=1.02, y=1),\n)\nfig.update_yaxes(title_text=\"Volume (USD)\", row=1, col=1)\nfig.update_yaxes(title_text=\"Market Share %\", row=2, col=1)\nfig.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FundingRateDetector Implementation\n",
    "\n",
    "**Logic:**\n",
    "- Track consecutive positive funding rate periods per pair\n",
    "- Generate **RISE** signal when:\n",
    "  - Previous `n` funding rates were all positive (overbought longs)\n",
    "  - Current funding rate turns negative (shorts taking over = potential reversal up)\n",
    "\n",
    "**Rationale:** When funding switches from positive to negative after a sustained positive streak, it often indicates that overleveraged longs are being squeezed out, creating buying opportunities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "@sf_component(name=\"funding_rate_detector\")\n",
    "class FundingRateDetector(SignalDetector):\n",
    "    \"\"\"Detects potential reversals based on funding rate transitions.\n",
    "    \n",
    "    Generates RISE signal when:\n",
    "    - Previous `min_positive_streak` funding rates were all positive\n",
    "    - Current funding rate turns negative\n",
    "    \n",
    "    This pattern suggests overleveraged longs are exiting, potentially\n",
    "    creating upward price pressure as shorts cover.\n",
    "    \n",
    "    Attributes:\n",
    "        min_positive_streak: Minimum consecutive positive funding rates\n",
    "            before transition to negative triggers a signal.\n",
    "        funding_col: Column name for funding rate data.\n",
    "    \"\"\"\n",
    "    \n",
    "    component_type: ClassVar[SfComponentType] = SfComponentType.DETECTOR\n",
    "    signal_category: SignalCategory = SignalCategory.PRICE_DIRECTION\n",
    "    raw_data_type: RawDataType | str = RawDataType.PERPETUAL\n",
    "    \n",
    "    # Detector parameters\n",
    "    min_positive_streak: int = 3  # Minimum consecutive positive funding rates\n",
    "    funding_col: str = \"funding_rate\"\n",
    "    \n",
    "    # Signal types\n",
    "    allowed_signal_types: set[str] | None = field(\n",
    "        default_factory=lambda: {\"rise\"}\n",
    "    )\n",
    "    \n",
    "    def preprocess(\n",
    "        self,\n",
    "        raw_data_view: RawDataView,\n",
    "        context: dict[str, Any] | None = None,\n",
    "    ) -> pl.DataFrame:\n",
    "        \"\"\"Extract perpetual data with funding rates.\"\"\"\n",
    "        key = (\n",
    "            self.raw_data_type.value \n",
    "            if hasattr(self.raw_data_type, \"value\") \n",
    "            else str(self.raw_data_type)\n",
    "        )\n",
    "        df = raw_data_view.to_polars(key)\n",
    "        return df.sort([self.pair_col, self.ts_col])\n",
    "    \n",
    "    def detect(\n",
    "        self,\n",
    "        features: pl.DataFrame,\n",
    "        context: dict[str, Any] | None = None,\n",
    "    ) -> Signals:\n",
    "        \"\"\"Detect funding rate transition signals.\n",
    "        \n",
    "        Signal Logic:\n",
    "        1. Compute whether funding rate is positive for each row\n",
    "        2. Count consecutive positive funding rates (streak)\n",
    "        3. Detect when current funding turns negative after streak >= min_positive_streak\n",
    "        \"\"\"\n",
    "        funding = pl.col(self.funding_col)\n",
    "        pair = pl.col(self.pair_col)\n",
    "        \n",
    "        # Step 1: Mark positive funding rates\n",
    "        df = features.with_columns(\n",
    "            (funding > 0).cast(pl.Int32).alias(\"_is_positive\"),\n",
    "            (funding < 0).alias(\"_is_negative\"),\n",
    "        )\n",
    "        \n",
    "        # Step 2: Calculate consecutive positive streak using cumsum trick\n",
    "        # Reset counter when funding goes non-positive\n",
    "        df = df.with_columns(\n",
    "            # Create groups that reset when positive streak breaks\n",
    "            (pl.col(\"_is_positive\") == 0)\n",
    "            .cum_sum()\n",
    "            .over(pair)\n",
    "            .alias(\"_streak_group\"),\n",
    "        )\n",
    "        \n",
    "        # Count within each streak group\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"_is_positive\")\n",
    "            .cum_sum()\n",
    "            .over([pair, \"_streak_group\"])\n",
    "            .alias(\"_positive_streak\"),\n",
    "        )\n",
    "        \n",
    "        # Step 3: Get previous streak length (before current row)\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"_positive_streak\")\n",
    "            .shift(1)\n",
    "            .over(pair)\n",
    "            .fill_null(0)\n",
    "            .alias(\"_prev_streak\"),\n",
    "        )\n",
    "        \n",
    "        # Step 4: Signal when transitioning from positive streak to negative\n",
    "        signal_condition = (\n",
    "            pl.col(\"_is_negative\") &  # Current is negative\n",
    "            (pl.col(\"_prev_streak\") >= self.min_positive_streak)  # Had enough positive streak\n",
    "        )\n",
    "        \n",
    "        # Build signals DataFrame\n",
    "        signals_df = (\n",
    "            df.with_columns(\n",
    "                pl.when(signal_condition)\n",
    "                .then(pl.lit(\"rise\"))\n",
    "                .otherwise(pl.lit(None, dtype=pl.Utf8))\n",
    "                .alias(\"signal_type\"),\n",
    "                # Probability based on streak length (longer streak = stronger signal)\n",
    "                pl.when(signal_condition)\n",
    "                .then(\n",
    "                    (pl.col(\"_prev_streak\") / (self.min_positive_streak * 3))\n",
    "                    .clip(0.5, 1.0)\n",
    "                )\n",
    "                .otherwise(pl.lit(None, dtype=pl.Float64))\n",
    "                .alias(\"probability\"),\n",
    "                # Include streak length for analysis\n",
    "                pl.when(signal_condition)\n",
    "                .then(pl.col(\"_prev_streak\"))\n",
    "                .otherwise(pl.lit(None, dtype=pl.Int32))\n",
    "                .alias(\"streak_length\"),\n",
    "            )\n",
    "            .filter(pl.col(\"signal_type\").is_not_null())\n",
    "            .select([\n",
    "                self.pair_col,\n",
    "                self.ts_col,\n",
    "                \"signal_type\",\n",
    "                pl.lit(1).alias(\"signal\"),\n",
    "                \"probability\",\n",
    "                \"streak_length\",\n",
    "                self.funding_col,  # Include funding rate for analysis\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        return Signals(signals_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RawData object\n",
    "raw_data = RawData(perpetual=raw_df)\n",
    "\n",
    "# Initialize detector with different streak lengths to compare\n",
    "detector_n3 = FundingRateDetector(min_positive_streak=3)\n",
    "detector_n5 = FundingRateDetector(min_positive_streak=5)\n",
    "detector_n7 = FundingRateDetector(min_positive_streak=7)\n",
    "\n",
    "# Run detection\n",
    "signals_n3 = detector_n3.run(raw_data.view())\n",
    "signals_n5 = detector_n5.run(raw_data.view())\n",
    "signals_n7 = detector_n7.run(raw_data.view())\n",
    "\n",
    "print(f\"Signals (n=3): {len(signals_n3.df):,}\")\n",
    "print(f\"Signals (n=5): {len(signals_n5.df):,}\")\n",
    "print(f\"Signals (n=7): {len(signals_n7.df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine signals\n",
    "print(\"Sample signals (n=3):\")\n",
    "signals_n3.df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal statistics by pair\n",
    "print(\"Top 10 pairs by signal count (n=3):\")\n",
    "signals_n3.df.group_by(\"pair\").len().sort(\"len\", descending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streak length distribution\n",
    "print(\"Streak length distribution (n=3):\")\n",
    "signals_n3.df.select(\n",
    "    pl.col(\"streak_length\").mean().alias(\"mean\"),\n",
    "    pl.col(\"streak_length\").std().alias(\"std\"),\n",
    "    pl.col(\"streak_length\").min().alias(\"min\"),\n",
    "    pl.col(\"streak_length\").max().alias(\"max\"),\n",
    "    pl.col(\"streak_length\").quantile(0.5).alias(\"median\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Signal Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default signals (n=3)\n",
    "signals = signals_n3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Signal Distribution Metric\n",
    "\n",
    "Analyzes how signals are distributed across pairs and over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal distribution metric\n",
    "dist_metric = SignalDistributionMetric(\n",
    "    n_bars=15,\n",
    "    rolling_window_minutes=60 * 24,  # 1 day rolling window\n",
    "    ma_window_hours=24 * 7,  # 1 week moving average\n",
    ")\n",
    "\n",
    "dist_computed, dist_ctx = dist_metric.compute(\n",
    "    raw_data=raw_data,\n",
    "    signals=signals,\n",
    ")\n",
    "\n",
    "print(\"Distribution Metrics:\")\n",
    "for k, v in dist_computed[\"quant\"].items():\n",
    "    print(f\"  {k}: {v:.2f}\" if isinstance(v, float) else f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution\n",
    "fig = dist_metric.plot(dist_computed, dist_ctx, raw_data, signals)\n",
    "fig.update_layout(title=\"Funding Rate Signal Distribution\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Signal Profile Metric\n",
    "\n",
    "Analyzes post-signal price behavior - what happens to price after the signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal profile metric - analyze 3 days after signal (8h candles = 9 candles)\n",
    "# Note: look_ahead is in terms of number of rows/candles\n",
    "profile_metric = SignalProfileMetric(\n",
    "    look_ahead=9,  # 9 x 8h = 72 hours = 3 days\n",
    "    quantiles=(0.25, 0.75),\n",
    ")\n",
    "\n",
    "profile_computed, profile_ctx = profile_metric.compute(\n",
    "    raw_data=raw_data,\n",
    "    signals=signals,\n",
    ")\n",
    "\n",
    "print(\"Profile Metrics:\")\n",
    "for k, v in profile_computed[\"quant\"].items():\n",
    "    if isinstance(v, float):\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot price profile after signals\n",
    "fig = profile_metric.plot(profile_computed, profile_ctx, raw_data, signals)\n",
    "fig.update_layout(title=\"Price Profile After Funding Rate RISE Signals\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Custom Visualization: Compare Different Streak Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_profile_stats(signals: Signals, raw_data: RawData, look_ahead: int = 9):\n",
    "    \"\"\"Compute basic profile statistics for signals.\"\"\"\n",
    "    metric = SignalProfileMetric(look_ahead=look_ahead)\n",
    "    computed, _ = metric.compute(raw_data, signals)\n",
    "    return computed[\"quant\"]\n",
    "\n",
    "# Compare different streak lengths\n",
    "results = []\n",
    "for n, sig in [(3, signals_n3), (5, signals_n5), (7, signals_n7)]:\n",
    "    if len(sig.df) > 0:\n",
    "        stats = compute_profile_stats(sig, raw_data)\n",
    "        results.append({\n",
    "            \"min_streak\": n,\n",
    "            \"n_signals\": stats.get(\"n_signals\", 0),\n",
    "            \"final_mean_%\": stats.get(\"final_mean\", 0) * 100,\n",
    "            \"final_median_%\": stats.get(\"final_median\", 0) * 100,\n",
    "            \"max_uplift_%\": stats.get(\"avg_max_uplift\", 0) * 100,\n",
    "        })\n",
    "\n",
    "comparison_df = pl.DataFrame(results)\n",
    "print(\"Comparison of Different Streak Lengths:\")\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Number of Signals\", \"Average Return After Signal (%)\")\n",
    ")\n",
    "\n",
    "# Number of signals\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f\"n={r['min_streak']}\" for r in results],\n",
    "        y=[r[\"n_signals\"] for r in results],\n",
    "        marker_color=\"#2171b5\",\n",
    "        name=\"Signals\"\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Returns\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=[f\"n={r['min_streak']}\" for r in results],\n",
    "        y=[r[\"final_mean_%\"] for r in results],\n",
    "        marker_color=\"#41ab5d\",\n",
    "        name=\"Mean Return\"\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title=\"FundingRateDetector: Impact of min_positive_streak Parameter\",\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.update_yaxes(title_text=\"Count\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Return %\", row=1, col=2)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Signal Timing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze signal timing by day of week and hour\n",
    "signals_with_time = signals.df.with_columns(\n",
    "    pl.col(\"timestamp\").dt.weekday().alias(\"weekday\"),\n",
    "    pl.col(\"timestamp\").dt.hour().alias(\"hour\"),\n",
    ")\n",
    "\n",
    "# Weekday distribution\n",
    "weekday_dist = signals_with_time.group_by(\"weekday\").len().sort(\"weekday\")\n",
    "weekday_names = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    x=[weekday_names[int(r[\"weekday\"])] for r in weekday_dist.to_dicts()],\n",
    "    y=[r[\"len\"] for r in weekday_dist.to_dicts()],\n",
    "    marker_color=\"#2171b5\",\n",
    "))\n",
    "fig.update_layout(\n",
    "    title=\"Signal Distribution by Day of Week\",\n",
    "    xaxis_title=\"Day\",\n",
    "    yaxis_title=\"Number of Signals\",\n",
    "    height=400,\n",
    ")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Summary\n\nThis notebook demonstrated:\n\n1. **Multi-Exchange Data Loading**: \n   - Fetched perpetual pairs from **Binance, OKX, Bybit**\n   - Found common pairs across all 3 exchanges\n   - Binance provides `funding_rate` + `open_interest`, OKX/Bybit provide OHLCV\n\n2. **Aggregated Market Features**:\n   - **Aggregated Open Interest** from Binance (market-wide positioning)\n   - **Aggregated Volume** across all 3 exchanges (market sentiment)\n\n3. **FundingRateDetector**: Generates **RISE** signals when:\n   - Token had `n` consecutive positive funding rates (overleveraged longs)\n   - Funding rate turns negative (shorts taking control)\n   \n4. **Signal Metrics**:\n   - Distribution analysis: how signals spread across pairs and time\n   - Profile analysis: post-signal price behavior\n   - Parameter comparison: impact of `min_positive_streak` on signal quality\n\n### Key Insights\n\n- Longer positive funding streaks (higher `n`) produce fewer but potentially higher-quality signals\n- Multi-exchange volume analysis reveals market share dynamics\n- The transition from positive to negative funding can indicate a shift in market sentiment"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}