{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0b1c2d3",
   "metadata": {},
   "source": [
    "# SignalFlow Tutorial\n",
    "\n",
    "**SignalFlow** is a modular quantitative trading framework built on Polars for high-performance data processing.\n",
    "\n",
    "This tutorial walks through the complete pipeline:\n",
    "\n",
    "```\n",
    "Data Sources  -->  Raw Data  -->  Features  -->  Signals  -->  Labels  -->  Validation  -->  Backtest\n",
    "  (Exchange)      (DuckDB)     (Pipeline)    (Detector)   (Labeler)   (Meta-label)     (Strategy)\n",
    "```\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#1-setup--imports)\n",
    "2. [Data Layer](#2-data-layer)  \n",
    "3. [Feature Engineering](#3-feature-engineering)  \n",
    "4. [Signal Detection](#4-signal-detection)  \n",
    "5. [Signal Labeling](#5-signal-labeling)  \n",
    "6. [Signal Validation (Meta-Labeling)](#6-signal-validation-meta-labeling)  \n",
    "7. [Backtesting](#7-backtesting)  \n",
    "8. [Visualization](#8-visualization)  \n",
    "9. [Architecture & Next Steps](#9-architecture--next-steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c2d3e4",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3e4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import signalflow as sf\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e4f5a6",
   "metadata": {},
   "source": [
    "## 2. Data Layer\n",
    "\n",
    "SignalFlow's data layer consists of three components:\n",
    "\n",
    "| Component | Role | Examples |\n",
    "|-----------|------|----------|\n",
    "| **Data Source** | Downloads OHLCV from exchanges | `BinanceSpotLoader`, `BybitSpotLoader`, `OkxSpotLoader` |\n",
    "| **Raw Data Store** | Persists candles to disk | `DuckDbSpotStore`, `SqliteSpotStore`, `PgSpotStore` |\n",
    "| **RawDataFactory** | Loads stored data into memory | `RawDataFactory.from_duckdb_spot_store()` |\n",
    "\n",
    "For this tutorial we use `VirtualDataProvider` to generate synthetic data, so **no network access is required**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5a6b7",
   "metadata": {},
   "source": [
    "### 2.1 Data Store & Synthetic Data Generation\n",
    "\n",
    "`VirtualDataProvider` generates realistic OHLCV candles using a geometric random walk with configurable volatility and trend. It is a drop-in replacement for exchange loaders and writes directly to a `RawDataStore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.data.raw_store import DuckDbSpotStore\n",
    "from signalflow.data.source import VirtualDataProvider\n",
    "\n",
    "PAIRS = [\"BTCUSDT\", \"ETHUSDT\", \"SOLUSDT\"]\n",
    "N_BARS = 10_000  # ~7 days of 1-minute candles per pair\n",
    "START = datetime(2025, 1, 1)\n",
    "\n",
    "# 1) Create a DuckDB-backed store for raw OHLCV data\n",
    "spot_store = DuckDbSpotStore(db_path=Path(\"tutorial.duckdb\"))\n",
    "\n",
    "# 2) Generate synthetic data with realistic base prices\n",
    "provider = VirtualDataProvider(\n",
    "    store=spot_store,\n",
    "    base_prices={\"BTCUSDT\": 42_000.0, \"ETHUSDT\": 2_200.0, \"SOLUSDT\": 100.0},\n",
    "    volatility=0.003,  # per-bar return std deviation\n",
    "    trend=0.00005,  # slight uptrend drift per bar\n",
    "    seed=42,  # reproducible results\n",
    ")\n",
    "\n",
    "provider.download(pairs=PAIRS, n_bars=N_BARS, start=START)\n",
    "\n",
    "# Verify what we stored\n",
    "spot_store.get_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b8c9d0",
   "metadata": {},
   "source": [
    "### 2.2 Loading Data from Exchanges (Optional)\n",
    "\n",
    "SignalFlow supports three exchanges out of the box. Each loader is **async** and handles pagination, rate limits, and gap detection automatically.\n",
    "\n",
    "| Exchange | Spot Loader | Futures Loader |\n",
    "|----------|-------------|----------------|\n",
    "| **Binance** | `BinanceSpotLoader` | `BinanceFuturesUsdtLoader`, `BinanceFuturesCoinLoader` |\n",
    "| **Bybit** | `BybitSpotLoader` | `BybitFuturesLoader` |\n",
    "| **OKX** | `OkxSpotLoader` | `OkxFuturesLoader` |\n",
    "\n",
    "All sources normalize timestamps to **candle close time** (open time + 1 timeframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c9d0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Binance (requires network) ===\n",
    "from signalflow.data.source import BinanceSpotLoader\n",
    "\n",
    "loader = BinanceSpotLoader(store=spot_store, timeframe=\"1m\")\n",
    "await loader.download(\n",
    "    pairs=[\"BTCUSDT\", \"ETHUSDT\"],\n",
    "    start=datetime(2025, 12, 1),\n",
    "    end=datetime(2025, 12, 31),\n",
    ")\n",
    "\n",
    "# # === Bybit ===\n",
    "# from signalflow.data.source import BybitSpotLoader\n",
    "\n",
    "# loader = BybitSpotLoader(store=spot_store, timeframe=\"1m\")\n",
    "# await loader.download(\n",
    "#     pairs=[\"BTCUSDT\"],\n",
    "#     start=datetime(2025, 12, 1),\n",
    "#     end=datetime(2025, 12, 31),\n",
    "# )\n",
    "\n",
    "# # === OKX ===\n",
    "# from signalflow.data.source import OkxSpotLoader\n",
    "\n",
    "# loader = OkxSpotLoader(store=spot_store, timeframe=\"1m\")\n",
    "# await loader.download(\n",
    "#     pairs=[\"BTCUSDT\"],  # auto-converted to \"BTC-USDT\" for OKX API\n",
    "#     start=datetime(2025, 12, 1),\n",
    "#     end=datetime(2025, 12, 31),\n",
    "# )\n",
    "\n",
    "print(\"Uncomment the examples above to download real exchange data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2",
   "metadata": {},
   "source": [
    "### 2.3 RawDataFactory & RawDataView\n",
    "\n",
    "- **`RawData`** — immutable in-memory container holding Polars DataFrames keyed by type (e.g. `\"spot\"`). Created via `RawDataFactory`.\n",
    "- **`RawDataView`** — adapter that provides zero-copy Polars access (`to_polars()`) and optional Pandas conversion (`to_pandas()`).\n",
    "\n",
    "`RawDataFactory` validates the schema, removes duplicates, normalizes timestamps, and sorts by `(pair, timestamp)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e1f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.data import RawDataFactory\n",
    "\n",
    "raw_data = RawDataFactory.from_duckdb_spot_store(\n",
    "    spot_store_path=Path(\"tutorial.duckdb\"),\n",
    "    pairs=PAIRS,\n",
    "    start=START,\n",
    "    end=datetime(2025, 1, 8),\n",
    "    data_types=[\"spot\"],\n",
    ")\n",
    "\n",
    "raw_data_view = sf.RawDataView(raw=raw_data)\n",
    "\n",
    "# Access the spot DataFrame\n",
    "spot_df = raw_data_view.to_polars(\"spot\")\n",
    "print(f\"Shape: {spot_df.shape}\")\n",
    "print(f\"Pairs: {spot_df['pair'].unique().sort().to_list()}\")\n",
    "print(f\"Columns: {spot_df.columns}\")\n",
    "print(f\"Date range: {spot_df['timestamp'].min()} -> {spot_df['timestamp'].max()}\")\n",
    "spot_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Features transform raw OHLCV data into numerical indicators for signal detectors. SignalFlow provides:\n",
    "\n",
    "| Base Class | Scope | Override |\n",
    "|------------|-------|----------|\n",
    "| `Feature` | Per-pair (grouped by pair) | `compute_pair(df)` |\n",
    "| `GlobalFeature` | Cross-pair (all data at once) | `compute(df)` |\n",
    "\n",
    "Each feature declares `requires` (input columns) and `outputs` (produced columns) with `{param}` template support.\n",
    "\n",
    "`FeaturePipeline` orchestrates multiple features — it batches consecutive per-pair features into a single `group_by` call for performance, and validates that all column dependencies are satisfied at construction time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a3b4c5",
   "metadata": {},
   "source": [
    "### 3.1 Built-in Features\n",
    "\n",
    "| Feature | Class | Output | Key Params |\n",
    "|---------|-------|--------|------------|\n",
    "| RSI | `ExampleRsiFeature` | `rsi_{period}` | `period`, `price_col`, `normalized` |\n",
    "| SMA | `ExampleSmaFeature` | `sma_{period}` | `period`, `price_col`, `normalized` |\n",
    "| Global Mean RSI | `ExampleGlobalMeanRsiFeature` | `global_mean_rsi_{period}` | `period`, `add_diff` |\n",
    "| Linear Regression | `LinRegForecastFeature` | forecast values | various |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b4c5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.feature import ExampleRsiFeature, ExampleSmaFeature\n",
    "\n",
    "# Compute RSI-14 for a single pair\n",
    "btc_df = spot_df.filter(pl.col(\"pair\") == \"BTCUSDT\").sort(\"timestamp\")\n",
    "\n",
    "rsi = ExampleRsiFeature(period=14)\n",
    "btc_with_rsi = rsi.compute_pair(btc_df)\n",
    "\n",
    "print(f\"Output columns: {rsi.output_cols()}\")\n",
    "print(f\"Required columns: {rsi.required_cols()}\")\n",
    "print(f\"Warmup period: {rsi.warmup} bars\")\n",
    "btc_with_rsi.select([\"timestamp\", \"close\", \"rsi_14\"]).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c5d6e7",
   "metadata": {},
   "source": [
    "### 3.2 Custom Feature\n",
    "\n",
    "Creating a custom feature requires:\n",
    "1. Inherit from `Feature` (per-pair) or `GlobalFeature` (cross-pair)\n",
    "2. Declare `requires` and `outputs` (supports `{param}` templates)\n",
    "3. Implement `compute_pair()` — must return a DataFrame with the **same row count** as input\n",
    "4. Optionally decorate with `@sf_component(name=...)` to register in the component registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6e7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow import sf_component\n",
    "from signalflow.feature.base import Feature\n",
    "\n",
    "\n",
    "@dataclass\n",
    "@sf_component(name=\"custom/log_return\")\n",
    "class CustomLogReturnFeature(Feature):\n",
    "    \"\"\"Logarithmic return: ln(P_t / P_{t-n}).\"\"\"\n",
    "\n",
    "    price_col: str = \"close\"\n",
    "    period: int = 1\n",
    "\n",
    "    requires = [\"{price_col}\"]\n",
    "    outputs = [\"log_ret_{period}\"]\n",
    "\n",
    "    def compute_pair(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        col_name = f\"log_ret_{self.period}\"\n",
    "        return df.with_columns(pl.col(self.price_col).log().diff(n=self.period).alias(col_name))\n",
    "\n",
    "\n",
    "# Verify it works\n",
    "log_ret = CustomLogReturnFeature(period=60)\n",
    "print(f\"Requires: {log_ret.required_cols()}\")\n",
    "print(f\"Outputs:  {log_ret.output_cols()}\")\n",
    "\n",
    "# Test on single pair\n",
    "test_result = log_ret.compute_pair(btc_df)\n",
    "test_result.select([\"timestamp\", \"close\", \"log_ret_60\"]).tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e7f8a9",
   "metadata": {},
   "source": [
    "### 3.3 FeaturePipeline\n",
    "\n",
    "`FeaturePipeline` groups consecutive per-pair features into optimized batches (single `group_by` call). Global features are separated and applied between batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8a9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.feature import (\n",
    "    FeaturePipeline,\n",
    "    ExampleRsiFeature,\n",
    "    ExampleSmaFeature,\n",
    "    ExampleGlobalMeanRsiFeature,\n",
    "    OffsetFeature,\n",
    ")\n",
    "\n",
    "pipeline = FeaturePipeline(\n",
    "    features=[\n",
    "        # Per-pair features (batched into a single group_by)\n",
    "        ExampleRsiFeature(period=14),\n",
    "        ExampleRsiFeature(period=60),\n",
    "        ExampleSmaFeature(period=20),\n",
    "        ExampleSmaFeature(period=50),\n",
    "        CustomLogReturnFeature(period=60),\n",
    "        # Global feature (computed across all pairs per timestamp)\n",
    "        ExampleGlobalMeanRsiFeature(period=14, add_diff=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "features_df = pipeline.run(raw_data_view)\n",
    "print(f\"Pipeline outputs: {pipeline.output_cols()}\")\n",
    "print(f\"Features shape: {features_df.shape}\")\n",
    "features_df.select([\"pair\", \"timestamp\", \"rsi_14\", \"sma_20\", \"log_ret_60\", \"global_mean_rsi_14\"]).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a9b0c1",
   "metadata": {},
   "source": [
    "### 3.4 Multi-Timeframe Features: OffsetFeature\n",
    "\n",
    "`OffsetFeature` computes a registered feature on resampled (e.g. 15-minute) bars using **all possible offset alignments**. This captures multi-timeframe information without losing 1-minute resolution.\n",
    "\n",
    "How it works:\n",
    "1. Resample 1m OHLCV into `window`-minute bars for each possible offset (0 .. window-1)\n",
    "2. Compute the base feature on each resampled series\n",
    "3. Map results back to the original 1m timestamps\n",
    "\n",
    "The `feature_name` parameter references a component registered via `@sf_component(name=...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b0c1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_pipeline = FeaturePipeline(\n",
    "    features=[\n",
    "        ExampleRsiFeature(period=60),\n",
    "        OffsetFeature(\n",
    "            feature_name=\"example/rsi\",  # registered name of ExampleRsiFeature\n",
    "            feature_params={\"period\": 14},  # params for the base feature\n",
    "            window=15,  # 15-minute resampling window\n",
    "            prefix=\"ofs_\",  # output column prefix\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "offset_df = offset_pipeline.run(raw_data_view)\n",
    "print(f\"Offset outputs: {offset_pipeline.output_cols()}\")\n",
    "offset_df.filter(pl.col(\"pair\") == \"BTCUSDT\").select([\"timestamp\", \"rsi_60\", \"ofs_rsi_14\", \"offset\"]).tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1d2e3",
   "metadata": {},
   "source": [
    "## 4. Signal Detection\n",
    "\n",
    "A `SignalDetector` generates trading signals from raw data. The pipeline:\n",
    "\n",
    "```\n",
    "RawDataView  -->  preprocess()  -->  detect()  -->  validate  -->  Signals\n",
    "                  (features)         (logic)        (schema)       (pair, timestamp, signal_type, signal)\n",
    "```\n",
    "\n",
    "Each signal has:\n",
    "- `signal_type`: `\"rise\"` (bullish), `\"fall\"` (bearish), or `\"none\"`\n",
    "- `signal`: numeric value (typically +1 or -1)\n",
    "- optionally `probability`: confidence score\n",
    "\n",
    "The base class handles timezone normalization, schema validation, and duplicate detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "### 4.1 Built-in: SMA Cross Detector\n",
    "\n",
    "`ExampleSmaCrossDetector` generates signals on SMA crossovers:\n",
    "- **RISE**: fast SMA crosses above slow SMA\n",
    "- **FALL**: fast SMA crosses below slow SMA\n",
    "\n",
    "It automatically creates its own `FeaturePipeline` with two `ExampleSmaFeature` instances in `__post_init__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e3f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.detector import ExampleSmaCrossDetector\n",
    "\n",
    "sma_detector = ExampleSmaCrossDetector(fast_period=20, slow_period=50)\n",
    "sma_signals = sma_detector.run(raw_data_view)\n",
    "\n",
    "# The detector returns all rows including \"none\" - filter to actual crossovers\n",
    "active_sma = sma_signals.value.filter(pl.col(\"signal_type\") != \"none\")\n",
    "print(f\"Total crossovers detected: {active_sma.height}\")\n",
    "print(f\"  Rise: {active_sma.filter(pl.col('signal_type') == 'rise').height}\")\n",
    "print(f\"  Fall: {active_sma.filter(pl.col('signal_type') == 'fall').height}\")\n",
    "active_sma.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f4a5b6",
   "metadata": {},
   "source": [
    "### 4.2 Custom Signal Detector\n",
    "\n",
    "To create a custom detector:\n",
    "1. Inherit from `SignalDetector`\n",
    "2. Set `self.feature_pipeline` in `__post_init__()` for automatic feature extraction\n",
    "3. Implement `detect(features, context)` → return a `Signals` container\n",
    "\n",
    "The detector below fires when the 60-bar log return exceeds a threshold. Unlike the SMA cross detector, it **filters out** `\"none\"` signals in `detect()` for a cleaner output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a5b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.core import Signals, SignalType\n",
    "from signalflow.detector import SignalDetector\n",
    "from signalflow.feature import FeaturePipeline\n",
    "\n",
    "\n",
    "@dataclass\n",
    "@sf_component(name=\"momentum_breakout\")\n",
    "class MomentumBreakoutDetector(SignalDetector):\n",
    "    \"\"\"Detects large price moves based on log return thresholds.\"\"\"\n",
    "\n",
    "    threshold: float = 0.02\n",
    "    price_col: str = \"close\"\n",
    "    period: int = 60\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.feature_col = f\"log_ret_{self.period}\"\n",
    "        self.feature_pipeline = FeaturePipeline(\n",
    "            features=[CustomLogReturnFeature(price_col=self.price_col, period=self.period)]\n",
    "        )\n",
    "\n",
    "    def detect(self, features: pl.DataFrame, context: dict | None = None) -> Signals:\n",
    "        feat = pl.col(self.feature_col)\n",
    "\n",
    "        out = features.select(\n",
    "            [\n",
    "                self.pair_col,\n",
    "                self.ts_col,\n",
    "                pl.when(feat > self.threshold)\n",
    "                .then(pl.lit(SignalType.RISE.value))\n",
    "                .when(feat < -self.threshold)\n",
    "                .then(pl.lit(SignalType.FALL.value))\n",
    "                .otherwise(pl.lit(SignalType.NONE.value))\n",
    "                .alias(\"signal_type\"),\n",
    "                pl.when(feat > self.threshold)\n",
    "                .then(1)\n",
    "                .when(feat < -self.threshold)\n",
    "                .then(-1)\n",
    "                .otherwise(0)\n",
    "                .alias(\"signal\"),\n",
    "            ]\n",
    "        ).filter(pl.col(\"signal_type\") != SignalType.NONE.value)\n",
    "\n",
    "        return Signals(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6c7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MomentumBreakoutDetector(threshold=0.02, period=60)\n",
    "signals = detector.run(raw_data_view)\n",
    "\n",
    "print(f\"Detected {signals.value.height} momentum signals:\")\n",
    "print(f\"  Rise: {signals.value.filter(pl.col('signal_type') == 'rise').height}\")\n",
    "print(f\"  Fall: {signals.value.filter(pl.col('signal_type') == 'fall').height}\")\n",
    "signals.value.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7d8e9",
   "metadata": {},
   "source": [
    "## 5. Signal Labeling\n",
    "\n",
    "**Labelers** assign forward-looking labels to historical data: given a signal at time `t`, what happened to the price?\n",
    "\n",
    "This is used to train the **validator** (meta-labeler) that predicts signal quality.\n",
    "\n",
    "| Labeler | Strategy | Key Params |\n",
    "|---------|----------|------------|\n",
    "| `FixedHorizonLabeler` | Return after `N` bars | `horizon`, `price_col` |\n",
    "| `TripleBarrierLabeler` | First hit of profit/loss/time barrier (Numba-accelerated) | `vol_window`, `horizon`, `profit_multiplier` |\n",
    "| `TakeProfitLabeler` | Fixed-percentage barriers | varies |\n",
    "\n",
    "Labels are computed on the full price series but can be **masked** to signal timestamps only. To enable masking, pass `data_context={\"signal_keys\": ...}` with a DataFrame of `(pair, timestamp)` rows to label. Non-signal rows then get `label=\"none\"`, so only the detected signals receive meaningful labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.target import FixedHorizonLabeler\n",
    "\n",
    "labeler = FixedHorizonLabeler(\n",
    "    price_col=\"close\",\n",
    "    horizon=60,  # look 60 bars (minutes) ahead\n",
    "    include_meta=True,  # include t1 (future timestamp) and ret (log return)\n",
    ")\n",
    "\n",
    "# Extract signal timestamps for masking\n",
    "signal_keys = signals.value.select([\"pair\", \"timestamp\"])\n",
    "\n",
    "labeled_df = labeler.compute(\n",
    "    df=raw_data_view.to_polars(\"spot\"),\n",
    "    signals=signals,\n",
    "    data_context={\"signal_keys\": signal_keys},  # mask labels to signal timestamps\n",
    ")\n",
    "\n",
    "# Show labeled signal rows (non-signal rows have label=\"none\")\n",
    "labeled_signals = labeled_df.filter(pl.col(\"label\") != \"none\")\n",
    "print(f\"Total labeled signals: {labeled_signals.height}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "display(labeled_signals.group_by(\"label\").len().sort(\"label\"))\n",
    "print(f\"\\nSample:\")\n",
    "labeled_signals.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e9f0a1",
   "metadata": {},
   "source": [
    "## 6. Signal Validation (Meta-Labeling)\n",
    "\n",
    "The **validator** is a machine learning model that predicts the probability of each signal being correct. It works as a \"meta-labeler\":\n",
    "\n",
    "1. **Train** on historical features + labels (from the labeler)\n",
    "2. **Predict** class probabilities for each signal (`probability_rise`, `probability_fall`, `probability_none`)\n",
    "3. The strategy can then **filter** or **size** positions based on confidence\n",
    "\n",
    "`SklearnSignalValidator` supports: `random_forest`, `lightgbm`, `xgboost`, `logistic_regression`, `svm`, and `auto` (cross-validation model selection)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0a1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.validator import SklearnSignalValidator\n",
    "\n",
    "# 1) Get features at ALL timestamps from the detector's pipeline\n",
    "all_features = detector.preprocess(raw_data_view)\n",
    "\n",
    "# 2) Join features with labels on (pair, timestamp)\n",
    "train_df = all_features.join(\n",
    "    labeled_df,\n",
    "    on=[\"pair\", \"timestamp\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "# 3) Filter to signal rows only (recommended by validator docs)\n",
    "train_df = train_df.filter(pl.col(\"label\") != \"none\")\n",
    "print(f\"Training samples (signal rows only): {train_df.height}\")\n",
    "\n",
    "# 4) Time-based train/test split (80/20)\n",
    "split_idx = int(train_df.height * 0.8)\n",
    "\n",
    "# X must contain pair, timestamp, and feature columns\n",
    "feature_cols = [\"pair\", \"timestamp\", \"log_ret_60\"]\n",
    "X_train = train_df.slice(0, split_idx).select(feature_cols)\n",
    "X_test = train_df.slice(split_idx).select(feature_cols)\n",
    "y_train = train_df.slice(0, split_idx).select(\"label\")\n",
    "y_test = train_df.slice(split_idx).select(\"label\")\n",
    "\n",
    "# 5) Train the validator\n",
    "validator = SklearnSignalValidator(\n",
    "    model_type=\"random_forest\",\n",
    "    model_params={\"n_estimators\": 100, \"max_depth\": 5, \"random_state\": 42},\n",
    ")\n",
    "validator.fit(X_train, y_train)\n",
    "print(f\"Validator trained on {X_train.height} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a1b2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate_signals() adds probability columns to the Signals container\n",
    "validated_signals = validator.validate_signals(signals, all_features)\n",
    "\n",
    "print(f\"Validated {validated_signals.value.height} signals.\")\n",
    "print(f\"\\nTop signals by rise probability:\")\n",
    "validated_signals.value.select([\"pair\", \"timestamp\", \"signal_type\", \"probability_rise\", \"probability_fall\"]).sort(\n",
    "    \"probability_rise\", descending=True\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## 7. Backtesting\n",
    "\n",
    "The backtesting engine simulates strategy execution bar-by-bar:\n",
    "\n",
    "```\n",
    "For each timestamp:\n",
    "  1. Mark open positions to current prices\n",
    "  2. Compute metrics (equity, drawdown, sharpe, etc.)\n",
    "  3. Check exit rules → submit close orders\n",
    "  4. Check entry rules → submit open orders\n",
    "```\n",
    "\n",
    "### Key Components\n",
    "\n",
    "| Component | Class | Role |\n",
    "|-----------|-------|------|\n",
    "| **Entry Rule** | `SignalEntryRule` | Opens positions on validated signals, sizes by probability |\n",
    "| | `FixedSizeEntryRule` | Opens fixed-size positions |\n",
    "| **Exit Rule** | `TakeProfitStopLossExit` | Closes at TP/SL percentages |\n",
    "| **Executor** | `VirtualSpotExecutor` | Simulates fills with fees + slippage |\n",
    "| **Broker** | `BacktestBroker` | Manages orders, positions, and state |\n",
    "| **Metrics** | `TotalReturnMetric`, `DrawdownMetric`, etc. | Computed every bar |\n",
    "| **Runner** | `OptimizedBacktestRunner` | Pre-builds lookups for faster iteration |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b3c4d5",
   "metadata": {},
   "source": [
    "### 7.1 Setting Up Strategy Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from signalflow.strategy.broker import BacktestBroker\n",
    "from signalflow.strategy.broker.executor import VirtualSpotExecutor\n",
    "from signalflow.data.strategy_store import DuckDbStrategyStore\n",
    "from signalflow.strategy.runner import OptimizedBacktestRunner\n",
    "from signalflow.strategy.component.entry import SignalEntryRule\n",
    "from signalflow.strategy.component.exit import TakeProfitStopLossExit\n",
    "from signalflow.analytic.strategy import (\n",
    "    TotalReturnMetric,\n",
    "    BalanceAllocationMetric,\n",
    "    DrawdownMetric,\n",
    "    WinRateMetric,\n",
    "    SharpeRatioMetric,\n",
    ")\n",
    "\n",
    "INITIAL_CAPITAL = 10_000.0\n",
    "\n",
    "# Strategy state persistence\n",
    "strategy_store = DuckDbStrategyStore(\"tutorial_strategy.duckdb\")\n",
    "strategy_store.init()\n",
    "\n",
    "# Order execution with fees and slippage\n",
    "executor = VirtualSpotExecutor(fee_rate=0.001, slippage_pct=0.001)\n",
    "broker = BacktestBroker(executor=executor, store=strategy_store)\n",
    "\n",
    "# Entry rule: open positions on validated signals\n",
    "entry_rule = SignalEntryRule(\n",
    "    base_position_size=1000.0,  # base size in quote currency\n",
    "    use_probability_sizing=True,  # scale size by signal probability\n",
    "    min_probability=0.5,  # ignore signals below this confidence\n",
    "    max_positions_per_pair=1,  # no stacking positions\n",
    "    max_total_positions=20,\n",
    "    allow_shorts=False,  # long only\n",
    ")\n",
    "\n",
    "# Exit rule: symmetric take-profit and stop-loss\n",
    "exit_rule = TakeProfitStopLossExit(\n",
    "    take_profit_pct=0.02,  # +2% take profit\n",
    "    stop_loss_pct=0.02,  # -2% stop loss\n",
    ")\n",
    "\n",
    "# Performance metrics (computed every bar)\n",
    "metrics = [\n",
    "    TotalReturnMetric(initial_capital=INITIAL_CAPITAL),\n",
    "    BalanceAllocationMetric(initial_capital=INITIAL_CAPITAL),\n",
    "    DrawdownMetric(),\n",
    "    WinRateMetric(),\n",
    "    SharpeRatioMetric(initial_capital=INITIAL_CAPITAL, window_size=100),\n",
    "]\n",
    "\n",
    "print(\"Strategy components ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c4d5e6",
   "metadata": {},
   "source": [
    "### 7.2 Running the Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = OptimizedBacktestRunner(\n",
    "    strategy_id=\"tutorial_momentum\",\n",
    "    broker=broker,\n",
    "    entry_rules=[entry_rule],\n",
    "    exit_rules=[exit_rule],\n",
    "    metrics=metrics,\n",
    "    initial_capital=INITIAL_CAPITAL,\n",
    "    data_key=\"spot\",\n",
    ")\n",
    "\n",
    "# run() iterates over every timestamp in the raw data\n",
    "final_state = runner.run(raw_data, validated_signals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d5e6f7",
   "metadata": {},
   "source": [
    "### 7.3 Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = runner.get_results()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"BACKTEST RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"  Initial Capital:  ${INITIAL_CAPITAL:,.2f}\")\n",
    "print(f\"  Final Equity:     ${results.get('final_equity', 0):,.2f}\")\n",
    "print(f\"  Total Return:     {results.get('final_return', 0) * 100:.2f}%\")\n",
    "print(f\"  Max Drawdown:     {results.get('max_drawdown', 0) * 100:.2f}%\")\n",
    "print(f\"  Win Rate:         {results.get('win_rate', 0) * 100:.1f}%\")\n",
    "print(f\"  Sharpe Ratio:     {results.get('sharpe_ratio', 0):.3f}\")\n",
    "print(f\"  Total Trades:     {results.get('total_trades', 0)}\")\n",
    "print(f\"    Entries:        {results.get('entry_count', 0)}\")\n",
    "print(f\"    Exits:          {results.get('exit_count', 0)}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Show recent trades\n",
    "trades_df = results[\"trades_df\"]\n",
    "if trades_df.height > 0:\n",
    "    print(f\"\\nRecent trades ({trades_df.height} total):\")\n",
    "    display(trades_df.tail(10))\n",
    "\n",
    "# Metrics time series\n",
    "metrics_df = results[\"metrics_df\"]\n",
    "print(f\"\\nMetrics time series: {metrics_df.shape}\")\n",
    "metrics_df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6a7b8",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7b8c9",
   "metadata": {},
   "source": [
    "### 8.1 Signals on Price Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_signals(raw_df: pl.DataFrame, signals_df: pl.DataFrame, pair: str = \"BTCUSDT\"):\n",
    "    \"\"\"Plot price with buy/sell signal markers.\"\"\"\n",
    "    price = raw_df.filter(pl.col(\"pair\") == pair).sort(\"timestamp\")\n",
    "    price = price.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "\n",
    "    sigs = signals_df.filter(pl.col(\"pair\") == pair)\n",
    "    sigs = sigs.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "    sigs = sigs.join(price.select([\"timestamp\", \"close\"]), on=\"timestamp\", how=\"inner\")\n",
    "\n",
    "    df_plot = price.to_pandas()\n",
    "    sig_plot = sigs.to_pandas()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=df_plot[\"timestamp\"],\n",
    "            y=df_plot[\"close\"],\n",
    "            mode=\"lines\",\n",
    "            name=f\"{pair} Price\",\n",
    "            line=dict(color=\"#2E86C1\", width=1.5),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    buys = sig_plot[sig_plot[\"signal\"] == 1]\n",
    "    if not buys.empty:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=buys[\"timestamp\"],\n",
    "                y=buys[\"close\"],\n",
    "                mode=\"markers\",\n",
    "                name=\"Rise Signal\",\n",
    "                marker=dict(symbol=\"triangle-up\", size=12, color=\"#00CC96\", line=dict(width=1, color=\"black\")),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    sells = sig_plot[sig_plot[\"signal\"] == -1]\n",
    "    if not sells.empty:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=sells[\"timestamp\"],\n",
    "                y=sells[\"close\"],\n",
    "                mode=\"markers\",\n",
    "                name=\"Fall Signal\",\n",
    "                marker=dict(symbol=\"triangle-down\", size=12, color=\"#EF553B\", line=dict(width=1, color=\"black\")),\n",
    "            )\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"SignalFlow: {pair} Signals\",\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Price\",\n",
    "        template=\"plotly_white\",\n",
    "        height=500,\n",
    "        hovermode=\"x unified\",\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "for pair in PAIRS:\n",
    "    fig = plot_signals(spot_df, signals.value, pair=pair)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d1e2",
   "metadata": {},
   "source": [
    "### 8.2 Validated Signals with Confidence\n",
    "\n",
    "Visualize how the meta-labeler scores signals. Marker size reflects confidence; gray markers indicate low-confidence signals that would be **ignored** by the entry rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validated_signals(raw_df: pl.DataFrame, val_signals: sf.Signals, pair: str = \"BTCUSDT\"):\n",
    "    \"\"\"Plot signals colored and sized by validation probability.\"\"\"\n",
    "    price = raw_df.filter(pl.col(\"pair\") == pair).sort(\"timestamp\")\n",
    "    price = price.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "\n",
    "    sigs = val_signals.value.filter(pl.col(\"pair\") == pair)\n",
    "    sigs = sigs.with_columns(pl.col(\"timestamp\").cast(pl.Datetime(\"us\")))\n",
    "    merged = sigs.join(price.select([\"timestamp\", \"close\"]), on=\"timestamp\", how=\"inner\").to_pandas()\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=price.to_pandas()[\"timestamp\"],\n",
    "            y=price.to_pandas()[\"close\"],\n",
    "            mode=\"lines\",\n",
    "            name=\"Price\",\n",
    "            line=dict(color=\"#2962FF\", width=1.5),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for sig_type, prob_col, color_hi, color_lo, sym, label in [\n",
    "        (\"rise\", \"probability_rise\", \"#00C853\", \"#B0BEC5\", \"triangle-up\", \"Rise\"),\n",
    "        (\"fall\", \"probability_fall\", \"#C62828\", \"#B0BEC5\", \"triangle-down\", \"Fall\"),\n",
    "    ]:\n",
    "        subset = merged[merged[\"signal_type\"] == sig_type]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        # Low confidence (ignored by strategy)\n",
    "        low = subset[subset[prob_col] < 0.5]\n",
    "        if not low.empty:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=low[\"timestamp\"],\n",
    "                    y=low[\"close\"],\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"{label} (low conf)\",\n",
    "                    marker=dict(symbol=sym, size=7, color=color_lo),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # High confidence (acted on by strategy)\n",
    "        high = subset[subset[prob_col] >= 0.5]\n",
    "        if not high.empty:\n",
    "            sizes = 10 + (high[prob_col] * 15)\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=high[\"timestamp\"],\n",
    "                    y=high[\"close\"],\n",
    "                    mode=\"markers\",\n",
    "                    name=f\"{label} (high conf)\",\n",
    "                    marker=dict(symbol=sym, size=sizes, color=color_hi, line=dict(width=1, color=\"black\")),\n",
    "                    text=[f\"{p:.2f}\" for p in high[prob_col]],\n",
    "                    hovertemplate=f\"<b>{label}</b><br>Price: %{{y:.2f}}<br>Conf: %{{text}}<extra></extra>\",\n",
    "                )\n",
    "            )\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Validated Signals: {pair}\",\n",
    "        template=\"plotly_white\",\n",
    "        height=550,\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", y=1.02),\n",
    "    )\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_validated_signals(spot_df, validated_signals, pair=\"BTCUSDT\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e2f3a4",
   "metadata": {},
   "source": [
    "### 8.3 Backtest Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "def plot_backtest_performance(results: dict):\n",
    "    \"\"\"3-panel chart: return, positions, and drawdown.\"\"\"\n",
    "    metrics_df = results.get(\"metrics_df\")\n",
    "    if metrics_df is None or metrics_df.height == 0:\n",
    "        print(\"No metrics to plot.\")\n",
    "        return\n",
    "\n",
    "    if \"timestamp\" in metrics_df.columns:\n",
    "        ts = (\n",
    "            metrics_df.select(pl.from_epoch(pl.col(\"timestamp\").cast(pl.Int64), time_unit=\"s\").alias(\"dt\"))\n",
    "            .get_column(\"dt\")\n",
    "            .to_list()\n",
    "        )\n",
    "    else:\n",
    "        ts = list(range(metrics_df.height))\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=3,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.06,\n",
    "        subplot_titles=(\"Strategy Return (%)\", \"Open / Closed Positions\", \"Drawdown (%)\"),\n",
    "        row_heights=[0.4, 0.3, 0.3],\n",
    "    )\n",
    "\n",
    "    # Row 1: Total return\n",
    "    if \"total_return\" in metrics_df.columns:\n",
    "        ret_pct = (metrics_df.get_column(\"total_return\") * 100).to_list()\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ts,\n",
    "                y=ret_pct,\n",
    "                mode=\"lines\",\n",
    "                name=\"Return\",\n",
    "                line=dict(color=\"#1E88E5\", width=2),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "    fig.add_hline(y=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=1)\n",
    "\n",
    "    # Row 2: Positions\n",
    "    if \"open_positions\" in metrics_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ts,\n",
    "                y=metrics_df.get_column(\"open_positions\").to_list(),\n",
    "                mode=\"lines\",\n",
    "                name=\"Open\",\n",
    "                fill=\"tozeroy\",\n",
    "                line=dict(color=\"#43A047\", width=1.5),\n",
    "                fillcolor=\"rgba(67, 160, 71, 0.15)\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "    if \"closed_positions\" in metrics_df.columns:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ts,\n",
    "                y=metrics_df.get_column(\"closed_positions\").to_list(),\n",
    "                mode=\"lines\",\n",
    "                name=\"Closed\",\n",
    "                line=dict(color=\"#8E24AA\", width=1.5, dash=\"dot\"),\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    # Row 3: Drawdown\n",
    "    if \"current_drawdown\" in metrics_df.columns:\n",
    "        dd_pct = [-d * 100 for d in metrics_df.get_column(\"current_drawdown\").to_list()]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=ts,\n",
    "                y=dd_pct,\n",
    "                mode=\"lines\",\n",
    "                name=\"Drawdown\",\n",
    "                line=dict(color=\"#E53935\", width=2),\n",
    "                fill=\"tozeroy\",\n",
    "                fillcolor=\"rgba(229, 57, 53, 0.15)\",\n",
    "            ),\n",
    "            row=3,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    final_return = results.get(\"final_return\", 0) * 100\n",
    "    fig.update_layout(\n",
    "        title=f\"Backtest Results | Return: {final_return:.2f}%\",\n",
    "        template=\"plotly_white\",\n",
    "        height=800,\n",
    "        hovermode=\"x unified\",\n",
    "        legend=dict(orientation=\"h\", y=1.02),\n",
    "    )\n",
    "    fig.update_yaxes(title_text=\"Return (%)\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Count\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Drawdown (%)\", row=3, col=1)\n",
    "    fig.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_backtest_performance(results)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a4b5c6",
   "metadata": {},
   "source": [
    "## 9. Architecture & Next Steps\n",
    "\n",
    "### Data Flow\n",
    "\n",
    "```\n",
    "Exchange APIs ─── BinanceSpotLoader ──┐\n",
    "                  BybitSpotLoader  ───┤\n",
    "                  OkxSpotLoader  ─────┤\n",
    "                  VirtualProvider ────┘\n",
    "                         │\n",
    "                    RawDataStore (DuckDB / SQLite / PostgreSQL)\n",
    "                         │\n",
    "                    RawDataFactory\n",
    "                         │\n",
    "                ┌── RawData / RawDataView ──┐\n",
    "                │                           │\n",
    "         FeaturePipeline              SignalDetector\n",
    "          (Feature,                     (detect)\n",
    "           GlobalFeature,                  │\n",
    "           OffsetFeature)                  │\n",
    "                │                      Signals\n",
    "                │                          │\n",
    "                └───── Labeler ────────────┘\n",
    "                         │\n",
    "                  SklearnSignalValidator\n",
    "                         │\n",
    "                  Validated Signals\n",
    "                         │\n",
    "                ┌── BacktestRunner ──┐\n",
    "                │   (per-bar loop)   │\n",
    "                │                    │\n",
    "           EntryRules          ExitRules\n",
    "                │                    │\n",
    "                └── BacktestBroker ──┘\n",
    "                     (executor)\n",
    "                         │\n",
    "                   StrategyState\n",
    "                    (portfolio,\n",
    "                     positions,\n",
    "                     metrics)\n",
    "```\n",
    "\n",
    "### Key Design Decisions\n",
    "\n",
    "- **Component Registry**: All classes decorated with `@sf_component(name=...)` are discoverable at runtime via `sf.get_component(type, name)`. This enables declarative config-driven pipelines.\n",
    "- **Immutability**: Core containers (`RawData`, `Signals`, `Trade`, `OrderFill`) are frozen dataclasses for reproducibility.\n",
    "- **Polars-first**: All internal data processing uses Polars; Pandas is available for visualization via `RawDataView.to_pandas()`.\n",
    "- **Store Backends**: Choose `DuckDB` (fast, default), `SQLite` (zero extra deps), or `PostgreSQL` (multi-user, remote).\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Real exchange data** — Replace `VirtualDataProvider` with `BinanceSpotLoader` / `BybitSpotLoader` / `OkxSpotLoader`\n",
    "2. **Custom features** — Build domain-specific indicators by extending `Feature` or `GlobalFeature`\n",
    "3. **Triple barrier labeling** — Use `TripleBarrierLabeler` for volatility-aware, adaptive labels\n",
    "4. **Model tuning** — Use `validator.tune()` with Optuna for hyperparameter optimization\n",
    "5. **Live / paper trading** — Use `RealtimeRunner` with real executors for live execution\n",
    "6. **Signal composition** — Combine detectors using `Signals.__add__()` to merge signal sets with priority logic\n",
    "7. **Save / load models** — Use `validator.save(path)` and `SklearnSignalValidator.load(path)` for persistence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b5c6d7",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c6d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spot_store.close()\n",
    "strategy_store.close()\n",
    "\n",
    "# Optionally remove tutorial databases:\n",
    "# Path(\"tutorial.duckdb\").unlink(missing_ok=True)\n",
    "# Path(\"tutorial_strategy.duckdb\").unlink(missing_ok=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}